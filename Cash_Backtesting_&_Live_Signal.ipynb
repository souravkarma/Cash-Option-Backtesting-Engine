{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWFW-Q8opBu"
      },
      "source": [
        "# **30 Day close breakdown Intraday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbTQQBOb9UlL",
        "outputId": "117a8d8e-bdcf-48eb-b0cd-310abd2912e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed past-30-day mins (excluding today) for 500 symbols ‚Üí 240 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 10285 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 783 signals selected for trading\n",
            "‚úÖ Backtest completed. 779 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Memory-efficient BACKTEST for CASH DATA using Polars + Pandas\n",
        "# ================================================================\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 0) Load ONLY required times (09:20 & 15:29)\n",
        "# ------------------------------------------------\n",
        "def extract_relevant_times(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Load with Polars (fast + memory efficient)\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # ‚úÖ Remove timezone part (+05:30)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    # ‚úÖ Convert to proper datetime\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    # ‚úÖ Extract date & HH:MM\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    # ‚úÖ Keep only 09:20 & 15:29 rows\n",
        "    df = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "\n",
        "    if df.is_empty():\n",
        "        return None, None\n",
        "\n",
        "    # ‚úÖ Convert small subset ‚Üí pandas\n",
        "    pdf = df.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    # ‚úÖ Separate into two series\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920  = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "\n",
        "    return symbol, {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1) Build per-symbol data\n",
        "# ------------------------------------------------\n",
        "symbol_data = {}\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, data = extract_relevant_times(f)\n",
        "    if symbol and data:\n",
        "        symbol_data[symbol] = data\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_data)} symbols with required times\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2) Precompute **EXCLUDING TODAY** 30-day rolling min\n",
        "# ------------------------------------------------\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_data.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ rolling min of past 30 days (EXCLUDING today)\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# ‚úÖ Skip the first 31 trading days for accurate backtest\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3) MAIN BREAKDOWN SCAN ‚Üí collect ALL breakdowns\n",
        "# ------------------------------------------------\n",
        "all_breakdowns = []  # keep for verification\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_data.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        # ‚úÖ need at least some past data (not NaN)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        # ‚úÖ breakdown: today_close < previous 30-day min\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "# ‚úÖ Save ALL breakdowns for verification\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4) RANK by ROI (lowest) ‚Üí pick TOP 4 per day\n",
        "# ------------------------------------------------\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # sort by lowest ROI ‚Üí top 4 strongest breakdowns\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\",ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5) NEXT DAY TRADE ‚Üí Sell 09:20, Buy 15:29\n",
        "# ------------------------------------------------\n",
        "output_trades = []\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # get the available dates for this symbol\n",
        "    dates_list = sorted(symbol_data[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1  # next day\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ NEXT DAY prices\n",
        "    sell_price = symbol_data[sym][\"open_0920\"].get(trade_date, None)   # SELL next day 09:20\n",
        "    buy_price  = symbol_data[sym][\"close_1529\"].get(trade_date, None)  # BUY next day 15:29\n",
        "\n",
        "    if sell_price is None or buy_price is None:\n",
        "        continue\n",
        "\n",
        "    pnl = round((sell_price - buy_price), 2)  # SELL‚ÜíBUY\n",
        "    roi_trade = round((pnl / sell_price) * 100, 2) if sell_price != 0 else 0\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        sell_price,\n",
        "        buy_price,\n",
        "        pnl,\n",
        "        roi_trade\n",
        "    ])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6) Save executed trades\n",
        "# ------------------------------------------------\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"BUY_1529\", \"PNL\", \"TRADE_ROI%\"])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7XVnI94gjdR"
      },
      "source": [
        "Added SL Rank Lowest ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4JtN-FSr9Hj",
        "outputId": "4b001056-0815-440b-bc0f-89ba66c50a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed past-30-day mins (excluding today) for 500 symbols ‚Üí 261 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 11920 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 867 signals selected for trading\n",
            "‚úÖ Backtest completed. 863 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004     # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.05   # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.03      # -3% portfolio SL\n",
        "START_TIME = \"09:20\"          # Trade entry time\n",
        "END_TIME = \"15:20\"            # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_0920  = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_0920_1529[symbol] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=True).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = \"15:29\"\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ SL can trigger ONLY after entry time (09:20)\n",
        "        if cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    # Group by TRADE_DATE to get daily total PnL and ROI\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",  # average ROI per trade that day\n",
        "        \"SYMBOL\": \"count\"      # how many trades executed that day\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # ‚úÖ Optional cumulative PnL across days\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    # ‚úÖ Save as separate sheet\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFhK-33J713d"
      },
      "source": [
        "Rank Highest ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX8nMCo8ufOJ",
        "outputId": "05836524-aaa1-4ee6-bcf5-41f2b550f85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed past-30-day mins (excluding today) for 500 symbols ‚Üí 260 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 11916 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 863 signals selected for trading\n",
            "‚úÖ Backtest completed. 859 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.05   # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.03      # -3% portfolio SL\n",
        "START_TIME = \"09:20\"          # Trade entry time\n",
        "END_TIME = \"15:20\"            # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_0920  = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_0920_1529[symbol] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ SL can trigger ONLY after entry time (09:20)\n",
        "        if cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    # Group by TRADE_DATE to get daily total PnL and ROI\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",  # average ROI per trade that day\n",
        "        \"SYMBOL\": \"count\"      # how many trades executed that day\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # ‚úÖ Optional cumulative PnL across days\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    # ‚úÖ Save as separate sheet\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEzaOmzlZkxR"
      },
      "source": [
        " SL activate at 10:10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JghpaCNRzJzZ",
        "outputId": "c2b0e91d-f039-4d89-b06f-42de5dcb9dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed past-30-day mins (excluding today) for 500 symbols ‚Üí 259 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 11897 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 859 signals selected for trading\n",
            "‚úÖ Backtest completed. 855 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -3% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # ‚úÖ dynamically pick START_TIME instead of hardcoded \"09:20\"\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start  = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ entry price now uses START_TIME variable\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD5A2VaacRVO"
      },
      "source": [
        "Cash2 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA0XrufDa6JU",
        "outputId": "fd1c66b8-a29e-47cf-d320-4c5e4001ce42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed past-30-day mins for 500 symbols ‚Üí 635 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 20279 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 2264 signals selected for trading\n",
            "‚úÖ Backtest completed. 2260 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ SL/Target Params\n",
        "INDIVIDUAL_SL_PCT = 0.004\n",
        "PORTFOLIO_TARGET_PCT = 0.05\n",
        "PORTFOLIO_SL_PCT = -0.03\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# ‚úÖ Paths\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "\n",
        "# Modified to include all CSV files in the directory\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files\")\n",
        "\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "def load_summary_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return symbol, None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, close_1529, open_0920\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    sym, close_1529, open_0920 = load_summary_data(f)\n",
        "    if close_1529 is not None and open_0920 is not None:\n",
        "        symbol_close_0920_1529[sym] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "\n",
        "    if sym not in symbol_close_0920_1529:\n",
        "        continue\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # ‚úÖ Load full data only for this symbol\n",
        "    df_full = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJqpSwObC47P"
      },
      "source": [
        "VWAP + 30DAYS CLOSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jss4xpYyC9Gl",
        "outputId": "248f2605-b536-478a-a923-cf2700215867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with required times\n",
            "‚úÖ Computed past-30-day mins (excluding today) for 503 symbols ‚Üí 282 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 10823 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 941 signals selected for trading\n",
            "‚úÖ Backtest completed. 937 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -3% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # ‚úÖ dynamically pick START_TIME instead of hardcoded \"09:20\"\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start  = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        # Compute daily VWAP\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_data = df_full.filter(pl.col(\"TradeDate\") == trade_date)\n",
        "        if day_data.is_empty():\n",
        "            continue\n",
        "        typical_price = (day_data[\"High\"] + day_data[\"Low\"] + day_data[\"Close\"]) / 3\n",
        "        vwap_numerator = (typical_price * day_data[\"Volume\"]).sum()\n",
        "        vwap_denominator = day_data[\"Volume\"].sum()\n",
        "        if vwap_denominator == 0:\n",
        "            continue\n",
        "        vwap = vwap_numerator / vwap_denominator\n",
        "\n",
        "        if today_close < lowest_low_prev30 and today_close <= vwap * 1.002:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ entry price now uses START_TIME variable\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUnp7qliOEZA"
      },
      "source": [
        "VWAP ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7vtTdvyOHsn",
        "outputId": "7deb2511-4084-4f97-f01c-038069cf3c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Identified 259 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 87045 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 1036 signals selected for trading\n",
            "‚úÖ Backtest completed. 1032 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -3% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # ‚úÖ Dynamically pick START_TIME and \"15:29\"\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Identified {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        # Compute daily VWAP\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_data = df_full.filter(pl.col(\"TradeDate\") == trade_date)\n",
        "        if day_data.is_empty():\n",
        "            continue\n",
        "        typical_price = (day_data[\"High\"] + day_data[\"Low\"] + day_data[\"Close\"]) / 3\n",
        "        vwap_numerator = (typical_price * day_data[\"Volume\"]).sum()\n",
        "        vwap_denominator = day_data[\"Volume\"].sum()\n",
        "        if vwap_denominator == 0:\n",
        "            continue\n",
        "        vwap = vwap_numerator / vwap_denominator\n",
        "\n",
        "        # Check if closing price is less than or equal to 0.2% above VWAP\n",
        "        if today_close <= vwap * 1.002:\n",
        "            # Calculate percentage difference from VWAP for ranking (negative means close is below VWAP)\n",
        "            roi = round(((today_close / vwap) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, vwap, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"VWAP\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Rank by lowest decrease (smallest ROI, ascending order since ROI is negative)\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=True).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ Entry price uses START_TIME variable\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDXIOctHycZh"
      },
      "source": [
        "ADV 30day Close Strat with Live Highest ROI%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0VJ4Ft8yij0",
        "outputId": "d486aa9f-82bd-47b8-c20a-704c63ea47d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 484 cash files...\n",
            "‚úÖ Processed 50/484 symbols\n",
            "‚úÖ Processed 100/484 symbols\n",
            "‚úÖ Processed 150/484 symbols\n",
            "‚úÖ Processed 200/484 symbols\n",
            "‚úÖ Processed 250/484 symbols\n",
            "‚úÖ Processed 300/484 symbols\n",
            "‚úÖ Processed 350/484 symbols\n",
            "‚úÖ Processed 400/484 symbols\n",
            "‚úÖ Processed 450/484 symbols\n",
            "‚úÖ Loaded 484 symbols with required times\n",
            "‚úÖ Computed past-364-day mins (excluding today) for 484 symbols ‚Üí 389 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 5 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 5 signals selected for trading\n",
            "‚úÖ Backtest completed. 5 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -3% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data23\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # ‚úÖ dynamically pick START_TIME instead of hardcoded \"09:20\"\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start  = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "past364_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(364, min_periods=1).min().shift(1)\n",
        "    past364_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-364-day mins (excluding today) for {len(past364_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 365:\n",
        "    unique_trade_dates = unique_trade_dates[365:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        if sym not in past364_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev364 = past364_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev364 is None or pd.isna(lowest_low_prev364):\n",
        "            continue\n",
        "\n",
        "        # Compute daily VWAP\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_data = df_full.filter(pl.col(\"TradeDate\") == trade_date)\n",
        "        if day_data.is_empty():\n",
        "            continue\n",
        "        typical_price = (day_data[\"High\"] + day_data[\"Low\"] + day_data[\"Close\"]) / 3\n",
        "        vwap_numerator = (typical_price * day_data[\"Volume\"]).sum()\n",
        "        vwap_denominator = day_data[\"Volume\"].sum()\n",
        "        if vwap_denominator == 0:\n",
        "            continue\n",
        "        vwap = vwap_numerator / vwap_denominator\n",
        "\n",
        "        if today_close < lowest_low_prev364 and today_close <= vwap * 1.002:\n",
        "            roi = round(((today_close / lowest_low_prev364) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev364, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV364_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ entry price now uses START_TIME variable\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoV8ro0Kiykd"
      },
      "source": [
        "# **Live Signal for 30 days close breakdown Rank**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0KgZkJkq0e1",
        "outputId": "5a4c933a-c64e-4caf-c7ed-b6443fb4a5af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Latest available date: 2025-08-19 00:00:00\n",
            "‚úÖ Next-day live signals generated ‚Üí LIVE_SIGNALS_NEXT_DAY.csv\n",
            "            SYMBOL NEXT_TRADE_DATE  SELL_PRICE_0920  QTY  MARGIN_USED  \\\n",
            "0    cash_APARINDS      2025-08-20           8448.0    7      59136.0   \n",
            "1  cash_KALYANKJIL      2025-08-20            508.1  123      62496.3   \n",
            "2   cash_INTELLECT      2025-08-20            907.4   68      61703.2   \n",
            "3       cash_PTCIL      2025-08-20          13350.0    4      53400.0   \n",
            "\n",
            "   SL_PRICE  \n",
            "0   8481.79  \n",
            "1    510.13  \n",
            "2    911.03  \n",
            "3  13403.40  \n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable params\n",
        "INDIVIDUAL_SL_PCT = 0.004     # 1.5% individual SL\n",
        "INITIAL_CAPITAL = 250000      # starting capital for next-day trade\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_for_signals(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    return symbol, df, df_sel\n",
        "\n",
        "symbol_close_0920_1529 = {}\n",
        "all_dates = set()\n",
        "past30_min_dict = {}\n",
        "\n",
        "for f in all_files:\n",
        "    sym, df_full, df_sel = load_for_signals(f)\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_0920  = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_0920_1529[sym] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "        all_dates.update(close_1529.index)\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if not close_series.empty:\n",
        "        roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "        past30_min_dict[sym] = roll_min_excl_today\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "# ‚úÖ Latest available date in DB\n",
        "today_date = unique_trade_dates[-1]\n",
        "print(f\"‚úÖ Latest available date: {today_date}\")\n",
        "\n",
        "# ‚úÖ Determine NEXT trading date after latest date\n",
        "next_trade_date_global = None\n",
        "for dt in unique_trade_dates:\n",
        "    if dt > today_date:\n",
        "        next_trade_date_global = dt\n",
        "        break\n",
        "# If no next date, assume next date = today_date + 1 day (future trade)\n",
        "if next_trade_date_global is None:\n",
        "    import datetime\n",
        "    next_trade_date_global = today_date + pd.Timedelta(days=1)\n",
        "\n",
        "# ‚úÖ Get breakdown signals for today_date\n",
        "signals_today = []\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    today_close = d[\"close_1529\"].get(today_date, None)\n",
        "    if today_close is None:\n",
        "        continue\n",
        "    lowest_low_prev30 = past30_min_dict[sym].get(today_date, None)\n",
        "    if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "        continue\n",
        "    if today_close < lowest_low_prev30:\n",
        "        roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "        signals_today.append([sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "signals_df = pd.DataFrame(signals_today, columns=[\"SYMBOL\", \"TODAY_CLOSE\", \"PREV30_MIN\", \"ROI\"])\n",
        "if signals_df.empty:\n",
        "    print(\"‚ùå No breakdown signals for today\")\n",
        "else:\n",
        "    signals_df = signals_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "\n",
        "    per_stock_capital = INITIAL_CAPITAL / len(signals_df) if len(signals_df)>0 else 0\n",
        "    next_day_signals = []\n",
        "\n",
        "    for _, row in signals_df.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "\n",
        "        # ‚úÖ Try to get next day open price\n",
        "        next_open = symbol_close_0920_1529[sym][\"open_0920\"].get(next_trade_date_global, None)\n",
        "        if next_open is None:\n",
        "            # If next day price not available (future), use today close as reference for qty\n",
        "            next_open = row[\"TODAY_CLOSE\"]\n",
        "\n",
        "        qty = math.floor(per_stock_capital / next_open)\n",
        "        sl_price = round(next_open * (1 + INDIVIDUAL_SL_PCT), 2)\n",
        "\n",
        "        next_day_signals.append([\n",
        "            sym,\n",
        "            next_trade_date_global,\n",
        "            round(next_open,2),\n",
        "            qty,\n",
        "            round(qty * next_open, 2),\n",
        "            sl_price\n",
        "        ])\n",
        "\n",
        "    live_signal_df = pd.DataFrame(next_day_signals, columns=[\"SYMBOL\", \"NEXT_TRADE_DATE\", \"SELL_PRICE_0920\", \"QTY\", \"MARGIN_USED\", \"SL_PRICE\"])\n",
        "    live_signal_df.to_csv(\"LIVE_SIGNALS_NEXT_DAY.csv\", index=False)\n",
        "    print(\"‚úÖ Next-day live signals generated ‚Üí LIVE_SIGNALS_NEXT_DAY.csv\")\n",
        "    print(live_signal_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9qTnZ0gz4YR"
      },
      "source": [
        "# **Signal for VWAP+30day Close**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZF9TLx30BK3",
        "outputId": "4458aa60-6f3a-415c-ee26-f9d89d823c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 502 cash files...\n",
            "‚úÖ Processed 50/502 symbols\n",
            "‚úÖ Processed 100/502 symbols\n",
            "‚úÖ Processed 150/502 symbols\n",
            "‚úÖ Processed 200/502 symbols\n",
            "‚úÖ Processed 250/502 symbols\n",
            "‚úÖ Processed 300/502 symbols\n",
            "‚úÖ Processed 350/502 symbols\n",
            "‚úÖ Processed 400/502 symbols\n",
            "‚úÖ Processed 450/502 symbols\n",
            "‚úÖ Processed 500/502 symbols\n",
            "‚úÖ Loaded 502 symbols with required times\n",
            "‚úÖ Computed past-30-day mins (excluding today) for 502 symbols ‚Üí 265 trade dates\n",
            "‚úÖ Latest available date: 2025-08-25 00:00:00\n",
            "‚úÖ Next-day live signals generated ‚Üí LIVE_SIGNALS_NEXT_DAY.csv\n",
            "            SYMBOL NEXT_TRADE_DATE  SELL_PRICE_0919  QTY  MARGIN_USED  \\\n",
            "0    cash_DBREALTY      2025-08-26           174.62  357     62339.34   \n",
            "1  cash_NATCOPHARM      2025-08-26           873.00   71     61983.00   \n",
            "2        cash_BHEL      2025-08-26           216.51  288     62354.88   \n",
            "3        cash_IRFC      2025-08-26           124.29  502     62393.58   \n",
            "\n",
            "   SL_PRICE  \n",
            "0    175.32  \n",
            "1    876.49  \n",
            "2    217.38  \n",
            "3    124.79  \n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "INITIAL_CAPITAL = 250000       # starting capital for next-day trade\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # ‚úÖ dynamically pick START_TIME instead of hardcoded \"09:20\"\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start  = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# ‚úÖ Latest available date in DB\n",
        "today_date = unique_trade_dates[-1]\n",
        "print(f\"‚úÖ Latest available date: {today_date}\")\n",
        "\n",
        "# ‚úÖ Determine NEXT trading date after latest date\n",
        "next_trade_date_global = None\n",
        "for dt in unique_trade_dates:\n",
        "    if dt > today_date:\n",
        "        next_trade_date_global = dt\n",
        "        break\n",
        "# If no next date, assume next date = today_date + 1 day (future trade)\n",
        "if next_trade_date_global is None:\n",
        "    import datetime\n",
        "    next_trade_date_global = today_date + pd.Timedelta(days=1)\n",
        "\n",
        "# ‚úÖ Get breakdown signals for today_date\n",
        "signals_today = []\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    today_close = d[\"close_1529\"].get(today_date, None)\n",
        "    if today_close is None:\n",
        "        continue\n",
        "\n",
        "    lowest_low_prev30 = past30_min_dict[sym].get(today_date, None)\n",
        "    if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "        continue\n",
        "\n",
        "    # Compute daily VWAP\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_data = df_full.filter(pl.col(\"TradeDate\") == today_date)\n",
        "    if day_data.is_empty():\n",
        "        continue\n",
        "    typical_price = (day_data[\"High\"] + day_data[\"Low\"] + day_data[\"Close\"]) / 3\n",
        "    vwap_numerator = (typical_price * day_data[\"Volume\"]).sum()\n",
        "    vwap_denominator = day_data[\"Volume\"].sum()\n",
        "    if vwap_denominator == 0:\n",
        "        continue\n",
        "    vwap = vwap_numerator / vwap_denominator\n",
        "\n",
        "    if today_close < lowest_low_prev30 and today_close <= vwap * 1.002:\n",
        "        roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "        signals_today.append([sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "signals_df = pd.DataFrame(signals_today, columns=[\"SYMBOL\", \"TODAY_CLOSE\", \"PREV30_MIN\", \"ROI\"])\n",
        "if signals_df.empty:\n",
        "    print(\"‚ùå No breakdown signals for today\")\n",
        "else:\n",
        "    signals_df = signals_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "\n",
        "    per_stock_capital = INITIAL_CAPITAL / len(signals_df) if len(signals_df)>0 else 0\n",
        "    next_day_signals = []\n",
        "\n",
        "    for _, row in signals_df.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "\n",
        "        # ‚úÖ Try to get next day open price\n",
        "        next_open = symbol_close_start_end[sym][\"open_start\"].get(next_trade_date_global, None)\n",
        "        if next_open is None:\n",
        "            # If next day price not available (future), use today close as reference for qty\n",
        "            next_open = row[\"TODAY_CLOSE\"]\n",
        "\n",
        "        qty = math.floor(per_stock_capital / next_open)\n",
        "        sl_price = round(next_open * (1 + INDIVIDUAL_SL_PCT), 2)\n",
        "\n",
        "        next_day_signals.append([\n",
        "            sym,\n",
        "            next_trade_date_global,\n",
        "            round(next_open,2),\n",
        "            qty,\n",
        "            round(qty * next_open, 2),\n",
        "            sl_price\n",
        "        ])\n",
        "\n",
        "    live_signal_df = pd.DataFrame(next_day_signals, columns=[\"SYMBOL\", \"NEXT_TRADE_DATE\", \"SELL_PRICE_0919\", \"QTY\", \"MARGIN_USED\", \"SL_PRICE\"])\n",
        "    live_signal_df.to_csv(\"LIVE_SIGNALS_NEXT_DAY.csv\", index=False)\n",
        "    print(\"‚úÖ Next-day live signals generated ‚Üí LIVE_SIGNALS_NEXT_DAY.csv\")\n",
        "    print(live_signal_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpyI828K1yeR"
      },
      "source": [
        "# **30 day close Breakdown over FNO Symbols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS9uqlog14sv",
        "outputId": "ea0cb164-0dca-4895-e4a9-d08faa45e004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 252 common symbols. Saved to 'common_symbols.csv'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "fno_path = \"/content/drive/MyDrive/backup_main/NSE_FNO_Database\"\n",
        "cash_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "\n",
        "# 1. Get FNO symbols from folder names like: F_UnderlyingSymbol=INFY\n",
        "fno_folders = glob.glob(os.path.join(fno_path, \"F_UnderlyingSymbol=*\"))\n",
        "fno_symbols = [os.path.basename(p).split(\"=\")[-1] for p in fno_folders]\n",
        "\n",
        "# 2. Get Cash symbols from file names like: cash_INFY.csv\n",
        "cash_files = glob.glob(os.path.join(cash_path, \"cash_*.csv\"))\n",
        "cash_symbols = [os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") for f in cash_files]\n",
        "\n",
        "# 3. Find common symbols\n",
        "common_symbols = sorted(set(fno_symbols).intersection(set(cash_symbols)))\n",
        "\n",
        "# 4. Save to CSV\n",
        "pd.Series(common_symbols).to_csv(\"common_symbols.csv\", index=False, header=[\"SYMBOL\"])\n",
        "print(f\"‚úÖ Found {len(common_symbols)} common symbols. Saved to 'common_symbols.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X5e477T5Lzv",
        "outputId": "c66cc156-a7a7-4532-c08a-06850695dacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 248 cash files that match F&O symbols\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.015     # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.05   # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.03      # -3% portfolio SL\n",
        "START_TIME = \"09:20\"          # Trade entry time\n",
        "END_TIME = \"15:29\"            # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "# ‚úÖ Load filtered symbols\n",
        "common_symbols = pd.read_csv(\"common_symbols.csv\")[\"SYMBOL\"].tolist()\n",
        "\n",
        "# ‚úÖ Filter CSVs to only these symbols\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "print(f\"üöÄ Found {len(all_files)} cash files that match F&O symbols\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_0920  = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_0920_1529[symbol] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins (excluding today) for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = \"15:29\"\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ SL can trigger ONLY after entry time (09:20)\n",
        "        if cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    # Group by TRADE_DATE to get daily total PnL and ROI\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",  # average ROI per trade that day\n",
        "        \"SYMBOL\": \"count\"      # how many trades executed that day\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # ‚úÖ Optional cumulative PnL across days\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    # ‚úÖ Save as separate sheet\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oezGOchvBPfK"
      },
      "source": [
        "Lowest ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzdCZNJpBSMp",
        "outputId": "1e049b30-3723-41bd-fd3e-ceb6762e9e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 248 cash files that match F&O symbols\n",
            "‚úÖ Processed 50/248 symbols\n",
            "‚úÖ Processed 100/248 symbols\n",
            "‚úÖ Processed 150/248 symbols\n",
            "‚úÖ Processed 200/248 symbols\n",
            "‚úÖ Loaded 248 symbols with required times\n",
            "‚úÖ Computed past-30-day mins for 248 symbols ‚Üí 635 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 10294 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 2025 signals selected for trading\n",
            "‚úÖ Backtest completed. 2021 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ SL/Target Params\n",
        "INDIVIDUAL_SL_PCT = 0.015\n",
        "PORTFOLIO_TARGET_PCT = 0.05\n",
        "PORTFOLIO_SL_PCT = -0.03\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# ‚úÖ Paths\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "common_symbols = pd.read_csv(\"common_symbols.csv\")[\"SYMBOL\"].tolist()\n",
        "\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "print(f\"üöÄ Found {len(all_files)} cash files that match F&O symbols\")\n",
        "\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "def load_summary_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "\n",
        "\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return symbol, None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, close_1529, open_0920\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    sym, close_1529, open_0920 = load_summary_data(f)\n",
        "    if close_1529 is not None and open_0920 is not None:\n",
        "        symbol_close_0920_1529[sym] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=True).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "\n",
        "    if sym not in symbol_close_0920_1529:\n",
        "        continue\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # ‚úÖ Load full data only for this symbol\n",
        "    df_full = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        if cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_gQGcaBBNYB"
      },
      "source": [
        "Highest ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2SKtcszSQZK",
        "outputId": "c2a1699d-aa6c-4244-9d87-24b3dd1fd3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 248 cash files that match F&O symbols\n",
            "‚úÖ Processed 50/248 symbols\n",
            "‚úÖ Processed 100/248 symbols\n",
            "‚úÖ Processed 150/248 symbols\n",
            "‚úÖ Processed 200/248 symbols\n",
            "‚úÖ Loaded 248 symbols with required times\n",
            "‚úÖ Computed past-30-day mins for 248 symbols ‚Üí 635 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 10294 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 2025 signals selected for trading\n",
            "‚úÖ Backtest completed. 2021 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ SL/Target Params\n",
        "INDIVIDUAL_SL_PCT = 0.015\n",
        "PORTFOLIO_TARGET_PCT = 0.05\n",
        "PORTFOLIO_SL_PCT = -0.03\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# ‚úÖ Paths\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "common_symbols = pd.read_csv(\"common_symbols.csv\")[\"SYMBOL\"].tolist()\n",
        "\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "print(f\"üöÄ Found {len(all_files)} cash files that match F&O symbols\")\n",
        "\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "def load_summary_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "\n",
        "\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return symbol, None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, close_1529, open_0920\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    sym, close_1529, open_0920 = load_summary_data(f)\n",
        "    if close_1529 is not None and open_0920 is not None:\n",
        "        symbol_close_0920_1529[sym] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "\n",
        "    if sym not in symbol_close_0920_1529:\n",
        "        continue\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # ‚úÖ Load full data only for this symbol\n",
        "    df_full = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        if cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAWbUkLBkIt5"
      },
      "source": [
        "# **30 Day close Breakdown over Non FNO symbols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjzJSdrJJwEE",
        "outputId": "01638021-07bd-474f-e3e7-b8d501fa6608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 252 symbols only in Cash data. Saved to 'cash_only_symbols.csv'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "fno_path = \"/content/drive/MyDrive/backup_main/NSE_FNO_Database\"\n",
        "cash_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "\n",
        "# 1. Get FNO symbols from folder names like: F_UnderlyingSymbol=INFY\n",
        "fno_folders = glob.glob(os.path.join(fno_path, \"F_UnderlyingSymbol=*\"))\n",
        "fno_symbols = [os.path.basename(p).split(\"=\")[-1] for p in fno_folders]\n",
        "\n",
        "# 2. Get Cash symbols from file names like: cash_INFY.csv\n",
        "cash_files = glob.glob(os.path.join(cash_path, \"cash_*.csv\"))\n",
        "cash_symbols = [os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") for f in cash_files]\n",
        "\n",
        "# 3. Find symbols in Cash but NOT in FNO\n",
        "uncommon_symbols = sorted(set(cash_symbols) - set(fno_symbols))\n",
        "\n",
        "# 4. Save to CSV\n",
        "pd.Series(uncommon_symbols).to_csv(\"cash_only_symbols.csv\", index=False, header=[\"SYMBOL\"])\n",
        "print(f\"‚úÖ Found {len(uncommon_symbols)} symbols only in Cash data. Saved to 'cash_only_symbols.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HT0Sndlm5qr",
        "outputId": "ebbbdcb7-07c7-4f2e-996e-1b1f2bb7b745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 252 cash files that match F&O symbols\n",
            "‚úÖ Processed 50/252 symbols\n",
            "‚úÖ Processed 100/252 symbols\n",
            "‚úÖ Processed 150/252 symbols\n",
            "‚úÖ Processed 200/252 symbols\n",
            "‚úÖ Processed 250/252 symbols\n",
            "‚úÖ Loaded 252 symbols with required times\n",
            "‚úÖ Computed past-30-day mins for 252 symbols ‚Üí 635 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 9985 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 2011 signals selected for trading\n",
            "‚úÖ Backtest completed. 2007 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ SL/Target Params\n",
        "INDIVIDUAL_SL_PCT = 0.015\n",
        "PORTFOLIO_TARGET_PCT = 0.05\n",
        "PORTFOLIO_SL_PCT = -0.03\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:19\"\n",
        "\n",
        "# ‚úÖ Paths\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "common_symbols = pd.read_csv(\"cash_only_symbols.csv\")[\"SYMBOL\"].tolist()\n",
        "\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "print(f\"üöÄ Found {len(all_files)} cash files that match F&O symbols\")\n",
        "\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "def load_summary_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "\n",
        "\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return symbol, None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, close_1529, open_0920\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    sym, close_1529, open_0920 = load_summary_data(f)\n",
        "    if close_1529 is not None and open_0920 is not None:\n",
        "        symbol_close_0920_1529[sym] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_min_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "    roll_min_excl_today = close_series.rolling(30, min_periods=1).min().shift(1)\n",
        "    past30_min_dict[sym] = roll_min_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day mins for {len(past30_min_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_min_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        lowest_low_prev30 = past30_min_dict[sym].get(trade_date, None)\n",
        "        if lowest_low_prev30 is None or pd.isna(lowest_low_prev30):\n",
        "            continue\n",
        "\n",
        "        if today_close < lowest_low_prev30:\n",
        "            roi = round(((today_close / lowest_low_prev30) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, lowest_low_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MIN\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=True).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "\n",
        "    if sym not in symbol_close_0920_1529:\n",
        "        continue\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # ‚úÖ Load full data only for this symbol\n",
        "    df_full = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        if cur_price >= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(entry_price - exit_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"SELL_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_SELL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL_SELL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3T_QLV6cF4B"
      },
      "source": [
        "# **30 day close Breakout Bullish Intraday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVGhSN-ccM7x",
        "outputId": "12e57ca4-ed06-4446-c9f9-94f1bbe64d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed past-30-day max (excluding today) for 500 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Breakout scan finished ‚Üí Found 9648 breakout signals\n",
            "üìÑ Saved ALL breakouts with ROI ‚Üí ALL_BREAKOUTS.csv\n",
            "‚úÖ After ranking ‚Üí 898 signals selected for trading\n",
            "‚úÖ Backtest completed. 894 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL_BUY.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004     # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.05   # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.03      # -3% portfolio SL\n",
        "START_TIME = \"09:20\"          # Trade entry time\n",
        "END_TIME = \"15:20\"            # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_0920_1529 = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_0920  = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_0920_1529[symbol] = {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_0920_1529)} symbols with required times\")\n",
        "\n",
        "past30_max_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_0920_1529.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ rolling max of past 30 days (EXCLUDING today)\n",
        "    roll_max_excl_today = close_series.rolling(30, min_periods=1).max().shift(1)\n",
        "    past30_max_dict[sym] = roll_max_excl_today\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed past-30-day max (excluding today) for {len(past30_max_dict)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakouts = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        if sym not in past30_max_dict:\n",
        "            continue\n",
        "\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        highest_high_prev30 = past30_max_dict[sym].get(trade_date, None)\n",
        "        if highest_high_prev30 is None or pd.isna(highest_high_prev30):\n",
        "            continue\n",
        "\n",
        "        # ‚úÖ breakout: today_close > previous 30-day max\n",
        "        if today_close > highest_high_prev30:\n",
        "            roi = round(((today_close / highest_high_prev30) - 1) * 100, 2)\n",
        "            all_breakouts.append([trade_date, sym, today_close, highest_high_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakout scan finished ‚Üí Found {len(all_breakouts)} breakout signals\")\n",
        "\n",
        "breakout_df = pd.DataFrame(all_breakouts,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MAX\", \"ROI\"])\n",
        "breakout_df.to_csv(\"ALL_BREAKOUTS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakouts with ROI ‚Üí ALL_BREAKOUTS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakout_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # ‚úÖ Sort by lowest ROI ‚Üí top 4 smallest breakout moves\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=True).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ BUY next day 09:20\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)  # ‚úÖ SL for BUY is below entry\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ SL triggers if price drops below SL after entry\n",
        "        if cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)  # ‚úÖ BUY‚ÜíSELL PNL\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_0920\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST_BUY.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "# ‚úÖ Generate Daily PnL summary from executed trades\n",
        "if not output_df.empty:\n",
        "    # Group by TRADE_DATE ‚Üí sum PNL, average ROI, count trades\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # ‚úÖ Add running cumulative PnL\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    # ‚úÖ Save to CSV\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_BUY.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL_BUY.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kl1_ob8Es2F"
      },
      "source": [
        "VWAP ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKLW3AJPEvQ5",
        "outputId": "a732b2f9-503d-4e6c-ed91-cbc4f027acf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Identified 260 trade dates\n",
            "‚úÖ Breakdown scan finished ‚Üí Found 71043 breakdown signals\n",
            "üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 1040 signals selected for trading\n",
            "‚úÖ Backtest completed. 1036 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 1.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 5% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -3% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # ‚úÖ Dynamically pick START_TIME and \"15:29\"\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Identified {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        if today_close is None:\n",
        "            continue\n",
        "\n",
        "        # Compute daily VWAP\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_data = df_full.filter(pl.col(\"TradeDate\") == trade_date)\n",
        "        if day_data.is_empty():\n",
        "            continue\n",
        "        typical_price = (day_data[\"High\"] + day_data[\"Low\"] + day_data[\"Close\"]) / 3\n",
        "        vwap_numerator = (typical_price * day_data[\"Volume\"]).sum()\n",
        "        vwap_denominator = day_data[\"Volume\"].sum()\n",
        "        if vwap_denominator == 0:\n",
        "            continue\n",
        "        vwap = vwap_numerator / vwap_denominator\n",
        "\n",
        "        # Check if closing price is at least 99.8% of VWAP\n",
        "        if today_close >= vwap * 0.998:\n",
        "            # Calculate percentage difference from VWAP for ranking (positive means close is above VWAP)\n",
        "            roi = round(((today_close / vwap) - 1) * 100, 2)\n",
        "            all_breakdowns.append([trade_date, sym, today_close, vwap, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakdown scan finished ‚Üí Found {len(all_breakdowns)} breakdown signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"VWAP\", \"ROI\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL breakdowns with ROI ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Rank by highest increase (largest ROI, descending order since ROI is positive or small negative)\n",
        "    daily_sorted = daily_df.sort_values(\"ROI\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # ‚úÖ Entry price uses START_TIME variable\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # ‚úÖ Only monitor prices between START_TIME & END_TIME\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        exit_price = day_prices[day_prices[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_BUY.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DMPQEU3BL1p"
      },
      "source": [
        "# **30 Days close High breakout FNO Symbols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRRv8WcOsLgq",
        "outputId": "44fa9cb7-f267-4f46-a4e8-0e99c1432e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 250 common symbols. Saved to 'common_symbols.csv'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "fno_path = \"/content/drive/MyDrive/backup_main/NSE_FNO_Database\"\n",
        "cash_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "\n",
        "# 1. Get FNO symbols from folder names like: F_UnderlyingSymbol=INFY\n",
        "fno_folders = glob.glob(os.path.join(fno_path, \"F_UnderlyingSymbol=*\"))\n",
        "fno_symbols = [os.path.basename(p).split(\"=\")[-1] for p in fno_folders]\n",
        "\n",
        "# 2. Get Cash symbols from file names like: cash_INFY.csv\n",
        "cash_files = glob.glob(os.path.join(cash_path, \"cash_*.csv\"))\n",
        "cash_symbols = [os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") for f in cash_files]\n",
        "\n",
        "# 3. Find common symbols\n",
        "common_symbols = sorted(set(fno_symbols).intersection(set(cash_symbols)))\n",
        "\n",
        "# 4. Save to CSV\n",
        "pd.Series(common_symbols).to_csv(\"common_symbols.csv\", index=False, header=[\"SYMBOL\"])\n",
        "print(f\"‚úÖ Found {len(common_symbols)} common symbols. Saved to 'common_symbols.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HW-GH8yagCI"
      },
      "source": [
        "LowestROI Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo28s0qrsOAU",
        "outputId": "28362177-01ef-40a0-fe4f-eae661c15acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 250 matching cash files\n",
            "‚úÖ Processed 50/250\n",
            "‚úÖ Processed 100/250\n",
            "‚úÖ Processed 150/250\n",
            "‚úÖ Processed 200/250\n",
            "‚úÖ Processed 250/250\n",
            "‚úÖ Total symbols loaded: 250\n",
            "‚úÖ Breakouts found: 19114\n",
            "‚úÖ Ranked signals: 2315\n",
            "‚úÖ Executed trades: 2311 ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\n",
            "üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# === CONFIGURABLE PARAMETERS ===\n",
        "INDIVIDUAL_SL_PCT = 0.004\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# === PATHS ===\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "common_symbols_path = \"common_symbols.csv\"\n",
        "\n",
        "# === Load matching symbols\n",
        "common_symbols = pd.read_csv(common_symbols_path)[\"SYMBOL\"].tolist()\n",
        "\n",
        "# === Filter files to only common symbols\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "\n",
        "print(f\"üöÄ Found {len(all_files)} matching cash files\")\n",
        "\n",
        "# === Extract 09:20 & 15:29 closes for each symbol ===\n",
        "symbol_close_0920_1529 = {}\n",
        "past30_max_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "def extract_open_close(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "    df = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"),\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "# === Load data per symbol\n",
        "for i, file in enumerate(all_files, 1):\n",
        "    sym, data = extract_open_close(file)\n",
        "    if sym and data:\n",
        "        symbol_close_0920_1529[sym] = data\n",
        "        close_series = data[\"close_1529\"]\n",
        "        roll_max = close_series.rolling(30, min_periods=1).max().shift(1)\n",
        "        past30_max_dict[sym] = roll_max\n",
        "        all_dates.update(close_series.index)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)}\")\n",
        "\n",
        "print(f\"‚úÖ Total symbols loaded: {len(symbol_close_0920_1529)}\")\n",
        "\n",
        "# === Detect Breakouts\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakouts = []\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        highest_prev30 = past30_max_dict[sym].get(trade_date, None)\n",
        "        if today_close and highest_prev30 and today_close > highest_prev30:\n",
        "            roi = round(((today_close / highest_prev30) - 1) * 100, 2)\n",
        "            all_breakouts.append([trade_date, sym, today_close, highest_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakouts found: {len(all_breakouts)}\")\n",
        "\n",
        "breakout_df = pd.DataFrame(all_breakouts, columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MAX\", \"ROI\"])\n",
        "breakout_df.to_csv(\"ALL_BREAKOUTS.csv\", index=False)\n",
        "\n",
        "# === Rank Signals\n",
        "ranked_signals = []\n",
        "for signal_date, df_day in breakout_df.groupby(\"SIGNAL_DATE\"):\n",
        "    ranked_signals.append(df_day.sort_values(\"ROI\").head(4))\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ Ranked signals: {len(ranked_df)}\")\n",
        "\n",
        "# === Trade Simulation (RAM-EFFICIENT)\n",
        "output_trades = []\n",
        "cumulative_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # Get next trading date\n",
        "    date_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in date_list:\n",
        "        continue\n",
        "    idx = date_list.index(signal_date) + 1\n",
        "    if idx >= len(date_list):\n",
        "        continue\n",
        "    trade_date = date_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "    sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # === Load only that day's data lazily for SL monitoring\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "    df_day = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "        # Step-by-step to avoid ColumnNotFoundError\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "\n",
        "    df_day = df_day.filter(\n",
        "        (pl.col(\"TradeDate\") == trade_date) &\n",
        "        (pl.col(\"TradeTime\") >= START_TIME) &\n",
        "        (pl.col(\"TradeTime\") <= END_TIME)\n",
        "    ).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    if df_day.empty:\n",
        "        continue\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "    for _, r in df_day.iterrows():\n",
        "        if r[\"Close\"] <= sl_price:\n",
        "            exit_price = r[\"Close\"]\n",
        "            exit_reason = f\"SL@{r['TradeTime']}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        if END_TIME in df_day[\"TradeTime\"].values:\n",
        "            exit_price = df_day[df_day[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((pnl / entry_price) * 100, 2)\n",
        "    cumulative_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym, signal_date, trade_date,\n",
        "        entry_price, exit_price, pnl,\n",
        "        roi_trade, exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_return, 2)\n",
        "    ])\n",
        "\n",
        "# === Save All Trades\n",
        "output_df = pd.DataFrame(output_trades, columns=[\n",
        "    \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"BUY_0920\",\n",
        "    \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "    \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST_BUY.csv\", index=False)\n",
        "print(f\"‚úÖ Executed trades: {len(output_df)} ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\")\n",
        "\n",
        "# === Daily PnL Summary\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index().rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    })\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_BUY.csv\", index=False)\n",
        "    print(\"üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping PnL summary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8aX0KAyZpJP",
        "outputId": "a2614a28-d44b-4c33-9437-379e9999e9e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 250 matching cash files\n",
            "‚úÖ Processed 50/250\n",
            "‚úÖ Processed 100/250\n",
            "‚úÖ Processed 150/250\n",
            "‚úÖ Processed 200/250\n",
            "‚úÖ Processed 250/250\n",
            "‚úÖ Total symbols loaded: 250\n",
            "‚úÖ Breakouts found: 5196\n",
            "‚úÖ Ranked signals: 847\n",
            "‚úÖ Executed trades: 843 ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\n",
            "üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# === CONFIGURABLE PARAMETERS ===\n",
        "INDIVIDUAL_SL_PCT = 0.004\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# === PATHS ===\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "common_symbols_path = \"common_symbols.csv\"\n",
        "\n",
        "# === Load matching symbols\n",
        "common_symbols = pd.read_csv(common_symbols_path)[\"SYMBOL\"].tolist()\n",
        "\n",
        "# === Filter files to only common symbols\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"cash_*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "\n",
        "print(f\"üöÄ Found {len(all_files)} matching cash files\")\n",
        "\n",
        "# === Extract 09:20 & 15:29 closes for each symbol ===\n",
        "symbol_close_0920_1529 = {}\n",
        "past30_max_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "def extract_open_close(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "    df = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"),\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "# === Load data per symbol\n",
        "for i, file in enumerate(all_files, 1):\n",
        "    sym, data = extract_open_close(file)\n",
        "    if sym and data:\n",
        "        symbol_close_0920_1529[sym] = data\n",
        "        close_series = data[\"close_1529\"]\n",
        "        roll_max = close_series.rolling(30, min_periods=1).max().shift(1)\n",
        "        past30_max_dict[sym] = roll_max\n",
        "        all_dates.update(close_series.index)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)}\")\n",
        "\n",
        "print(f\"‚úÖ Total symbols loaded: {len(symbol_close_0920_1529)}\")\n",
        "\n",
        "# === Detect Breakouts\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakouts = []\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        highest_prev30 = past30_max_dict[sym].get(trade_date, None)\n",
        "        if today_close and highest_prev30 and today_close > highest_prev30:\n",
        "            roi = round(((today_close / highest_prev30) - 1) * 100, 2)\n",
        "            all_breakouts.append([trade_date, sym, today_close, highest_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakouts found: {len(all_breakouts)}\")\n",
        "\n",
        "breakout_df = pd.DataFrame(all_breakouts, columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MAX\", \"ROI\"])\n",
        "breakout_df.to_csv(\"ALL_BREAKOUTS.csv\", index=False)\n",
        "\n",
        "# === Rank Signals\n",
        "ranked_signals = []\n",
        "for signal_date, df_day in breakout_df.groupby(\"SIGNAL_DATE\"):\n",
        "    ranked_signals.append(df_day.sort_values(\"ROI\").head(4))\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ Ranked signals: {len(ranked_df)}\")\n",
        "\n",
        "# === Trade Simulation (RAM-EFFICIENT)\n",
        "output_trades = []\n",
        "cumulative_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # Get next trading date\n",
        "    date_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in date_list:\n",
        "        continue\n",
        "    idx = date_list.index(signal_date) + 1\n",
        "    if idx >= len(date_list):\n",
        "        continue\n",
        "    trade_date = date_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "    sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # === Load only that day's data lazily for SL monitoring\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "    df_day = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "        # Step-by-step to avoid ColumnNotFoundError\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "\n",
        "    df_day = df_day.filter(\n",
        "        (pl.col(\"TradeDate\") == trade_date) &\n",
        "        (pl.col(\"TradeTime\") >= START_TIME) &\n",
        "        (pl.col(\"TradeTime\") <= END_TIME)\n",
        "    ).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    if df_day.empty:\n",
        "        continue\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "    for _, r in df_day.iterrows():\n",
        "        if r[\"Close\"] <= sl_price:\n",
        "            exit_price = r[\"Close\"]\n",
        "            exit_reason = f\"SL@{r['TradeTime']}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        if END_TIME in df_day[\"TradeTime\"].values:\n",
        "            exit_price = df_day[df_day[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((pnl / entry_price) * 100, 2)\n",
        "    cumulative_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym, signal_date, trade_date,\n",
        "        entry_price, exit_price, pnl,\n",
        "        roi_trade, exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_return, 2)\n",
        "    ])\n",
        "\n",
        "# === Save All Trades\n",
        "output_df = pd.DataFrame(output_trades, columns=[\n",
        "    \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"BUY_0920\",\n",
        "    \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "    \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST_BUY.csv\", index=False)\n",
        "print(f\"‚úÖ Executed trades: {len(output_df)} ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\")\n",
        "\n",
        "# === Daily PnL Summary\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index().rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    })\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_BUY.csv\", index=False)\n",
        "    print(\"üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping PnL summary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XddaVQw9akn7"
      },
      "source": [
        "HighestROI Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT4stSSVanDF",
        "outputId": "00db354d-d4c2-4ac5-8e52-8e67ade17e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 248 matching cash files\n",
            "‚úÖ Processed 50/248\n",
            "‚úÖ Processed 100/248\n",
            "‚úÖ Processed 150/248\n",
            "‚úÖ Processed 200/248\n",
            "‚úÖ Total symbols loaded: 248\n",
            "‚úÖ Breakouts found: 18974\n",
            "‚úÖ Ranked signals: 2314\n",
            "‚úÖ Executed trades: 2310 ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\n",
            "üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# === CONFIGURABLE PARAMETERS ===\n",
        "INDIVIDUAL_SL_PCT = 0.015\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# === PATHS ===\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "common_symbols_path = \"common_symbols.csv\"\n",
        "\n",
        "# === Load matching symbols\n",
        "common_symbols = pd.read_csv(common_symbols_path)[\"SYMBOL\"].tolist()\n",
        "\n",
        "# === Filter files to only common symbols\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "\n",
        "print(f\"üöÄ Found {len(all_files)} matching cash files\")\n",
        "\n",
        "# === Extract 09:20 & 15:29 closes for each symbol ===\n",
        "symbol_close_0920_1529 = {}\n",
        "past30_max_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "def extract_open_close(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "    df = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"),\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "# === Load data per symbol\n",
        "for i, file in enumerate(all_files, 1):\n",
        "    sym, data = extract_open_close(file)\n",
        "    if sym and data:\n",
        "        symbol_close_0920_1529[sym] = data\n",
        "        close_series = data[\"close_1529\"]\n",
        "        roll_max = close_series.rolling(30, min_periods=1).max().shift(1)\n",
        "        past30_max_dict[sym] = roll_max\n",
        "        all_dates.update(close_series.index)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)}\")\n",
        "\n",
        "print(f\"‚úÖ Total symbols loaded: {len(symbol_close_0920_1529)}\")\n",
        "\n",
        "# === Detect Breakouts\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakouts = []\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        highest_prev30 = past30_max_dict[sym].get(trade_date, None)\n",
        "        if today_close and highest_prev30 and today_close > highest_prev30:\n",
        "            roi = round(((today_close / highest_prev30) - 1) * 100, 2)\n",
        "            all_breakouts.append([trade_date, sym, today_close, highest_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakouts found: {len(all_breakouts)}\")\n",
        "\n",
        "breakout_df = pd.DataFrame(all_breakouts, columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MAX\", \"ROI\"])\n",
        "breakout_df.to_csv(\"ALL_BREAKOUTS.csv\", index=False)\n",
        "\n",
        "# === Rank Signals\n",
        "ranked_signals = []\n",
        "for signal_date, df_day in breakout_df.groupby(\"SIGNAL_DATE\"):\n",
        "    ranked_signals.append(df_day.sort_values(\"ROI\",ascending=False).head(4))\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ Ranked signals: {len(ranked_df)}\")\n",
        "\n",
        "# === Trade Simulation (RAM-EFFICIENT)\n",
        "output_trades = []\n",
        "cumulative_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # Get next trading date\n",
        "    date_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in date_list:\n",
        "        continue\n",
        "    idx = date_list.index(signal_date) + 1\n",
        "    if idx >= len(date_list):\n",
        "        continue\n",
        "    trade_date = date_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "    sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # === Load only that day's data lazily for SL monitoring\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "    df_day = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "        # Step-by-step to avoid ColumnNotFoundError\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "\n",
        "    df_day = df_day.filter(\n",
        "        (pl.col(\"TradeDate\") == trade_date) &\n",
        "        (pl.col(\"TradeTime\") >= START_TIME) &\n",
        "        (pl.col(\"TradeTime\") <= END_TIME)\n",
        "    ).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    if df_day.empty:\n",
        "        continue\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "    for _, r in df_day.iterrows():\n",
        "        if r[\"Close\"] <= sl_price:\n",
        "            exit_price = r[\"Close\"]\n",
        "            exit_reason = f\"SL@{r['TradeTime']}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        if END_TIME in df_day[\"TradeTime\"].values:\n",
        "            exit_price = df_day[df_day[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((pnl / entry_price) * 100, 2)\n",
        "    cumulative_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym, signal_date, trade_date,\n",
        "        entry_price, exit_price, pnl,\n",
        "        roi_trade, exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_return, 2)\n",
        "    ])\n",
        "\n",
        "# === Save All Trades\n",
        "output_df = pd.DataFrame(output_trades, columns=[\n",
        "    \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"BUY_0920\",\n",
        "    \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "    \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST_BUY.csv\", index=False)\n",
        "print(f\"‚úÖ Executed trades: {len(output_df)} ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\")\n",
        "\n",
        "# === Daily PnL Summary\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index().rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    })\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_BUY.csv\", index=False)\n",
        "    print(\"üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping PnL summary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJt2iSegh4cx"
      },
      "source": [
        "# 30 Day close Breakout Non FNO Symbol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDbKe4BUiBdr",
        "outputId": "6aecc8be-ec86-4d84-8854-acd487282019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 252 symbols only in Cash data. Saved to 'cash_only_symbols.csv'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "fno_path = \"/content/drive/MyDrive/backup_main/NSE_FNO_Database\"\n",
        "cash_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "\n",
        "# 1. Get FNO symbols from folder names like: F_UnderlyingSymbol=INFY\n",
        "fno_folders = glob.glob(os.path.join(fno_path, \"F_UnderlyingSymbol=*\"))\n",
        "fno_symbols = [os.path.basename(p).split(\"=\")[-1] for p in fno_folders]\n",
        "\n",
        "# 2. Get Cash symbols from file names like: cash_INFY.csv\n",
        "cash_files = glob.glob(os.path.join(cash_path, \"cash_*.csv\"))\n",
        "cash_symbols = [os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") for f in cash_files]\n",
        "\n",
        "# 3. Find symbols in Cash but NOT in FNO\n",
        "uncommon_symbols = sorted(set(cash_symbols) - set(fno_symbols))\n",
        "\n",
        "# 4. Save to CSV\n",
        "pd.Series(uncommon_symbols).to_csv(\"cash_only_symbols.csv\", index=False, header=[\"SYMBOL\"])\n",
        "print(f\"‚úÖ Found {len(uncommon_symbols)} symbols only in Cash data. Saved to 'cash_only_symbols.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rub5CTwKh9v8",
        "outputId": "02698731-c299-4d05-e56d-206394cd1b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 252 matching cash files\n",
            "‚úÖ Processed 50/252\n",
            "‚úÖ Processed 100/252\n",
            "‚úÖ Processed 150/252\n",
            "‚úÖ Processed 200/252\n",
            "‚úÖ Processed 250/252\n",
            "‚úÖ Total symbols loaded: 252\n",
            "‚úÖ Breakouts found: 15586\n",
            "‚úÖ Ranked signals: 2313\n",
            "‚úÖ Executed trades: 2309 ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\n",
            "üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# === CONFIGURABLE PARAMETERS ===\n",
        "INDIVIDUAL_SL_PCT = 0.015\n",
        "START_TIME = \"09:20\"\n",
        "END_TIME = \"15:20\"\n",
        "\n",
        "# === PATHS ===\n",
        "data_path = \"/content/drive/MyDrive/Cash_data2\"\n",
        "common_symbols_path = \"cash_only_symbols.csv\"\n",
        "\n",
        "# === Load matching symbols\n",
        "common_symbols = pd.read_csv(common_symbols_path)[\"SYMBOL\"].tolist()\n",
        "\n",
        "# === Filter files to only common symbols\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "\n",
        "print(f\"üöÄ Found {len(all_files)} matching cash files\")\n",
        "\n",
        "# === Extract 09:20 & 15:29 closes for each symbol ===\n",
        "symbol_close_0920_1529 = {}\n",
        "past30_max_dict = {}\n",
        "all_dates = set()\n",
        "\n",
        "def extract_open_close(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0].replace(\"cash_\", \"\")\n",
        "    df = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"),\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:20\", \"15:29\"]))\n",
        "    if df_sel.is_empty():\n",
        "        return None, None\n",
        "\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    open_0920 = pdf[pdf[\"TradeTime\"] == \"09:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    return symbol, {\"close_1529\": close_1529, \"open_0920\": open_0920}\n",
        "\n",
        "# === Load data per symbol\n",
        "for i, file in enumerate(all_files, 1):\n",
        "    sym, data = extract_open_close(file)\n",
        "    if sym and data:\n",
        "        symbol_close_0920_1529[sym] = data\n",
        "        close_series = data[\"close_1529\"]\n",
        "        roll_max = close_series.rolling(30, min_periods=1).max().shift(1)\n",
        "        past30_max_dict[sym] = roll_max\n",
        "        all_dates.update(close_series.index)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)}\")\n",
        "\n",
        "print(f\"‚úÖ Total symbols loaded: {len(symbol_close_0920_1529)}\")\n",
        "\n",
        "# === Detect Breakouts\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "if len(unique_trade_dates) > 31:\n",
        "    unique_trade_dates = unique_trade_dates[31:]\n",
        "\n",
        "all_breakouts = []\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym, d in symbol_close_0920_1529.items():\n",
        "        today_close = d[\"close_1529\"].get(trade_date, None)\n",
        "        highest_prev30 = past30_max_dict[sym].get(trade_date, None)\n",
        "        if today_close and highest_prev30 and today_close > highest_prev30:\n",
        "            roi = round(((today_close / highest_prev30) - 1) * 100, 2)\n",
        "            all_breakouts.append([trade_date, sym, today_close, highest_prev30, roi])\n",
        "\n",
        "print(f\"‚úÖ Breakouts found: {len(all_breakouts)}\")\n",
        "\n",
        "breakout_df = pd.DataFrame(all_breakouts, columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PREV30_MAX\", \"ROI\"])\n",
        "breakout_df.to_csv(\"ALL_BREAKOUTS.csv\", index=False)\n",
        "\n",
        "# === Rank Signals\n",
        "ranked_signals = []\n",
        "for signal_date, df_day in breakout_df.groupby(\"SIGNAL_DATE\"):\n",
        "    ranked_signals.append(df_day.sort_values(\"ROI\").head(4))\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ Ranked signals: {len(ranked_df)}\")\n",
        "\n",
        "# === Trade Simulation (RAM-EFFICIENT)\n",
        "output_trades = []\n",
        "cumulative_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # Get next trading date\n",
        "    date_list = sorted(symbol_close_0920_1529[sym][\"close_1529\"].index)\n",
        "    if signal_date not in date_list:\n",
        "        continue\n",
        "    idx = date_list.index(signal_date) + 1\n",
        "    if idx >= len(date_list):\n",
        "        continue\n",
        "    trade_date = date_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_0920_1529[sym][\"open_0920\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "    sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # === Load only that day's data lazily for SL monitoring\n",
        "    file_path = os.path.join(data_path, f\"cash_{sym}.csv\")\n",
        "    df_day = pl.read_csv(file_path).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "        # Step-by-step to avoid ColumnNotFoundError\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ])\n",
        "\n",
        "    df_day = df_day.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "\n",
        "    df_day = df_day.filter(\n",
        "        (pl.col(\"TradeDate\") == trade_date) &\n",
        "        (pl.col(\"TradeTime\") >= START_TIME) &\n",
        "        (pl.col(\"TradeTime\") <= END_TIME)\n",
        "    ).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    if df_day.empty:\n",
        "        continue\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = \"15:29\"\n",
        "    for _, r in df_day.iterrows():\n",
        "        if r[\"Close\"] <= sl_price:\n",
        "            exit_price = r[\"Close\"]\n",
        "            exit_reason = f\"SL@{r['TradeTime']}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        if END_TIME in df_day[\"TradeTime\"].values:\n",
        "            exit_price = df_day[df_day[\"TradeTime\"] == END_TIME][\"Close\"].values[0]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((pnl / entry_price) * 100, 2)\n",
        "    cumulative_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym, signal_date, trade_date,\n",
        "        entry_price, exit_price, pnl,\n",
        "        roi_trade, exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_return, 2)\n",
        "    ])\n",
        "\n",
        "# === Save All Trades\n",
        "output_df = pd.DataFrame(output_trades, columns=[\n",
        "    \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"BUY_0920\",\n",
        "    \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "    \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST_BUY.csv\", index=False)\n",
        "print(f\"‚úÖ Executed trades: {len(output_df)} ‚Üí Saved in OUTPUT_BACKTEST_BUY.csv\")\n",
        "\n",
        "# === Daily PnL Summary\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index().rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    })\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_BUY.csv\", index=False)\n",
        "    print(\"üìÑ Daily PnL saved ‚Üí DAILY_PNL_BUY.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping PnL summary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBK7tr26OLtt"
      },
      "source": [
        "# **Live Signal for 30 days Breakout**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjHJlyOhORjz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sba0zhCiOSHs"
      },
      "source": [
        "# **Intraday Buy NSNT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdELLfqiSUYq",
        "outputId": "a26e99d9-cc59-4a70-ba13-c130ba2836c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 502 cash files...\n",
            "‚úÖ Processed 50/502 symbols\n",
            "‚úÖ Processed 100/502 symbols\n",
            "‚úÖ Processed 150/502 symbols\n",
            "‚úÖ Processed 200/502 symbols\n",
            "‚úÖ Processed 250/502 symbols\n",
            "‚úÖ Processed 300/502 symbols\n",
            "‚úÖ Processed 350/502 symbols\n",
            "‚úÖ Processed 400/502 symbols\n",
            "‚úÖ Processed 450/502 symbols\n",
            "‚úÖ Processed 500/502 symbols\n",
            "‚úÖ Loaded 502 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Computed daily metrics for 487 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 508 momentum signals\n",
            "üìÑ Saved ALL momentum signals with prev/start/NIFTY500 ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 40 signals selected for trading\n",
            "‚úÖ Backtest completed. 36 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params (unchanged)\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files (unchanged)\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (change if filename differs)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory (like you had)\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # select rows for START_TIME and prev-day 15:29 (we store series with index TradeDate)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # prev day 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close/open (09:19): indexed by TradeDate\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time 09:19) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build per-symbol daily metrics from 15:29 close series (like you had)\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(252)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list using your momentum filter but augmented with prev/start close and ROI & NIFTY500 ROI -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            # require 12m ret available (same as you)\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            # momentum / ema conditions (unchanged)\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                # fetch prev day 15:29 close and current day start time close\n",
        "                prev_close = None\n",
        "                start_close = None\n",
        "                try:\n",
        "                    prev_close = float(symbol_close_start_end[sym][\"close_1529\"].loc[trade_date])\n",
        "                except Exception:\n",
        "                    prev_close = None\n",
        "                try:\n",
        "                    start_close = float(symbol_close_start_end[sym][\"open_start\"].loc[trade_date])\n",
        "                except Exception:\n",
        "                    start_close = None\n",
        "\n",
        "                # If either missing skip (we need both to compute ROI)\n",
        "                if prev_close is None or start_close is None or start_close == 0:\n",
        "                    continue\n",
        "\n",
        "                # ROI formula as requested: (start_time_close - prev_day_15:29_close) / start_time_close * 100\n",
        "                roi_pct = ((start_close - prev_close) / start_close) * 100.0\n",
        "\n",
        "                # NIFTY500 ROI for same trade_date (if available)\n",
        "                nifty_roi = None\n",
        "                if nifty500_close_1529 is not None and nifty500_open_start is not None:\n",
        "                    try:\n",
        "                        nifty_prev = float(nifty500_close_1529.loc[trade_date])\n",
        "                        nifty_start = float(nifty500_open_start.loc[trade_date])\n",
        "                        if nifty_start != 0:\n",
        "                            nifty_roi = ((nifty_start - nifty_prev) / nifty_start) * 100.0\n",
        "                    except Exception:\n",
        "                        nifty_roi = None\n",
        "\n",
        "                all_breakdowns.append([\n",
        "                    trade_date, sym,\n",
        "                    prev_close,\n",
        "                    start_close,\n",
        "                    roi_pct,\n",
        "                    nifty_roi\n",
        "                ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with requested fields (and include NIFTY500 ROI)\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0919\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "\n",
        "# Ensure numeric formatting\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(4)\n",
        "if \"NIFTY500_ROI_%\" in breakdown_df.columns:\n",
        "    breakdown_df[\"NIFTY500_ROI_%\"] = breakdown_df[\"NIFTY500_ROI_%\"].astype(float).round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with prev/start/NIFTY500 ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "# ----- Ranking logic per your new rule: compare each SYMBOL ROI vs NIFTY500 ROI on that SIGNAL_DATE -----\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # find the NIFTY ROI for that date (take first non-null)\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    nifty_roi_for_date = float(nifty_vals[0]) if len(nifty_vals) > 0 else None\n",
        "\n",
        "    # If NIFTY ROI known:\n",
        "    if nifty_roi_for_date is not None:\n",
        "        if nifty_roi_for_date > 0:\n",
        "            # NIFTY up -> pick highest outperformers (largest positive ROI)\n",
        "            daily_sorted = daily_df.sort_values(\"ROI_%\", ascending=False).head(4)\n",
        "        else:\n",
        "            # NIFTY down or zero -> pick biggest underperformers (lowest ROI)\n",
        "            daily_sorted = daily_df.sort_values(\"ROI_%\", ascending=True).head(4)\n",
        "    else:\n",
        "        # If NIFTY ROI is missing, fallback to picking highest ROI (conservative)\n",
        "        daily_sorted = daily_df.sort_values(\"ROI_%\", ascending=False).head(4)\n",
        "\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns)\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "# ----- Backtest/execution loop (kept structure & logic unchanged) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # get dates list for the symbol (from the stored prev close series indices)\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1  # trade happens next available day after signal\n",
        "    if idx >= len(dates_list):\n",
        "        # no next day to trade\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # entry price = START_TIME close on trade_date\n",
        "    entry_price = None\n",
        "    try:\n",
        "        entry_price = float(symbol_close_start_end[sym][\"open_start\"].get(trade_date, None))\n",
        "    except Exception:\n",
        "        entry_price = None\n",
        "\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # pull full-day minute prices for trade_date\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades (unchanged)\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCv_w_n9Yarg",
        "outputId": "2bb994bd-11ca-4875-aed6-2a464b532316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 502 cash files...\n",
            "‚úÖ Processed 50/502 symbols\n",
            "‚úÖ Processed 100/502 symbols\n",
            "‚úÖ Processed 150/502 symbols\n",
            "‚úÖ Processed 200/502 symbols\n",
            "‚úÖ Processed 250/502 symbols\n",
            "‚úÖ Processed 300/502 symbols\n",
            "‚úÖ Processed 350/502 symbols\n",
            "‚úÖ Processed 400/502 symbols\n",
            "‚úÖ Processed 450/502 symbols\n",
            "‚úÖ Processed 500/502 symbols\n",
            "‚úÖ Loaded 502 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Computed daily metrics for 487 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 6336 momentum signals\n",
            "üìÑ Saved ALL momentum signals with prev/start/NIFTY500 ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 320 signals selected for trading\n",
            "‚úÖ Backtest completed. 316 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params (unchanged)\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:30\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files (unchanged)\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (change if filename differs)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory (like you had)\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # select rows for START_TIME and prev-day 15:29 (we store series with index TradeDate)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # prev day 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close/open (09:19): indexed by TradeDate\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time 09:19) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build per-symbol daily metrics from 15:29 close series (like you had)\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(31)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(62)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(92)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(182)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list using your momentum filter but augmented with prev/start close and ROI & NIFTY500 ROI -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            # require 12m ret available (same as you)\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            # momentum / ema conditions (unchanged)\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                # fetch prev day 15:29 close and current day start time close\n",
        "                prev_close = None\n",
        "                start_close = None\n",
        "                try:\n",
        "                    prev_close = float(symbol_close_start_end[sym][\"close_1529\"].loc[trade_date])\n",
        "                except Exception:\n",
        "                    prev_close = None\n",
        "                try:\n",
        "                    start_close = float(symbol_close_start_end[sym][\"open_start\"].loc[trade_date])\n",
        "                except Exception:\n",
        "                    start_close = None\n",
        "\n",
        "                # If either missing skip (we need both to compute ROI)\n",
        "                if prev_close is None or start_close is None or start_close == 0:\n",
        "                    continue\n",
        "\n",
        "                # ROI formula as requested: (start_time_close - prev_day_15:29_close) / start_time_close * 100\n",
        "                roi_pct = ((start_close - prev_close) / start_close) * 100.0\n",
        "\n",
        "                # NIFTY500 ROI for same trade_date (if available)\n",
        "                nifty_roi = None\n",
        "                if nifty500_close_1529 is not None and nifty500_open_start is not None:\n",
        "                    try:\n",
        "                        nifty_prev = float(nifty500_close_1529.loc[trade_date])\n",
        "                        nifty_start = float(nifty500_open_start.loc[trade_date])\n",
        "                        if nifty_start != 0:\n",
        "                            nifty_roi = ((nifty_start - nifty_prev) / nifty_start) * 100.0\n",
        "                    except Exception:\n",
        "                        nifty_roi = None\n",
        "\n",
        "                all_breakdowns.append([\n",
        "                    trade_date, sym,\n",
        "                    prev_close,\n",
        "                    start_close,\n",
        "                    roi_pct,\n",
        "                    nifty_roi\n",
        "                ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with requested fields (and include NIFTY500 ROI)\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0919\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "\n",
        "# Ensure numeric formatting\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(4)\n",
        "if \"NIFTY500_ROI_%\" in breakdown_df.columns:\n",
        "    breakdown_df[\"NIFTY500_ROI_%\"] = breakdown_df[\"NIFTY500_ROI_%\"].astype(float).round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with prev/start/NIFTY500 ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "# ----- Ranking logic per your new rule: compare each SYMBOL ROI vs NIFTY500 ROI on that SIGNAL_DATE -----\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # find the NIFTY ROI for that date (take first non-null)\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    nifty_roi_for_date = float(nifty_vals[0]) if len(nifty_vals) > 0 else None\n",
        "\n",
        "    # If NIFTY ROI known:\n",
        "    if nifty_roi_for_date is not None:\n",
        "        if nifty_roi_for_date > 0:\n",
        "            # NIFTY up -> pick highest outperformers (largest positive ROI)\n",
        "            daily_sorted = daily_df.sort_values(\"ROI_%\", ascending=False).head(4)\n",
        "        else:\n",
        "            # NIFTY down or zero -> pick biggest underperformers (lowest ROI)\n",
        "            daily_sorted = daily_df.sort_values(\"ROI_%\", ascending=True).head(4)\n",
        "    else:\n",
        "        # If NIFTY ROI is missing, fallback to picking highest ROI (conservative)\n",
        "        daily_sorted = daily_df.sort_values(\"ROI_%\", ascending=False).head(4)\n",
        "\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns)\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "# ----- Backtest/execution loop (kept structure & logic unchanged) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # get dates list for the symbol (from the stored prev close series indices)\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1  # trade happens next available day after signal\n",
        "    if idx >= len(dates_list):\n",
        "        # no next day to trade\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # entry price = START_TIME close on trade_date\n",
        "    entry_price = None\n",
        "    try:\n",
        "        entry_price = float(symbol_close_start_end[sym][\"open_start\"].get(trade_date, None))\n",
        "    except Exception:\n",
        "        entry_price = None\n",
        "\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # pull full-day minute prices for trade_date\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades (unchanged)\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHyew-3cj3ri",
        "outputId": "e601f464-dff3-4019-d688-c82bc7d4f3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 508 momentum signals\n",
            "üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 150 signals selected for trading\n",
            "‚úÖ Backtest completed. 135 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(252)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Removed the date restriction to use all available dates\n",
        "# if len(unique_trade_dates) > 253:\n",
        "#     unique_trade_dates = unique_trade_dates[252:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], row['ret_1m']])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"RET_1M\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"RET_1M\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VeAsoDUbvUr",
        "outputId": "d028ea35-870f-451c-861e-a428de3911fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 261 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 29050 momentum signals\n",
            "üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 920 signals selected for trading\n",
            "‚úÖ Backtest completed. 916 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1   #21\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(25)) - 1  #63\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(28)) - 1  #126\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(31)) - 1 #252\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Removed the date restriction to use all available dates\n",
        "# if len(unique_trade_dates) > 253:\n",
        "#     unique_trade_dates = unique_trade_dates[252:]\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], row['ret_1m']])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"RET_1M\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"RET_1M\", ascending=True).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUHfzQOKojjT"
      },
      "source": [
        "# NSNT INTRA LOWEST BUY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtWcAWCbvMws",
        "outputId": "a5305c8e-02e1-48be-d291-50691fd5fa40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 508 momentum signals\n",
            "üìÑ Saved ALL momentum signals with Prev Close, Start-Time Close, ROI% ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking (by ROI% decrease) ‚Üí 36 signals selected for trading\n",
            "‚úÖ Backtest completed. 36 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:30\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"10:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(252)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Removed the date restriction to use all available dates\n",
        "# if len(unique_trade_dates) > 253:\n",
        "#     unique_trade_dates = unique_trade_dates[252:]\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Momentum scan (unchanged logic) ‚Üí collect signals by filters\n",
        "# -------------------------------------------------------------\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (\n",
        "                row['ret_1m'] > 0 and row['ret_3m'] > 0 and\n",
        "                row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']\n",
        "            ):\n",
        "                # Keep same payload shape, we‚Äôll enrich later\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], row['ret_1m']])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Build ALL_BREAKDOWNS.csv enriched with prev close, start-time\n",
        "# close (next trading day), and ROI% = (start - prev)/start*100\n",
        "# -------------------------------------------------------------\n",
        "breakdown_cols = [\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"RET_1M\"]\n",
        "breakdown_df = pd.DataFrame(all_breakdowns, columns=breakdown_cols)\n",
        "\n",
        "# Enrich per row with TRADE_DATE (next trading day), START_TIME_CLOSE, ROI%\n",
        "enriched_rows = []\n",
        "for _, r in breakdown_df.iterrows():\n",
        "    signal_date = r[\"SIGNAL_DATE\"]\n",
        "    sym = r[\"SYMBOL\"]\n",
        "    prev_close = r[\"CLOSE_1529\"]\n",
        "\n",
        "    # Find next trading day for this symbol (the actual trade date)\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        trade_date = pd.NaT\n",
        "        start_time_close = float('nan')\n",
        "        roi_gap_pct = float('nan')\n",
        "    else:\n",
        "        idx = dates_list.index(signal_date) + 1\n",
        "        if idx >= len(dates_list):\n",
        "            trade_date = pd.NaT\n",
        "            start_time_close = float('nan')\n",
        "            roi_gap_pct = float('nan')\n",
        "        else:\n",
        "            trade_date = dates_list[idx]\n",
        "            start_time_close = symbol_close_start_end[sym][\"open_start\"].get(trade_date, float('nan'))\n",
        "            if pd.isna(start_time_close):\n",
        "                roi_gap_pct = float('nan')\n",
        "            else:\n",
        "                # % change from prev day's 15:29 close to current day's START_TIME close\n",
        "                # (start - prev) / start * 100\n",
        "                roi_gap_pct = ((start_time_close - prev_close) / start_time_close) * 100.0\n",
        "\n",
        "    enriched_rows.append({\n",
        "        \"SIGNAL_DATE\": signal_date,\n",
        "        \"SYMBOL\": sym,\n",
        "        \"PREV_CLOSE_1529\": prev_close,\n",
        "        \"TRADE_DATE\": trade_date,\n",
        "        \"START_TIME_CLOSE\": start_time_close,\n",
        "        \"ROI%\": None if pd.isna(roi_gap_pct) else round(roi_gap_pct, 4),  # as requested\n",
        "        \"RET_1M\": r[\"RET_1M\"]  # retained for reference\n",
        "    })\n",
        "\n",
        "enriched_breakdown_df = pd.DataFrame(enriched_rows, columns=[\n",
        "    \"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"TRADE_DATE\",\n",
        "    \"START_TIME_CLOSE\", \"ROI%\", \"RET_1M\"\n",
        "])\n",
        "\n",
        "enriched_breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with Prev Close, Start-Time Close, ROI% ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Ranking per SIGNAL_DATE based on highest decrease first\n",
        "# i.e., sort ascending by ROI% (most negative values first)\n",
        "# -------------------------------------------------------------\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in enriched_breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Only rank rows that have a valid ROI%\n",
        "    valid = daily_df.dropna(subset=[\"ROI%\"])\n",
        "    # Sort ascending (most negative ‚Äî biggest decrease ‚Äî comes first)\n",
        "    daily_sorted = valid.sort_values(\"ROI%\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "if len(ranked_signals) > 0:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=enriched_breakdown_df.columns)\n",
        "\n",
        "print(f\"‚úÖ After ranking (by ROI% decrease) ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Execute trades: buy at START_TIME on TRADE_DATE, exit per rules\n",
        "# (Trade logic unchanged)\n",
        "# -------------------------------------------------------------\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    # Derive the actual trade_date (next session after SIGNAL_DATE)\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # Entry price at START_TIME\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None or pd.isna(entry_price):\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Individual SL price (buy strategy ‚Üí SL below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # Get full day minute data and slice by trade window\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades (unchanged)\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY--Z5_4ls51",
        "outputId": "8cfd5fb9-567d-40be-a6c9-a66183731b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 508 momentum signals\n",
            "üìÑ Saved ALL momentum signals with PCT_CHANGE ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 40 signals selected for trading\n",
            "‚úÖ Backtest completed. 36 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:19\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(252)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                # Calculate percentage change from previous day's 15:29 to current day's start\n",
        "                dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "                if trade_date not in dates_list:\n",
        "                    continue\n",
        "                idx = dates_list.index(trade_date)\n",
        "                if idx == 0:  # No previous day available\n",
        "                    continue\n",
        "                prev_date = dates_list[idx - 1]\n",
        "                prev_close = symbol_close_start_end[sym][\"close_1529\"].get(prev_date, None)\n",
        "                curr_open = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "                if prev_close is None or curr_open is None:\n",
        "                    continue\n",
        "                pct_change = (curr_open - prev_close) / prev_close\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], pct_change])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PCT_CHANGE\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with PCT_CHANGE ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"PCT_CHANGE\", ascending=False).head(4)  # Highest decrease first\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6PjuNOoonzo",
        "outputId": "904daeb1-d57d-4eb0-a459-776a9fe0c77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 262 trade dates\n",
            "‚úÖ Momentum scan finished ‚Üí Found 4685 momentum signals\n",
            "üìÑ Saved ALL momentum signals with PCT_CHANGE ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 248 signals selected for trading\n",
            "‚úÖ Backtest completed. 244 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.015       # 5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.10    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01      # -1% portfolio SL\n",
        "START_TIME = \"09:20\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:20\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "symbol_daily_data = {}\n",
        "all_dates = set()\n",
        "\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    close_series = d[\"close_1529\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(200)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "    all_dates.update(close_series.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                # Calculate percentage change from previous day's 15:29 to current day's start\n",
        "                dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "                if trade_date not in dates_list:\n",
        "                    continue\n",
        "                idx = dates_list.index(trade_date)\n",
        "                if idx == 0:  # No previous day available\n",
        "                    continue\n",
        "                prev_date = dates_list[idx - 1]\n",
        "                prev_close = symbol_close_start_end[sym][\"close_1529\"].get(prev_date, None)\n",
        "                curr_open = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "                if prev_close is None or curr_open is None:\n",
        "                    continue\n",
        "                pct_change = (curr_open - prev_close) / prev_close\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], pct_change])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"PCT_CHANGE\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with PCT_CHANGE ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"PCT_CHANGE\", ascending=False).head(4)  # Highest decrease first\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "# Group trades by trade date to process portfolio-level SL and target\n",
        "trades_by_date = ranked_df.groupby(\"SIGNAL_DATE\")\n",
        "\n",
        "for signal_date, trades in trades_by_date:\n",
        "    trade_date = None\n",
        "    daily_trades = []\n",
        "    portfolio_exit_triggered = False\n",
        "    portfolio_exit_time = None\n",
        "    portfolio_exit_reason = None\n",
        "\n",
        "    # Get trade date (next day after signal)\n",
        "    dates_list = sorted(all_dates)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    # Get all symbols and their entry prices for the trade date\n",
        "    trade_symbols = trades[\"SYMBOL\"].tolist()\n",
        "    entry_prices = {}\n",
        "    for sym in trade_symbols:\n",
        "        entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "        if entry_price is None:\n",
        "            continue\n",
        "        entry_prices[sym] = entry_price\n",
        "\n",
        "    if not entry_prices:\n",
        "        continue\n",
        "\n",
        "    # Get all minute-by-minute prices for the trade date across all symbols\n",
        "    all_day_prices = []\n",
        "    for sym in trade_symbols:\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"SYMBOL\"] = sym\n",
        "        all_day_prices.append(day_prices)\n",
        "\n",
        "    if not all_day_prices:\n",
        "        continue\n",
        "    all_day_prices = pd.concat(all_day_prices).sort_values(\"TradeTime\")\n",
        "\n",
        "    # Get unique times to iterate over\n",
        "    unique_times = all_day_prices[\"TradeTime\"].unique()\n",
        "    unique_times = sorted([t for t in unique_times if START_TIME <= t <= END_TIME])\n",
        "\n",
        "    # Track active trades and their stop-loss prices\n",
        "    active_trades = {sym: {\"entry_price\": entry_prices[sym],\n",
        "                           \"indiv_sl_price\": entry_prices[sym] * (1 - INDIVIDUAL_SL_PCT),\n",
        "                           \"exit_price\": None,\n",
        "                           \"exit_reason\": END_TIME} for sym in trade_symbols}\n",
        "\n",
        "    # Monitor portfolio PnL minute by minute\n",
        "    for cur_time in unique_times:\n",
        "        if portfolio_exit_triggered:\n",
        "            break\n",
        "\n",
        "        minute_prices = all_day_prices[all_day_prices[\"TradeTime\"] == cur_time]\n",
        "        total_pnl_pct = 0.0\n",
        "        num_active_trades = len([s for s, t in active_trades.items() if t[\"exit_price\"] is None])\n",
        "\n",
        "        if num_active_trades == 0:\n",
        "            break\n",
        "\n",
        "        # Calculate portfolio PnL for active trades\n",
        "        for sym in trade_symbols:\n",
        "            if active_trades[sym][\"exit_price\"] is not None:\n",
        "                continue  # Skip trades already exited\n",
        "\n",
        "            minute_price = minute_prices[minute_prices[\"SYMBOL\"] == sym][\"Close\"]\n",
        "            if minute_price.empty:\n",
        "                continue\n",
        "\n",
        "            cur_price = minute_price.values[0]\n",
        "            entry_price = active_trades[sym][\"entry_price\"]\n",
        "            trade_pnl_pct = (cur_price - entry_price) / entry_price\n",
        "\n",
        "            # Check individual stop-loss (after SL_ACTIVATION_TIME)\n",
        "            if cur_time >= SL_ACTIVATION_TIME and cur_price <= active_trades[sym][\"indiv_sl_price\"]:\n",
        "                active_trades[sym][\"exit_price\"] = cur_price\n",
        "                active_trades[sym][\"exit_reason\"] = f\"INDIV_SL_{cur_time}\"\n",
        "                num_active_trades -= 1\n",
        "                continue\n",
        "\n",
        "            total_pnl_pct += trade_pnl_pct / num_active_trades  # Equal weighting\n",
        "\n",
        "        # Check portfolio-level target or stop-loss\n",
        "        if num_active_trades > 0:\n",
        "            if total_pnl_pct >= PORTFOLIO_TARGET_PCT:\n",
        "                portfolio_exit_triggered = True\n",
        "                portfolio_exit_time = cur_time\n",
        "                portfolio_exit_reason = f\"PORTFOLIO_TARGET_{cur_time}\"\n",
        "            elif total_pnl_pct <= PORTFOLIO_SL_PCT:\n",
        "                portfolio_exit_triggered = True\n",
        "                portfolio_exit_time = cur_time\n",
        "                portfolio_exit_reason = f\"PORTFOLIO_SL_{cur_time}\"\n",
        "\n",
        "    # Finalize trade exits\n",
        "    for sym in trade_symbols:\n",
        "        entry_price = active_trades[sym][\"entry_price\"]\n",
        "        if active_trades[sym][\"exit_price\"] is not None:\n",
        "            exit_price = active_trades[sym][\"exit_price\"]\n",
        "            exit_reason = active_trades[sym][\"exit_reason\"]\n",
        "        elif portfolio_exit_triggered:\n",
        "            # Exit at portfolio exit time\n",
        "            exit_prices = all_day_prices[(all_day_prices[\"TradeTime\"] == portfolio_exit_time) & (all_day_prices[\"SYMBOL\"] == sym)][\"Close\"]\n",
        "            exit_price = exit_prices.values[0] if not exit_prices.empty else entry_price\n",
        "            exit_reason = portfolio_exit_reason\n",
        "        else:\n",
        "            # Use END_TIME price if no other exit triggered\n",
        "            end_time_prices = all_day_prices[(all_day_prices[\"TradeTime\"] == END_TIME) & (all_day_prices[\"SYMBOL\"] == sym)][\"Close\"]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices.values[0]\n",
        "                exit_reason = END_TIME\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                day_prices = all_day_prices[all_day_prices[\"SYMBOL\"] == sym]\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        trade_pnl = round(exit_price - entry_price, 2)\n",
        "        roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "        cumulative_portfolio_return += roi_trade\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            trade_date,\n",
        "            entry_price,\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reason,\n",
        "            round(roi_trade, 2),\n",
        "            round(cumulative_portfolio_return, 2)\n",
        "        ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So-2S0NkuI7m"
      },
      "source": [
        "# Weekly positional NSNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "gxYkd7h5ffuK",
        "outputId": "3f6194ab-f68c-47c4-e0cb-8c0de1b8e19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 57 weekly trading days\n",
            "‚úÖ Momentum scan finished ‚Üí Found 161 momentum signals\n",
            "üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 12 signals selected for trading\n"
          ]
        },
        {
          "ename": "InvalidOperationError",
          "evalue": "'is_in' cannot check for Datetime(Microseconds, None) values in Date data\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nFILTER col(\"TradeDate\").is_in([Series]) FROM\n  DF [\"Timestamp\", \"Open\", \"High\", \"Low\", ...]; PROJECT */10 COLUMNS",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidOperationError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3777900133.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Filter prices for the entire week (from entry_date to exit_date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mdate_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentry_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mweek_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TradeDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TradeDate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TradeTime\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mweek_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweek_prices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweek_prices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TradeTime\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mSL_ACTIVATION_TIME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/dataframe/frame.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, *predicates, **constraints)\u001b[0m\n\u001b[1;32m   5099\u001b[0m         \u001b[0;31m‚îî\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚î¥\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚î¥\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îò\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5100\u001b[0m         \"\"\"\n\u001b[0;32m-> 5101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5103\u001b[0m     def remove(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"engine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"old-streaming\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/lazyframe/frame.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, _check_order, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0;31m# Only for testing purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_opt_callback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidOperationError\u001b[0m: 'is_in' cannot check for Datetime(Microseconds, None) values in Date data\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nFILTER col(\"TradeDate\").is_in([Series]) FROM\n  DF [\"Timestamp\", \"Open\", \"High\", \"Low\", ...]; PROJECT */10 COLUMNS"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import timedelta\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.05      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "ENTRY_TIME = \"15:20\"           # Trade entry/exit time (last trading day of week)\n",
        "SL_ACTIVATION_TIME = \"09:40\"   # SL activation time each day\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_data = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\") == ENTRY_TIME)\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"Close\"]).to_pandas()\n",
        "        close_1520 = pdf.set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_data[symbol] = {\"close_1520\": close_1520}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_data)} symbols with required times\")\n",
        "\n",
        "# Identify last trading day of each week\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_data.items():\n",
        "    all_dates.update(d[\"close_1520\"].index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "date_df = pd.DataFrame({\"TradeDate\": pd.to_datetime(unique_trade_dates)})\n",
        "date_df[\"Week\"] = date_df[\"TradeDate\"].dt.isocalendar().week\n",
        "date_df[\"Year\"] = date_df[\"TradeDate\"].dt.year\n",
        "weekly_last_days = date_df.groupby([\"Year\", \"Week\"])[\"TradeDate\"].max().reset_index()\n",
        "\n",
        "symbol_daily_data = {}\n",
        "for sym, d in symbol_close_data.items():\n",
        "    close_series = d[\"close_1520\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(252)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(weekly_last_days)} weekly trading days\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in weekly_last_days[\"TradeDate\"]:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], row['ret_1m']])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1520\", \"RET_1M\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"RET_1M\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for idx, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    weekly_last_days_list = sorted(weekly_last_days[\"TradeDate\"])\n",
        "    if signal_date not in weekly_last_days_list:\n",
        "        continue\n",
        "    week_idx = weekly_last_days_list.index(signal_date)\n",
        "    if week_idx + 1 >= len(weekly_last_days_list):\n",
        "        continue\n",
        "\n",
        "    entry_date = signal_date\n",
        "    exit_date = weekly_last_days_list[week_idx + 1]\n",
        "\n",
        "    entry_price = symbol_close_data[sym][\"close_1520\"].get(entry_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # Filter prices for the entire week (from entry_date to exit_date)\n",
        "    date_range = pd.date_range(start=entry_date, end=exit_date, freq='D')\n",
        "    date_range = [d.date() for d in date_range]  # Convert to date objects\n",
        "    week_prices = df_full.filter(pl.col(\"TradeDate\").is_in(date_range)).select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    week_prices = week_prices[week_prices[\"TradeTime\"] >= SL_ACTIVATION_TIME]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = f\"EXIT_{exit_date}_1520\"\n",
        "    exit_time = None\n",
        "\n",
        "    for _, minute_row in week_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "        cur_date = minute_row[\"TradeDate\"]\n",
        "\n",
        "        # Check SL from SL_ACTIVATION_TIME onward each day\n",
        "        if cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_date}_{cur_time}\"\n",
        "            exit_time = cur_time\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Try to get exit price at 15:20 on the exit date\n",
        "        exit_day_prices = week_prices[week_prices[\"TradeDate\"] == exit_date]\n",
        "        exit_time_prices = exit_day_prices[exit_day_prices[\"TradeTime\"] == ENTRY_TIME]\n",
        "        if not exit_time_prices.empty:\n",
        "            exit_price = exit_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price on exit date\n",
        "            exit_price = exit_day_prices[\"Close\"].iloc[-1] if not exit_day_prices.empty else entry_price\n",
        "            exit_reason = f\"FALLBACK_LAST_PRICE_{exit_date}\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        entry_date,\n",
        "        exit_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"ENTRY_DATE\", \"EXIT_DATE\",\n",
        "                                  \"BUY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Weekly PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    weekly_pnl_df = output_df.groupby(\"EXIT_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    weekly_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"WEEKLY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    weekly_pnl_df[\"CUMULATIVE_PNL\"] = weekly_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    weekly_pnl_df.to_csv(\"WEEKLY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Weekly PnL summary saved in: WEEKLY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Weekly PnL sheet.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhoiSny1vtd4",
        "outputId": "8ee64d9b-2089-43ef-8bff-23c5c2afe80c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 500 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 57 weekly trading days\n",
            "‚úÖ Momentum scan finished ‚Üí Found 1525 momentum signals\n",
            "üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 96 signals selected for trading\n",
            "‚úÖ Backtest completed. 92 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Weekly PnL summary saved in: WEEKLY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import timedelta\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.15      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "ENTRY_TIME = \"15:20\"           # Trade entry/exit time (last trading day of week)\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation time each day\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_data = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\") == ENTRY_TIME)\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"Close\"]).to_pandas()\n",
        "        close_1520 = pdf.set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_data[symbol] = {\"close_1520\": close_1520}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_data)} symbols with required times\")\n",
        "\n",
        "# Identify last trading day of each week\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_data.items():\n",
        "    all_dates.update(d[\"close_1520\"].index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "date_df = pd.DataFrame({\"TradeDate\": pd.to_datetime(unique_trade_dates)})\n",
        "date_df[\"Week\"] = date_df[\"TradeDate\"].dt.isocalendar().week\n",
        "date_df[\"Year\"] = date_df[\"TradeDate\"].dt.year\n",
        "weekly_last_days = date_df.groupby([\"Year\", \"Week\"])[\"TradeDate\"].max().reset_index()\n",
        "\n",
        "symbol_daily_data = {}\n",
        "for sym, d in symbol_close_data.items():\n",
        "    close_series = d[\"close_1520\"]\n",
        "    if close_series.empty or len(close_series) < 252:\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame({'Close': close_series}).sort_index()\n",
        "    df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['ret_1m'] = (df['Close'] / df['Close'].shift(21)) - 1\n",
        "    df['ret_3m'] = (df['Close'] / df['Close'].shift(63)) - 1\n",
        "    df['ret_6m'] = (df['Close'] / df['Close'].shift(126)) - 1\n",
        "    df['ret_12m'] = (df['Close'] / df['Close'].shift(152)) - 1\n",
        "    symbol_daily_data[sym] = df\n",
        "\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(weekly_last_days)} weekly trading days\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in weekly_last_days[\"TradeDate\"]:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['ret_12m']):\n",
        "                continue\n",
        "            if (row['ret_1m'] > 0 and row['ret_3m'] > 0 and row['ret_6m'] > 0 and row['ret_12m'] > 0 and\n",
        "                row['Close'] > row['EMA10'] > row['EMA20']):\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], row['ret_1m']])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Momentum scan finished ‚Üí Found {len(all_breakdowns)} momentum signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1520\", \"RET_1M\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL momentum signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"RET_1M\", ascending=False).head(4)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for idx, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    weekly_last_days_list = sorted(weekly_last_days[\"TradeDate\"])\n",
        "    if signal_date not in weekly_last_days_list:\n",
        "        continue\n",
        "    week_idx = weekly_last_days_list.index(signal_date)\n",
        "    if week_idx + 1 >= len(weekly_last_days_list):\n",
        "        continue\n",
        "\n",
        "    entry_date = signal_date\n",
        "    exit_date = weekly_last_days_list[week_idx + 1]\n",
        "\n",
        "    entry_price = symbol_close_data[sym][\"close_1520\"].get(entry_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    # Filter prices for the entire week (from entry_date to exit_date)\n",
        "    date_range = pd.date_range(start=entry_date, end=exit_date, freq='D')\n",
        "    date_range = [d.date() for d in date_range]  # Convert to date objects\n",
        "    week_prices = df_full.filter(pl.col(\"TradeDate\").is_in(date_range)).select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    week_prices = week_prices[week_prices[\"TradeTime\"] >= SL_ACTIVATION_TIME]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = f\"EXIT_{exit_date}_1520\"\n",
        "    exit_time = None\n",
        "\n",
        "    for _, minute_row in week_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "        cur_date = minute_row[\"TradeDate\"]\n",
        "\n",
        "        # Check SL from SL_ACTIVATION_TIME onward each day\n",
        "        if cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_date}_{cur_time}\"\n",
        "            exit_time = cur_time\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Try to get exit price at 15:20 on the exit date\n",
        "        exit_day_prices = week_prices[week_prices[\"TradeDate\"] == exit_date]\n",
        "        exit_time_prices = exit_day_prices[exit_day_prices[\"TradeTime\"] == ENTRY_TIME]\n",
        "        if not exit_time_prices.empty:\n",
        "            exit_price = exit_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price on exit date\n",
        "            exit_price = exit_day_prices[\"Close\"].iloc[-1] if not exit_day_prices.empty else entry_price\n",
        "            exit_reason = f\"FALLBACK_LAST_PRICE_{exit_date}\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        entry_date,\n",
        "        exit_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"ENTRY_DATE\", \"EXIT_DATE\",\n",
        "                                  \"BUY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Weekly PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    weekly_pnl_df = output_df.groupby(\"EXIT_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    weekly_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"WEEKLY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    weekly_pnl_df[\"CUMULATIVE_PNL\"] = weekly_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    weekly_pnl_df.to_csv(\"WEEKLY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Weekly PnL summary saved in: WEEKLY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Weekly PnL sheet.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXYL-Ys_KPRK"
      },
      "source": [
        "# Prev close=current_open"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H--JaQoqKSQk",
        "outputId": "f8cde161-a860-4621-b802-4f37b7cbc712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 500 cash files...\n",
            "‚úÖ Processed 50/500 symbols\n",
            "‚úÖ Processed 100/500 symbols\n",
            "‚úÖ Processed 150/500 symbols\n",
            "‚úÖ Processed 200/500 symbols\n",
            "‚úÖ Processed 250/500 symbols\n",
            "‚úÖ Processed 300/500 symbols\n",
            "‚úÖ Processed 350/500 symbols\n",
            "‚úÖ Processed 400/500 symbols\n",
            "‚úÖ Processed 450/500 symbols\n",
            "‚úÖ Processed 500/500 symbols\n",
            "‚úÖ Loaded 485 symbols with required times\n",
            "‚úÖ Computed daily metrics for 485 symbols ‚Üí 261 trade dates\n",
            "‚úÖ Volatility scan finished ‚Üí Found 290 volatility signals\n",
            "üìÑ Saved ALL volatility signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\n",
            "‚úÖ After ranking ‚Üí 203 signals selected for trading\n",
            "‚úÖ Backtest completed. 201 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "symbol_daily_data = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    # Compute proper daily OHLCV\n",
        "    daily = df.group_by(\"TradeDate\").agg(\n",
        "        pl.col(\"Open\").first().alias(\"Open\"),\n",
        "        pl.col(\"High\").max().alias(\"High\"),\n",
        "        pl.col(\"Low\").min().alias(\"Low\"),\n",
        "        pl.col(\"Close\").last().alias(\"Close\"),\n",
        "        pl.col(\"Volume\").sum().alias(\"Volume\")\n",
        "    ).sort(\"TradeDate\")\n",
        "\n",
        "    pdf_daily = daily.to_pandas().set_index(\"TradeDate\")\n",
        "\n",
        "    if len(pdf_daily) < 252:\n",
        "        continue\n",
        "\n",
        "    pdf_daily['Range'] = pdf_daily['High'] - pdf_daily['Low']\n",
        "    pdf_daily['Range_1'] = pdf_daily['Range'].shift(1)\n",
        "    pdf_daily['Range_2'] = pdf_daily['Range'].shift(2)\n",
        "    pdf_daily['Range_3'] = pdf_daily['Range'].shift(3)\n",
        "    pdf_daily['Range_4'] = pdf_daily['Range'].shift(4)\n",
        "    pdf_daily['Range_5'] = pdf_daily['Range'].shift(5)\n",
        "    pdf_daily['Range_6'] = pdf_daily['Range'].shift(6)\n",
        "    pdf_daily['Range_7'] = pdf_daily['Range'].shift(7)\n",
        "    pdf_daily['Close_1'] = pdf_daily['Close'].shift(1)\n",
        "    pdf_daily['Volume_1'] = pdf_daily['Volume'].shift(1)\n",
        "    pdf_daily['SMA20'] = pdf_daily['Close'].rolling(20).mean()\n",
        "    pdf_daily['SMA50'] = pdf_daily['Close'].rolling(50).mean()\n",
        "    pdf_daily['SMA200'] = pdf_daily['Close'].rolling(200).mean()\n",
        "    pdf_daily['ret_1m'] = (pdf_daily['Close'] / pdf_daily['Close'].shift(21)) - 1\n",
        "\n",
        "    # Weekly\n",
        "    weekly = pdf_daily.resample('W-FRI').agg({'Open': 'first', 'Close': 'last'})\n",
        "    weekly['Weekly_Up'] = weekly['Close'] > weekly['Open']\n",
        "    previous_weekly_up = weekly['Weekly_Up'].shift(1)\n",
        "    pdf_daily['Prev_Weekly_Up'] = previous_weekly_up.reindex(pdf_daily.index).ffill()\n",
        "\n",
        "    # Monthly\n",
        "    monthly = pdf_daily.resample('ME').agg({'Open': 'first', 'Close': 'last'})\n",
        "    monthly['Monthly_Up'] = monthly['Close'] > monthly['Open']\n",
        "    previous_monthly_up = monthly['Monthly_Up'].shift(1)\n",
        "    pdf_daily['Prev_Monthly_Up'] = previous_monthly_up.reindex(pdf_daily.index).ffill()\n",
        "\n",
        "    symbol_daily_data[symbol] = pdf_daily\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily_data)} symbols with required times\")\n",
        "\n",
        "all_dates = set()\n",
        "\n",
        "for df in symbol_daily_data.values():\n",
        "    all_dates.update(df.index)\n",
        "\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Computed daily metrics for {len(symbol_daily_data)} symbols ‚Üí {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    for sym in symbol_daily_data:\n",
        "        try:\n",
        "            row = symbol_daily_data[sym].loc[trade_date]\n",
        "            if pd.isna(row['SMA200']) or pd.isna(row['ret_1m']) or pd.isna(row['Prev_Weekly_Up']) or pd.isna(row['Prev_Monthly_Up']):\n",
        "                continue\n",
        "            if (\n",
        "                row['Range'] > row['Range_1'] and\n",
        "                row['Range'] > row['Range_2'] and\n",
        "                row['Range'] > row['Range_3'] and\n",
        "                row['Range'] > row['Range_4'] and\n",
        "                row['Range'] > row['Range_5'] and\n",
        "                row['Range'] > row['Range_6'] and\n",
        "                row['Range'] > row['Range_7'] and\n",
        "                row['Close'] > row['Open'] and\n",
        "                row['Close'] > row['Close_1'] and\n",
        "                row['Prev_Weekly_Up'] and\n",
        "                row['Prev_Monthly_Up'] and\n",
        "                row['Volume_1'] > 10000 and\n",
        "                row['SMA20'] > row['SMA50'] and\n",
        "                row['SMA50'] > row['SMA200']\n",
        "            ):\n",
        "                all_breakdowns.append([trade_date, sym, row['Close'], row['ret_1m']])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Volatility scan finished ‚Üí Found {len(all_breakdowns)} volatility signals\")\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"CLOSE_1529\", \"RET_1M\"])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL volatility signals with RET_1M ‚Üí ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "ranked_signals = []\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    daily_sorted = daily_df.sort_values(\"RET_1M\", ascending=False).head(5)\n",
        "    ranked_signals.append(daily_sorted)\n",
        "\n",
        "ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading\")\n",
        "\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "\n",
        "    dates_list = sorted(symbol_close_start_end[sym][\"close_1529\"].index)\n",
        "    if signal_date not in dates_list:\n",
        "        continue\n",
        "    idx = dates_list.index(signal_date) + 1\n",
        "    if idx >= len(dates_list):\n",
        "        continue\n",
        "\n",
        "    trade_date = dates_list[idx]\n",
        "\n",
        "    entry_price = symbol_close_start_end[sym][\"open_start\"].get(trade_date, None)\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # ‚úÖ Calculate individual SL price (buy strategy, so SL is below entry)\n",
        "    indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # ‚úÖ Activate SL only from SL_ACTIVATION_TIME onward\n",
        "        if cur_time >= SL_ACTIVATION_TIME and cur_price <= indiv_sl_price:\n",
        "            exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price in the day if END_TIME not found\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    trade_pnl = round(exit_price - entry_price, 2)\n",
        "    roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\",\n",
        "                                  \"BUY_START\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPGAcUiAcHx5"
      },
      "source": [
        "#Index based intraday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnrEvk_mwWNa",
        "outputId": "13e559f3-213f-44b9-e275-2dc6aab5a5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Loaded 540 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 295 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 156574 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 1176 signals selected for trading (up to 8 per date)\n",
            "‚úÖ Backtest completed. 1176 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params (unchanged)\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files (unchanged)\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (change if filename differs)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close/open (09:17): indexed by TradeDate\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time 09:17) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0917, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_open_start.loc[trade_date])\n",
        "            if nifty_start != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_start) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:17 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"open_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or start_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / start_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0917\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top4 (best) and bottom4 (worst) relative to NIFTY500 -----\n",
        "ranked_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # If NIFTY ROI available for the day, use it\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    nifty_roi_for_date = float(nifty_vals[0]) if len(nifty_vals) > 0 else None\n",
        "\n",
        "    # Pick top4 (highest ROI_%) and bottom4 (lowest ROI_%)\n",
        "    try:\n",
        "        top4 = daily_df.sort_values(\"ROI_%\", ascending=True).head(2).copy()\n",
        "        if not top4.empty:\n",
        "            top4[\"SIDE\"] = \"LONG\"\n",
        "        bottom4 = daily_df.sort_values(\"ROI_%\", ascending=False).head(2).copy()\n",
        "        if not bottom4.empty:\n",
        "            bottom4[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine top and bottom into day's signals\n",
        "    day_selected = pd.concat([top4, bottom4], ignore_index=True) if (not top4.empty or not bottom4.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading (up to 8 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "    side = row.get(\"SIDE\", \"LONG\")\n",
        "\n",
        "    # Trade_date is the same as signal_date (entry at START_TIME)\n",
        "    trade_date = signal_date\n",
        "\n",
        "    # Entry price = START_TIME close on trade_date\n",
        "    entry_price = None\n",
        "    try:\n",
        "        entry_price = float(symbol_close_start_end[sym][\"open_start\"].get(trade_date, None))\n",
        "    except Exception:\n",
        "        entry_price = None\n",
        "\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # Determine SL depending on SIDE\n",
        "    if side == \"LONG\":\n",
        "        indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "    else:\n",
        "        indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # Pull full-day minute prices for trade_date\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "    day_prices = day_prices[(day_prices[\"TradeTime\"] >= START_TIME) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # SL activation time logic\n",
        "        if cur_time >= SL_ACTIVATION_TIME:\n",
        "            if side == \"LONG\" and cur_price <= indiv_sl_price:\n",
        "                exit_price = cur_price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                break\n",
        "            if side == \"SHORT\" and cur_price >= indiv_sl_price:\n",
        "                exit_price = cur_price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    # Compute PnL and ROI depending on side\n",
        "    if side == \"LONG\":\n",
        "        trade_pnl = round(exit_price - entry_price, 2)\n",
        "        roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "    else:  # SHORT\n",
        "        trade_pnl = round(entry_price - exit_price, 2)\n",
        "        roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        side,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6VI_fUamzyN"
      },
      "source": [
        "#Add portfolio sl Index Intraday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRhwSJEtwtNP",
        "outputId": "b7334a0a-5c65-4961-bd94-4eca9d1c9819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 502 cash files...\n",
            "‚úÖ Processed 50/502 symbols\n",
            "‚úÖ Processed 100/502 symbols\n",
            "‚úÖ Processed 150/502 symbols\n",
            "‚úÖ Processed 200/502 symbols\n",
            "‚úÖ Processed 250/502 symbols\n",
            "‚úÖ Processed 300/502 symbols\n",
            "‚úÖ Processed 350/502 symbols\n",
            "‚úÖ Processed 400/502 symbols\n",
            "‚úÖ Processed 450/502 symbols\n",
            "‚úÖ Processed 500/502 symbols\n",
            "‚úÖ Loaded 502 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 270 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 133997 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 1076 signals selected for trading (up to 4 per date)\n",
            "‚úÖ Backtest completed. 1076 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily P&L summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ‚úÖ User-configurable SL params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_SL_PCT = -0.005      # -0.5% portfolio SL\n",
        "START_TIME = \"09:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:32\"   # Individual SL activation time\n",
        "PORTFOLIO_SL_ACTIVATION_TIME = \"09:45\"  # Portfolio SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# Set initial portfolio value for calculations\n",
        "INITIAL_PORTFOLIO_VALUE = 1000000.0\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # Start time close/open (09:15): indexed by TradeDate\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# Load NIFTY500 series (prev close 15:29 and start time 09:15)\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# Build ALL_BREAKDOWNS list for all symbols on each date\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_open_start.loc[trade_date])\n",
        "            if nifty_start != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_start) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:15 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"open_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or start_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / start_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0917\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# Ranking logic: for each SIGNAL_DATE pick top2 (best) and bottom2 (worst) relative to NIFTY500\n",
        "ranked_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # If NIFTY ROI available for the day, use it\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    nifty_roi_for_date = float(nifty_vals[0]) if len(nifty_vals) > 0 else None\n",
        "\n",
        "    # Pick top2 (highest ROI_%) and bottom2 (lowest ROI_%)\n",
        "    try:\n",
        "        top2 = daily_df.sort_values(\"ROI_%\", ascending=True).head(2).copy()\n",
        "        if not top2.empty:\n",
        "            top2[\"SIDE\"] = \"LONG\"\n",
        "        bottom2 = daily_df.sort_values(\"ROI_%\", ascending=False).head(2).copy()\n",
        "        if not bottom2.empty:\n",
        "            bottom2[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine top and bottom into day's signals\n",
        "    day_selected = pd.concat([top2, bottom2], ignore_index=True) if (not top2.empty or not bottom2.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading (up to 4 per date)\")\n",
        "\n",
        "# Backtest/execution loop\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for signal_date, day_group in ranked_df.groupby(\"SIGNAL_DATE\"):\n",
        "    trade_date = signal_date\n",
        "    trades = []\n",
        "    for _, row in day_group.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "        entry_price = float(symbol_close_start_end[sym][\"open_start\"].get(trade_date, None))\n",
        "        if entry_price is None:\n",
        "            continue\n",
        "        trades.append({\n",
        "            \"sym\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"exited\": False,\n",
        "            \"exit_price\": None,\n",
        "            \"exit_reason\": None,\n",
        "            \"exit_time\": None\n",
        "        })\n",
        "\n",
        "    num_trades_day = len(trades)\n",
        "    if num_trades_day == 0:\n",
        "        continue\n",
        "\n",
        "    # Load minute data for all symbols this day\n",
        "    day_data = {}\n",
        "    for trade in trades:\n",
        "        sym = trade[\"sym\"]\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == trade_date).select([\"TradeTime\", \"Close\"]).to_pandas().set_index(\"TradeTime\")\n",
        "        day_prices = day_prices[(day_prices.index >= START_TIME) & (day_prices.index <= END_TIME)]\n",
        "        day_data[sym] = day_prices[\"Close\"]\n",
        "\n",
        "    # Generate all minute times\n",
        "    def generate_minutes(start_str, end_str):\n",
        "        start_parts = list(map(int, start_str.split(\":\")))\n",
        "        end_parts = list(map(int, end_str.split(\":\")))\n",
        "        start_dt = datetime(2000, 1, 1, start_parts[0], start_parts[1])\n",
        "        end_dt = datetime(2000, 1, 1, end_parts[0], end_parts[1])\n",
        "        times = []\n",
        "        current = start_dt\n",
        "        while current <= end_dt:\n",
        "            times.append(current.strftime(\"%H:%M\"))\n",
        "            current += timedelta(minutes=1)\n",
        "        return times\n",
        "\n",
        "    all_times = generate_minutes(START_TIME, END_TIME)\n",
        "\n",
        "    # Simulate trades\n",
        "    for cur_time in all_times:\n",
        "        # Check individual SL if after activation\n",
        "        if cur_time >= SL_ACTIVATION_TIME:\n",
        "            for trade in trades:\n",
        "                if trade[\"exited\"]:\n",
        "                    continue\n",
        "                sym = trade[\"sym\"]\n",
        "                cur_price = day_data[sym].get(cur_time, None)\n",
        "                if cur_price is None:\n",
        "                    continue\n",
        "                if trade[\"side\"] == \"LONG\":\n",
        "                    indiv_sl_price = trade[\"entry_price\"] * (1 - INDIVIDUAL_SL_PCT)\n",
        "                    if cur_price <= indiv_sl_price:\n",
        "                        trade[\"exited\"] = True\n",
        "                        trade[\"exit_price\"] = cur_price\n",
        "                        trade[\"exit_reason\"] = f\"INDIV_SL_{cur_time}\"\n",
        "                        trade[\"exit_time\"] = cur_time\n",
        "                else:\n",
        "                    indiv_sl_price = trade[\"entry_price\"] * (1 + INDIVIDUAL_SL_PCT)\n",
        "                    if cur_price >= indiv_sl_price:\n",
        "                        trade[\"exited\"] = True\n",
        "                        trade[\"exit_price\"] = cur_price\n",
        "                        trade[\"exit_reason\"] = f\"INDIV_SL_{cur_time}\"\n",
        "                        trade[\"exit_time\"] = cur_time\n",
        "\n",
        "        # Check portfolio SL if after portfolio SL activation\n",
        "        if cur_time >= PORTFOLIO_SL_ACTIVATION_TIME:\n",
        "            current_rois = []\n",
        "            all_exited = True\n",
        "            for trade in trades:\n",
        "                if trade[\"exited\"]:\n",
        "                    if trade[\"side\"] == \"LONG\":\n",
        "                        roi = ((trade[\"exit_price\"] - trade[\"entry_price\"]) / trade[\"entry_price\"]) * 100\n",
        "                    else:\n",
        "                        roi = ((trade[\"entry_price\"] - trade[\"exit_price\"]) / trade[\"entry_price\"]) * 100\n",
        "                    current_rois.append(roi)\n",
        "                else:\n",
        "                    all_exited = False\n",
        "                    sym = trade[\"sym\"]\n",
        "                    prices = day_data[sym]\n",
        "                    cur_price = prices.get(cur_time, None)\n",
        "                    if cur_price is None:\n",
        "                        prev_times = [t for t in prices.index if t <= cur_time]\n",
        "                        if prev_times:\n",
        "                            last_t = max(prev_times)\n",
        "                            cur_price = prices[last_t]\n",
        "                        else:\n",
        "                            cur_price = trade[\"entry_price\"]\n",
        "                    if trade[\"side\"] == \"LONG\":\n",
        "                        roi = ((cur_price - trade[\"entry_price\"]) / trade[\"entry_price\"]) * 100\n",
        "                    else:\n",
        "                        roi = ((trade[\"entry_price\"] - cur_price) / trade[\"entry_price\"]) * 100\n",
        "                    current_rois.append(roi)\n",
        "\n",
        "            if current_rois:\n",
        "                portfolio_return_pct = sum(current_rois) / len(current_rois)\n",
        "                if portfolio_return_pct <= PORTFOLIO_SL_PCT * 100:\n",
        "                    # Trigger portfolio SL, exit all remaining\n",
        "                    for trade in trades:\n",
        "                        if not trade[\"exited\"]:\n",
        "                            sym = trade[\"sym\"]\n",
        "                            prices = day_data[sym]\n",
        "                            cur_price = prices.get(cur_time, None)\n",
        "                            if cur_price is None:\n",
        "                                prev_times = [t for t in prices.index if t <= cur_time]\n",
        "                                if prev_times:\n",
        "                                    last_t = max(prev_times)\n",
        "                                    cur_price = prices[last_t]\n",
        "                                else:\n",
        "                                    cur_price = trade[\"entry_price\"]\n",
        "                            trade[\"exited\"] = True\n",
        "                            trade[\"exit_price\"] = cur_price\n",
        "                            trade[\"exit_reason\"] = f\"PORT_SL_{cur_time}\"\n",
        "                            trade[\"exit_time\"] = cur_time\n",
        "                    all_exited = True\n",
        "\n",
        "            if all_exited:\n",
        "                break\n",
        "\n",
        "    # Exit remaining at END_TIME\n",
        "    for trade in trades:\n",
        "        if not trade[\"exited\"]:\n",
        "            sym = trade[\"sym\"]\n",
        "            prices = day_data[sym]\n",
        "            exit_price = prices.get(END_TIME, None)\n",
        "            if exit_price is None:\n",
        "                if not prices.empty:\n",
        "                    exit_price = prices.iloc[-1]\n",
        "                else:\n",
        "                    exit_price = trade[\"entry_price\"]\n",
        "            trade[\"exit_price\"] = exit_price\n",
        "            trade[\"exit_reason\"] = END_TIME\n",
        "            trade[\"exit_time\"] = END_TIME\n",
        "\n",
        "    # Compute P&L, ROIs, day ROI, final values\n",
        "    day_rois = []\n",
        "    for trade in trades:\n",
        "        sym = trade[\"sym\"]\n",
        "        side = trade[\"side\"]\n",
        "        entry_price = trade[\"entry_price\"]\n",
        "        exit_price = trade[\"exit_price\"]\n",
        "        exit_reason = trade[\"exit_reason\"]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entry_price, 2)\n",
        "            roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entry_price - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "        day_rois.append(roi_trade)\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            trade_date,\n",
        "            side,\n",
        "            entry_price,\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reason,\n",
        "            round(roi_trade, 2),\n",
        "            round(cumulative_portfolio_return, 2),\n",
        "            INITIAL_PORTFOLIO_VALUE,\n",
        "            None  # Placeholder for exit portfolio value\n",
        "        ])\n",
        "\n",
        "    if day_rois:\n",
        "        day_roi = sum(day_rois) / len(day_rois)\n",
        "        cumulative_portfolio_return += day_roi\n",
        "        final_portfolio_value = INITIAL_PORTFOLIO_VALUE * (1 + day_roi / 100)\n",
        "        # Update the last num_trades_day rows\n",
        "        for i in range(len(output_trades) - num_trades_day, len(output_trades)):\n",
        "            output_trades[i][9] = round(day_roi, 2)  # PORTFOLIO_RETURN%\n",
        "            output_trades[i][10] = round(cumulative_portfolio_return, 2)\n",
        "            output_trades[i][-1] = final_portfolio_value\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"START_PORTFOLIO_VALUE\", \"EXIT_PORTFOLIO_VALUE\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# Generate Daily P&L from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily P&L summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily P&L sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rtt1p6VZ193"
      },
      "source": [
        "#Indexintra adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYs9lXqeZ5ks",
        "outputId": "676f7b40-dbc7-4b90-a68a-c36dcb36a308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Loaded 540 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 286 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 151762 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After advanced selection ‚Üí 638 signals selected for trading (up to 4 per date)\n",
            "‚úÖ Backtest completed. 638 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params (unchanged)\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:15\"           # Trade selection time (09:15 close for ROI)\n",
        "ENTRY_TIME = \"09:32\"           # Trade entry time (09:32 close)\n",
        "SL_ACTIVATION_TIME = \"09:33\"   # SL activation time (after entry)\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files (unchanged)\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (change if filename differs)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME (09:15), ENTRY_TIME (09:32), and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, ENTRY_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # entry time close (09:32): indexed by TradeDate\n",
        "        open_entry = pdf[pdf[\"TradeTime\"] == ENTRY_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start, \"open_entry\": open_entry}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time 09:15) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0915, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_open_start.loc[trade_date])\n",
        "            if nifty_start != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_start) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:15 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"open_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or start_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / start_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0917\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Advanced selection logic: top20/bottom20 candidates, monitor 09:15-09:30, breakout/breakdown at 09:31, confirm at 09:32 -----\n",
        "ranked_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # If NIFTY ROI available for the day, use it\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    nifty_roi_for_date = float(nifty_vals[0]) if len(nifty_vals) > 0 else None\n",
        "\n",
        "    # Select bottom 20 (most negative ROI) for LONG candidates, top 20 (most positive ROI) for SHORT candidates\n",
        "    long_candidates = daily_df.nsmallest(20, \"ROI_%\")\n",
        "    short_candidates = daily_df.nlargest(20, \"ROI_%\")\n",
        "\n",
        "    # ----- LONG candidates: breakout above 09:15-09:30 max close -----\n",
        "    long_breakouts = []\n",
        "    for _, cand in long_candidates.iterrows():\n",
        "        sym = cand[\"SYMBOL\"]\n",
        "        if sym not in symbol_full_data:\n",
        "            continue\n",
        "        df_day = symbol_full_data[sym].filter(pl.col(\"TradeDate\") == pd.Timestamp(signal_date))\n",
        "        # Closes between 09:15 and 09:30\n",
        "        period_df = df_day.filter((pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= \"09:30\"))\n",
        "        if period_df.is_empty():\n",
        "            continue\n",
        "        period_max_close = period_df[\"Close\"].max()\n",
        "        # 09:31 close\n",
        "        close_0931_df = df_day.filter(pl.col(\"TradeTime\") == \"09:31\")\n",
        "        if close_0931_df.is_empty():\n",
        "            continue\n",
        "        close_0931_val = close_0931_df[\"Close\"].item(0)\n",
        "        if close_0931_val <= period_max_close:\n",
        "            continue\n",
        "        # Breakout strength\n",
        "        strength = (close_0931_val - period_max_close) / period_max_close * 100.0\n",
        "        # 09:32 close for confirmation and entry\n",
        "        close_0932_df = df_day.filter(pl.col(\"TradeTime\") == \"09:32\")\n",
        "        if close_0932_df.is_empty():\n",
        "            continue\n",
        "        close_0932_val = close_0932_df[\"Close\"].item(0)\n",
        "        # Confirmation: 09:32 does not close below period max close (i.e., >=)\n",
        "        if close_0932_val >= period_max_close:\n",
        "            long_breakouts.append({\n",
        "                \"SIGNAL_DATE\": signal_date,\n",
        "                \"SYMBOL\": sym,\n",
        "                \"PREV_CLOSE_1529\": cand[\"PREV_CLOSE_1529\"],\n",
        "                \"START_CLOSE_0917\": cand[\"START_CLOSE_0917\"],\n",
        "                \"ROI_%\": cand[\"ROI_%\"],\n",
        "                \"NIFTY500_ROI_%\": cand[\"NIFTY500_ROI_%\"],\n",
        "                \"SIDE\": \"LONG\",\n",
        "                \"ENTRY_PRICE\": close_0932_val,\n",
        "                \"PERIOD_LEVEL\": float(period_max_close),\n",
        "                \"STRENGTH\": round(strength, 4)\n",
        "            })\n",
        "\n",
        "    # Select top 2 LONG by strength\n",
        "    if long_breakouts:\n",
        "        long_breakouts.sort(key=lambda x: x[\"STRENGTH\"], reverse=True)\n",
        "        selected_longs = long_breakouts[:2]\n",
        "        for sel in selected_longs:\n",
        "            ranked_signals.append(pd.DataFrame([sel]))\n",
        "\n",
        "    # ----- SHORT candidates: breakdown below 09:15-09:30 min close -----\n",
        "    short_breakdowns = []\n",
        "    for _, cand in short_candidates.iterrows():\n",
        "        sym = cand[\"SYMBOL\"]\n",
        "        if sym not in symbol_full_data:\n",
        "            continue\n",
        "        df_day = symbol_full_data[sym].filter(pl.col(\"TradeDate\") == pd.Timestamp(signal_date))\n",
        "        # Closes between 09:15 and 09:30\n",
        "        period_df = df_day.filter((pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= \"09:30\"))\n",
        "        if period_df.is_empty():\n",
        "            continue\n",
        "        period_min_close = period_df[\"Close\"].min()\n",
        "        # 09:31 close\n",
        "        close_0931_df = df_day.filter(pl.col(\"TradeTime\") == \"09:31\")\n",
        "        if close_0931_df.is_empty():\n",
        "            continue\n",
        "        close_0931_val = close_0931_df[\"Close\"].item(0)\n",
        "        if close_0931_val >= period_min_close:\n",
        "            continue\n",
        "        # Breakdown strength\n",
        "        strength = (period_min_close - close_0931_val) / period_min_close * 100.0\n",
        "        # 09:32 close for confirmation and entry\n",
        "        close_0932_df = df_day.filter(pl.col(\"TradeTime\") == \"09:32\")\n",
        "        if close_0932_df.is_empty():\n",
        "            continue\n",
        "        close_0932_val = close_0932_df[\"Close\"].item(0)\n",
        "        # Confirmation: 09:32 does not close above period min close (i.e., <=)\n",
        "        if close_0932_val <= period_min_close:\n",
        "            short_breakdowns.append({\n",
        "                \"SIGNAL_DATE\": signal_date,\n",
        "                \"SYMBOL\": sym,\n",
        "                \"PREV_CLOSE_1529\": cand[\"PREV_CLOSE_1529\"],\n",
        "                \"START_CLOSE_0917\": cand[\"START_CLOSE_0917\"],\n",
        "                \"ROI_%\": cand[\"ROI_%\"],\n",
        "                \"NIFTY500_ROI_%\": cand[\"NIFTY500_ROI_%\"],\n",
        "                \"SIDE\": \"SHORT\",\n",
        "                \"ENTRY_PRICE\": close_0932_val,\n",
        "                \"PERIOD_LEVEL\": float(period_min_close),\n",
        "                \"STRENGTH\": round(strength, 4)\n",
        "            })\n",
        "\n",
        "    # Select top 2 SHORT by strength\n",
        "    if short_breakdowns:\n",
        "        short_breakdowns.sort(key=lambda x: x[\"STRENGTH\"], reverse=True)\n",
        "        selected_shorts = short_breakdowns[:2]\n",
        "        for sel in selected_shorts:\n",
        "            ranked_signals.append(pd.DataFrame([sel]))\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0917\", \"ROI_%\", \"NIFTY500_ROI_%\", \"SIDE\", \"ENTRY_PRICE\", \"PERIOD_LEVEL\", \"STRENGTH\"])\n",
        "\n",
        "print(f\"‚úÖ After advanced selection ‚Üí {len(ranked_df)} signals selected for trading (up to 4 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for _, row in ranked_df.iterrows():\n",
        "    signal_date = row[\"SIGNAL_DATE\"]\n",
        "    sym = row[\"SYMBOL\"]\n",
        "    side = row[\"SIDE\"]\n",
        "\n",
        "    # Trade_date is the same as signal_date (entry at ENTRY_TIME)\n",
        "    trade_date = signal_date\n",
        "\n",
        "    # Entry price = ENTRY_TIME close on trade_date (pre-computed in selection)\n",
        "    entry_price = row[\"ENTRY_PRICE\"]\n",
        "\n",
        "    if entry_price is None:\n",
        "        continue\n",
        "\n",
        "    # Determine SL depending on SIDE\n",
        "    if side == \"LONG\":\n",
        "        indiv_sl_price = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "    else:\n",
        "        indiv_sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    # Pull full-day minute prices for trade_date from ENTRY_TIME onwards\n",
        "    df_full = symbol_full_data[sym]\n",
        "    day_prices = df_full.filter((pl.col(\"TradeDate\") == trade_date) & (pl.col(\"TradeTime\") >= ENTRY_TIME) & (pl.col(\"TradeTime\") <= END_TIME)).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    exit_price = None\n",
        "    exit_reason = END_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # SL activation time logic\n",
        "        if cur_time >= SL_ACTIVATION_TIME:\n",
        "            if side == \"LONG\" and cur_price <= indiv_sl_price:\n",
        "                exit_price = cur_price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                break\n",
        "            if side == \"SHORT\" and cur_price >= indiv_sl_price:\n",
        "                exit_price = cur_price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                break\n",
        "\n",
        "    if exit_price is None:\n",
        "        # Use END_TIME price if no SL triggered\n",
        "        end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "        if not end_time_prices.empty:\n",
        "            exit_price = end_time_prices[\"Close\"].values[0]\n",
        "        else:\n",
        "            # Fallback to last available price\n",
        "            exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "            exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "    # Compute PnL and ROI depending on side\n",
        "    if side == \"LONG\":\n",
        "        trade_pnl = round(exit_price - entry_price, 2)\n",
        "        roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "    else:  # SHORT\n",
        "        trade_pnl = round(entry_price - exit_price, 2)\n",
        "        roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        sym,\n",
        "        signal_date,\n",
        "        trade_date,\n",
        "        side,\n",
        "        entry_price,\n",
        "        exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2),\n",
        "        row[\"PERIOD_LEVEL\"],\n",
        "        row[\"STRENGTH\"]\n",
        "    ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"PERIOD_LEVEL\", \"STRENGTH\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cct46T7TBOK2"
      },
      "source": [
        "# IndexIntra VWAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH3m3oEdBWOU",
        "outputId": "83245989-db7f-4b75-a3ac-d90f3ace9bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 275 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 136503 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 1096 signals selected for trading (up to 4 per date)\n",
            "‚úÖ Backtest completed. 1096 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params (unchanged)\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"09:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"09:30\"   # SL activation time\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files (unchanged)\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (change if filename differs)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close/open (09:17): indexed by TradeDate\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time 09:17) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0917, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_open_start.loc[trade_date])\n",
        "            if nifty_start != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_start) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:17 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"open_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or start_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / start_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0917\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top2 (best) and bottom2 (worst) relative to NIFTY500 -----\n",
        "ranked_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # If NIFTY ROI available for the day, use it\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    if len(nifty_vals) == 0:\n",
        "        continue\n",
        "    nifty_roi_for_date = float(nifty_vals[0])\n",
        "\n",
        "    # Compute relative alpha\n",
        "    daily_df[\"ALPHA\"] = daily_df[\"ROI_%\"] - nifty_roi_for_date\n",
        "\n",
        "    # Pick top2 (lowest ALPHA) and bottom2 (highest ALPHA)\n",
        "    try:\n",
        "        top2 = daily_df.sort_values(\"ALPHA\", ascending=True).head(2).copy()\n",
        "        if not top2.empty:\n",
        "            top2[\"SIDE\"] = \"LONG\"\n",
        "        bottom2 = daily_df.sort_values(\"ALPHA\", ascending=False).head(2).copy()\n",
        "        if not bottom2.empty:\n",
        "            bottom2[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine top and bottom into day's signals\n",
        "    day_selected = pd.concat([top2, bottom2], ignore_index=True) if (not top2.empty or not bottom2.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(ranked_df)} signals selected for trading (up to 4 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for trade_date, day_group in ranked_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect the symbols and sides\n",
        "    signals = day_group.set_index(\"SYMBOL\")[\"SIDE\"].to_dict()\n",
        "    num_signals = len(signals)\n",
        "    if num_signals == 0:\n",
        "        continue\n",
        "    weight = 1.0 / num_signals  # equal weight assuming full portfolio allocation across signals\n",
        "\n",
        "    # Get entry prices, indiv SL prices\n",
        "    entries = {}\n",
        "    indiv_sls = {}\n",
        "    for sym, side in list(signals.items()):\n",
        "        entry_price = symbol_close_start_end.get(sym, {}).get(\"open_start\", {}).get(trade_date, None)\n",
        "        if entry_price is None or entry_price == 0:\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entries[sym] = entry_price\n",
        "        if side == \"LONG\":\n",
        "            indiv_sls[sym] = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "        else:\n",
        "            indiv_sls[sym] = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    if not signals:\n",
        "        continue\n",
        "\n",
        "    # Update num_signals and weight if some skipped\n",
        "    num_signals = len(signals)\n",
        "    weight = 1.0 / num_signals\n",
        "\n",
        "    # Collect day prices for symbols\n",
        "    all_times = set()\n",
        "    day_prices = {}\n",
        "    for sym in signals:\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == trade_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_df = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)]\n",
        "        day_df = day_df.set_index(\"TradeTime\")\n",
        "        day_prices[sym] = day_df[\"Close\"]\n",
        "        all_times.update(day_df.index)\n",
        "\n",
        "    all_times = sorted(all_times)\n",
        "\n",
        "    # Create simulation df with prices, ffill/bfill missing\n",
        "    sim_df = pd.DataFrame(index=all_times)\n",
        "    for sym in signals:\n",
        "        sym_prices = day_prices[sym].reindex(all_times).ffill().bfill()\n",
        "        sim_df[sym] = sym_prices\n",
        "\n",
        "    # Initialize exits\n",
        "    exit_times = {sym: None for sym in signals}\n",
        "    exit_prices = {sym: None for sym in signals}\n",
        "    exit_reasons = {sym: END_TIME for sym in signals}\n",
        "\n",
        "    for t in sim_df.index:\n",
        "        if t < SL_ACTIVATION_TIME:\n",
        "            continue\n",
        "\n",
        "        # Compute current rois and portfolio pnl\n",
        "        current_rois = {}\n",
        "        portfolio_pnl_decimal = 0.0\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if side == \"LONG\":\n",
        "                current_roi = (cur_price - entries[sym]) / entries[sym] * 100\n",
        "            else:\n",
        "                current_roi = (entries[sym] - cur_price) / entries[sym] * 100\n",
        "            current_rois[sym] = current_roi\n",
        "            portfolio_pnl_decimal += weight * (current_roi / 100)\n",
        "\n",
        "        # Check portfolio target/SL\n",
        "        if portfolio_pnl_decimal >= PORTFOLIO_TARGET_PCT:\n",
        "            for sym in open_trades:\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = sim_df.at[t, sym]\n",
        "                exit_reasons[sym] = f\"PORTFOLIO_TARGET_{t}\"\n",
        "            continue\n",
        "\n",
        "        if portfolio_pnl_decimal <= PORTFOLIO_SL_PCT:\n",
        "            for sym in open_trades:\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = sim_df.at[t, sym]\n",
        "                exit_reasons[sym] = f\"PORTFOLIO_SL_{t}\"\n",
        "            continue\n",
        "\n",
        "        # Check individual SL\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if (side == \"LONG\" and cur_price <= indiv_sls[sym]) or (side == \"SHORT\" and cur_price >= indiv_sls[sym]):\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = cur_price\n",
        "                exit_reasons[sym] = f\"INDIV_SL_{t}\"\n",
        "\n",
        "    # Exit remaining at end\n",
        "    end_time = all_times[-1] if all_times else END_TIME\n",
        "    for sym in signals:\n",
        "        if exit_times[sym] is None:\n",
        "            exit_times[sym] = end_time\n",
        "            try:\n",
        "                exit_prices[sym] = sim_df.at[end_time, sym]\n",
        "            except:\n",
        "                exit_prices[sym] = entries[sym]\n",
        "            exit_reasons[sym] = f\"END_TIME_{end_time}\"\n",
        "\n",
        "    # Compute day results\n",
        "    day_portfolio_return = 0.0\n",
        "    for sym, side in signals.items():\n",
        "        exit_price = exit_prices[sym]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entries[sym], 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entries[sym] - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        day_portfolio_return += weight * roi_trade\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            trade_date,\n",
        "            trade_date,\n",
        "            side,\n",
        "            entries[sym],\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reasons[sym],\n",
        "            round(day_portfolio_return, 2),\n",
        "            None  # placeholder for cumulative\n",
        "        ])\n",
        "\n",
        "    cumulative_portfolio_return += day_portfolio_return\n",
        "\n",
        "    # Update cumulative in the last appended rows (last num_signals)\n",
        "    for i in range(len(output_trades) - num_signals, len(output_trades)):\n",
        "        output_trades[i][-1] = round(cumulative_portfolio_return, 2)\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWnKo4Iepy5S"
      },
      "source": [
        "# **Price Action Based Swing** (30 Day Breakout Daily)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqLsL6c2xWI_",
        "outputId": "b1b73463-5863-4142-f73c-771c260e7793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with daily data\n",
            "‚úÖ Found 280 potential trade dates from symbol data\n",
            "‚úÖ After date filtering: 280 trade dates\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 3527 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\n",
            "‚úÖ 3527 signals selected for trading (all recommendations)\n",
            "‚úÖ Backtest completed. 3148 trades executed.\n",
            "üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = -0.05      # -5% individual SL\n",
        "TARGET_PROFIT_PCT = 0.10       # +10% target profit\n",
        "START_DATE = None  # e.g., \"2020-01-01\" or None for full period\n",
        "END_DATE = None    # e.g., \"2025-01-01\" or None for full period\n",
        "LOOKBACK_PERIODS = 30          # Lookback for max close price\n",
        "END_TIME = \"15:29\"             # Daily close time\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars, filter for 15:29 daily close.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    # Filter for 15:29\n",
        "    df = df.filter(pl.col(\"TradeTime\") == END_TIME)\n",
        "    pdf = df.select([\"TradeDate\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "    pdf['TradeDate'] = pd.to_datetime(pdf['TradeDate'])\n",
        "    pdf = pdf.set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    return symbol, pdf\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_daily = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, daily = load_full_data(f)\n",
        "    symbol_daily[symbol] = daily\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily)} symbols with daily data\")\n",
        "\n",
        "# Build list of unique trading dates from all symbols\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Filter unique_trade_dates based on START_DATE and END_DATE\n",
        "unique_trade_dates = [pd.Timestamp(dt) for dt in unique_trade_dates]\n",
        "if START_DATE:\n",
        "    start_dt = pd.to_datetime(START_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt >= start_dt]\n",
        "if END_DATE:\n",
        "    end_dt = pd.to_datetime(END_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt <= end_dt]\n",
        "print(f\"‚úÖ After date filtering: {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if d < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# Helper function to get n days back\n",
        "def get_n_days_back(trade_date, all_dates, n):\n",
        "    \"\"\"Return the trading day n days back from trade_date.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    try:\n",
        "        idx = all_dates.index(trade_date)\n",
        "        if idx < n:\n",
        "            return None\n",
        "        return all_dates[idx - n]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each trading day -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, BREAKOUT_STRENGTH\n",
        "all_breakdowns = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    for sym, daily in symbol_daily.items():\n",
        "        if date not in daily.index:\n",
        "            continue\n",
        "        df_up_to = daily.loc[:date]\n",
        "        if len(df_up_to) < LOOKBACK_PERIODS + 1:\n",
        "            continue\n",
        "        close = daily['Close'][date]\n",
        "        lookback_start = get_n_days_back(date, unique_trade_dates, LOOKBACK_PERIODS)\n",
        "        if lookback_start is None:\n",
        "            continue\n",
        "        lookback_data = df_up_to.loc[lookback_start:date].iloc[:-1]\n",
        "        if len(lookback_data) < LOOKBACK_PERIODS:\n",
        "            continue\n",
        "        max_close = lookback_data['Close'].max()\n",
        "        if close <= max_close:\n",
        "            continue\n",
        "        # Check for pullback and re-breakout\n",
        "        breakout_date = None\n",
        "        breakout_high = None\n",
        "        for check_date in df_up_to.index[::-1]:\n",
        "            if check_date >= date:\n",
        "                continue\n",
        "            if df_up_to['Close'][check_date] > max_close:\n",
        "                breakout_date = check_date\n",
        "                breakout_high = df_up_to['High'][check_date]\n",
        "                break\n",
        "        if breakout_date is None:\n",
        "            continue\n",
        "        post_breakout = df_up_to.loc[breakout_date:date].iloc[1:-1]\n",
        "        if post_breakout.empty:\n",
        "            continue\n",
        "        pullback_occurred = (post_breakout['Close'] < breakout_high).any()\n",
        "        if not pullback_occurred:\n",
        "            continue\n",
        "        if close <= breakout_high:\n",
        "            continue\n",
        "        # Calculate breakout strength as percentage gain of breakout candle\n",
        "        breakout_open = df_up_to['Open'][breakout_date]\n",
        "        if breakout_open <= 0:\n",
        "            continue\n",
        "        breakout_strength = (df_up_to['Close'][breakout_date] - breakout_open) / breakout_open\n",
        "        all_breakdowns.append([\n",
        "            date,\n",
        "            sym,\n",
        "            breakout_strength\n",
        "        ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"BREAKOUT_STRENGTH\"])\n",
        "breakdown_df[\"BREAKOUT_STRENGTH\"] = breakdown_df[\"BREAKOUT_STRENGTH\"].astype(float).round(6)\n",
        "breakdown_df['SIGNAL_DATE'] = pd.to_datetime(breakdown_df['SIGNAL_DATE'])\n",
        "breakdown_df = breakdown_df.sort_values(['SIGNAL_DATE', 'BREAKOUT_STRENGTH'], ascending=[True, False])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(f\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\")\n",
        "\n",
        "# ----- No ranking, enter all signals -----\n",
        "ranked_df = breakdown_df.copy()\n",
        "ranked_df[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "print(f\"‚úÖ {len(ranked_df)} signals selected for trading (all recommendations)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "open_positions = []\n",
        "output_trades = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    date = pd.Timestamp(date)\n",
        "    # Handle individual SL and target profit\n",
        "    if open_positions:\n",
        "        to_remove = []\n",
        "        for pos in open_positions:\n",
        "            if pos[\"sym\"] not in symbol_daily:\n",
        "                continue\n",
        "            daily = symbol_daily[pos[\"sym\"]]\n",
        "            if date not in daily.index:\n",
        "                continue\n",
        "            cur_price = daily[\"Close\"][date]\n",
        "            # Individual SL\n",
        "            if cur_price < pos[\"entry_price\"] * (1 + INDIVIDUAL_SL_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"INDIVIDUAL_SL\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "            # Target Profit\n",
        "            elif cur_price >= pos[\"entry_price\"] * (1 + TARGET_PROFIT_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"TARGET_PROFIT\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "        for pos in to_remove:\n",
        "            open_positions.remove(pos)\n",
        "\n",
        "    # Handle entries: if prev_date has signal, enter at next day's open\n",
        "    prev_date = get_prev_trading_day(date, unique_trade_dates)\n",
        "    if prev_date is not None:\n",
        "        day_signals = ranked_df[ranked_df[\"SIGNAL_DATE\"] == prev_date]\n",
        "        if not day_signals.empty:\n",
        "            for _, signal in day_signals.iterrows():\n",
        "                sym = signal[\"SYMBOL\"]\n",
        "                if sym not in symbol_daily:\n",
        "                    continue\n",
        "                daily = symbol_daily[sym]\n",
        "                if date not in daily.index:\n",
        "                    continue\n",
        "                entry_price = daily[\"Open\"][date]\n",
        "                if entry_price <= 0:\n",
        "                    continue\n",
        "                quantity = 1\n",
        "                open_positions.append({\n",
        "                    \"sym\": sym,\n",
        "                    \"signal_date\": prev_date,\n",
        "                    \"trade_date\": date,\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"entry_price\": entry_price,\n",
        "                    \"quantity\": quantity\n",
        "                })\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"EXIT_DATE\"])\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "\n",
        "# Save to Excel with multiple sheets\n",
        "with pd.ExcelWriter(\"OUTPUT_BACKTEST.xlsx\") as writer:\n",
        "    output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "\n",
        "    # Stock-wise summary sheet with success rate\n",
        "    if not output_df.empty:\n",
        "        stock_summary = output_df.groupby(\"SYMBOL\").agg(\n",
        "            TOTAL_PNL=(\"PNL\", \"sum\"),\n",
        "            AVG_TRADE_ROI=(\"TRADE_ROI%\", \"mean\"),\n",
        "            NUM_TRADES=(\"PNL\", \"count\"),\n",
        "            NUM_WIN_TRADES=(\"PNL\", lambda x: (x > 0).sum()),\n",
        "            TRADE_DATES=(\"TRADE_DATE\", lambda x: list(x)),\n",
        "            EXIT_DATES=(\"EXIT_DATE\", lambda x: list(x))\n",
        "        ).reset_index()\n",
        "        stock_summary[\"WIN_RATE%\"] = (stock_summary[\"NUM_WIN_TRADES\"] / stock_summary[\"NUM_TRADES\"]) * 100\n",
        "        stock_summary.to_excel(writer, sheet_name=\"Stock_Summary\", index=False)\n",
        "\n",
        "    # Generate Daily PnL from executed trades\n",
        "    if not output_df.empty:\n",
        "        output_df[\"EXIT_DATE\"] = output_df[\"EXIT_DATE\"].dt.date\n",
        "        daily_pnl_df = output_df.groupby(\"EXIT_DATE\").agg({\n",
        "            \"PNL\": \"sum\",\n",
        "            \"TRADE_ROI%\": \"mean\",\n",
        "            \"SYMBOL\": \"count\"\n",
        "        }).reset_index()\n",
        "\n",
        "        daily_pnl_df.rename(columns={\n",
        "            \"SYMBOL\": \"NUM_TRADES\",\n",
        "            \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "            \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "        }, inplace=True)\n",
        "\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"Daily_PnL\", index=False)\n",
        "\n",
        "    # Generate Monthly PnL\n",
        "    if not output_df.empty:\n",
        "        monthly_pnl_df = output_df.copy()\n",
        "        monthly_pnl_df['MONTH'] = pd.to_datetime(monthly_pnl_df['EXIT_DATE']).dt.to_period('M')\n",
        "        monthly_pnl_df = monthly_pnl_df.groupby('MONTH').agg({\n",
        "            \"PNL\": \"sum\"\n",
        "        }).reset_index()\n",
        "        monthly_pnl_df.to_excel(writer, sheet_name=\"Monthly_PnL\", index=False)\n",
        "        print(f\"üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No trades found, skipping additional sheets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsnDH3aX71zV",
        "outputId": "a6790c15-9a2c-4567-c62f-2fb365f9ad7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with daily data\n",
            "‚úÖ Found 280 potential trade dates from symbol data\n",
            "‚úÖ After date filtering: 280 trade dates\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 2771 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\n",
            "‚úÖ 2771 signals selected for trading (all recommendations)\n",
            "‚úÖ Backtest completed. 2503 trades executed.\n",
            "üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = -0.05      # -5% individual SL\n",
        "TARGET_PROFIT_PCT = 0.10       # +10% target profit\n",
        "START_DATE = None  # e.g., \"2020-01-01\" or None for full period\n",
        "END_DATE = None    # e.g., \"2025-01-01\" or None for full period\n",
        "LOOKBACK_PERIODS = 30          # Lookback for max close price\n",
        "END_TIME = \"15:29\"             # Daily close time\n",
        "ALLOCATION_LIMIT = 2500        # Per stock allocation limit based on signal day close price\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars, filter for 15:29 daily close.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    # Filter for 15:29\n",
        "    df = df.filter(pl.col(\"TradeTime\") == END_TIME)\n",
        "    pdf = df.select([\"TradeDate\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "    pdf['TradeDate'] = pd.to_datetime(pdf['TradeDate'])\n",
        "    pdf = pdf.set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    return symbol, pdf\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_daily = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, daily = load_full_data(f)\n",
        "    symbol_daily[symbol] = daily\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily)} symbols with daily data\")\n",
        "\n",
        "# Build list of unique trading dates from all symbols\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Filter unique_trade_dates based on START_DATE and END_DATE\n",
        "unique_trade_dates = [pd.Timestamp(dt) for dt in unique_trade_dates]\n",
        "if START_DATE:\n",
        "    start_dt = pd.to_datetime(START_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt >= start_dt]\n",
        "if END_DATE:\n",
        "    end_dt = pd.to_datetime(END_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt <= end_dt]\n",
        "print(f\"‚úÖ After date filtering: {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if d < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# Helper function to get n days back\n",
        "def get_n_days_back(trade_date, all_dates, n):\n",
        "    \"\"\"Return the trading day n days back from trade_date.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    try:\n",
        "        idx = all_dates.index(trade_date)\n",
        "        if idx < n:\n",
        "            return None\n",
        "        return all_dates[idx - n]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each trading day -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, BREAKOUT_STRENGTH\n",
        "all_breakdowns = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    for sym, daily in symbol_daily.items():\n",
        "        if date not in daily.index:\n",
        "            continue\n",
        "        df_up_to = daily.loc[:date]\n",
        "        if len(df_up_to) < LOOKBACK_PERIODS + 1:\n",
        "            continue\n",
        "        close = daily['Close'][date]\n",
        "        lookback_start = get_n_days_back(date, unique_trade_dates, LOOKBACK_PERIODS)\n",
        "        if lookback_start is None:\n",
        "            continue\n",
        "        lookback_data = df_up_to.loc[lookback_start:date].iloc[:-1]\n",
        "        if len(lookback_data) < LOOKBACK_PERIODS:\n",
        "            continue\n",
        "        max_close = lookback_data['Close'].max()\n",
        "        if close <= max_close:\n",
        "            continue\n",
        "        # Check for pullback and re-breakout\n",
        "        breakout_date = None\n",
        "        breakout_high = None\n",
        "        for check_date in df_up_to.index[::-1]:\n",
        "            if check_date >= date:\n",
        "                continue\n",
        "            if df_up_to['Close'][check_date] > max_close:\n",
        "                breakout_date = check_date\n",
        "                breakout_high = df_up_to['High'][check_date]\n",
        "                break\n",
        "        if breakout_date is None:\n",
        "            continue\n",
        "        post_breakout = df_up_to.loc[breakout_date:date].iloc[1:-1]\n",
        "        if post_breakout.empty:\n",
        "            continue\n",
        "        pullback_occurred = (post_breakout['Close'] < breakout_high).any()\n",
        "        if not pullback_occurred:\n",
        "            continue\n",
        "        if close <= breakout_high:\n",
        "            continue\n",
        "        # Calculate breakout strength as percentage gain of breakout candle\n",
        "        breakout_open = df_up_to['Open'][breakout_date]\n",
        "        if breakout_open <= 0:\n",
        "            continue\n",
        "        breakout_strength = (df_up_to['Close'][breakout_date] - breakout_open) / breakout_open\n",
        "        if close < ALLOCATION_LIMIT:\n",
        "            all_breakdowns.append([\n",
        "                date,\n",
        "                sym,\n",
        "                breakout_strength\n",
        "            ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"BREAKOUT_STRENGTH\"])\n",
        "breakdown_df[\"BREAKOUT_STRENGTH\"] = breakdown_df[\"BREAKOUT_STRENGTH\"].astype(float).round(6)\n",
        "breakdown_df['SIGNAL_DATE'] = pd.to_datetime(breakdown_df['SIGNAL_DATE'])\n",
        "breakdown_df = breakdown_df.sort_values(['SIGNAL_DATE', 'BREAKOUT_STRENGTH'], ascending=[True, False])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(f\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\")\n",
        "\n",
        "# ----- No ranking, enter all signals -----\n",
        "ranked_df = breakdown_df.copy()\n",
        "ranked_df[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "print(f\"‚úÖ {len(ranked_df)} signals selected for trading (all recommendations)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "open_positions = []\n",
        "output_trades = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    date = pd.Timestamp(date)\n",
        "    # Handle individual SL and target profit\n",
        "    if open_positions:\n",
        "        to_remove = []\n",
        "        for pos in open_positions:\n",
        "            if pos[\"sym\"] not in symbol_daily:\n",
        "                continue\n",
        "            daily = symbol_daily[pos[\"sym\"]]\n",
        "            if date not in daily.index:\n",
        "                continue\n",
        "            cur_price = daily[\"Close\"][date]\n",
        "            # Individual SL\n",
        "            if cur_price < pos[\"entry_price\"] * (1 + INDIVIDUAL_SL_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"INDIVIDUAL_SL\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "            # Target Profit\n",
        "            elif cur_price >= pos[\"entry_price\"] * (1 + TARGET_PROFIT_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"TARGET_PROFIT\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "        for pos in to_remove:\n",
        "            open_positions.remove(pos)\n",
        "\n",
        "    # Handle entries: if prev_date has signal, enter at next day's open\n",
        "    prev_date = get_prev_trading_day(date, unique_trade_dates)\n",
        "    if prev_date is not None:\n",
        "        day_signals = ranked_df[ranked_df[\"SIGNAL_DATE\"] == prev_date]\n",
        "        if not day_signals.empty:\n",
        "            for _, signal in day_signals.iterrows():\n",
        "                sym = signal[\"SYMBOL\"]\n",
        "                if sym not in symbol_daily:\n",
        "                    continue\n",
        "                daily = symbol_daily[sym]\n",
        "                if date not in daily.index:\n",
        "                    continue\n",
        "                entry_price = daily[\"Open\"][date]\n",
        "                if entry_price <= 0:\n",
        "                    continue\n",
        "                quantity = 1\n",
        "                open_positions.append({\n",
        "                    \"sym\": sym,\n",
        "                    \"signal_date\": prev_date,\n",
        "                    \"trade_date\": date,\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"entry_price\": entry_price,\n",
        "                    \"quantity\": quantity\n",
        "                })\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"EXIT_DATE\"])\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "\n",
        "# Save to Excel with multiple sheets\n",
        "with pd.ExcelWriter(\"OUTPUT_BACKTEST.xlsx\") as writer:\n",
        "    output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "\n",
        "    # Stock-wise summary sheet with success rate\n",
        "    if not output_df.empty:\n",
        "        stock_summary = output_df.groupby(\"SYMBOL\").agg(\n",
        "            TOTAL_PNL=(\"PNL\", \"sum\"),\n",
        "            AVG_TRADE_ROI=(\"TRADE_ROI%\", \"mean\"),\n",
        "            NUM_TRADES=(\"PNL\", \"count\"),\n",
        "            NUM_WIN_TRADES=(\"PNL\", lambda x: (x > 0).sum()),\n",
        "            TRADE_DATES=(\"TRADE_DATE\", lambda x: list(x)),\n",
        "            EXIT_DATES=(\"EXIT_DATE\", lambda x: list(x))\n",
        "        ).reset_index()\n",
        "        stock_summary[\"WIN_RATE%\"] = (stock_summary[\"NUM_WIN_TRADES\"] / stock_summary[\"NUM_TRADES\"]) * 100\n",
        "        stock_summary.to_excel(writer, sheet_name=\"Stock_Summary\", index=False)\n",
        "\n",
        "    # Generate Daily PnL from executed trades\n",
        "    if not output_df.empty:\n",
        "        output_df[\"EXIT_DATE\"] = output_df[\"EXIT_DATE\"].dt.date\n",
        "        daily_pnl_df = output_df.groupby(\"EXIT_DATE\").agg({\n",
        "            \"PNL\": \"sum\",\n",
        "            \"TRADE_ROI%\": \"mean\",\n",
        "            \"SYMBOL\": \"count\"\n",
        "        }).reset_index()\n",
        "\n",
        "        daily_pnl_df.rename(columns={\n",
        "            \"SYMBOL\": \"NUM_TRADES\",\n",
        "            \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "            \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "        }, inplace=True)\n",
        "\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"Daily_PnL\", index=False)\n",
        "\n",
        "    # Generate Monthly PnL\n",
        "    if not output_df.empty:\n",
        "        monthly_pnl_df = output_df.copy()\n",
        "        monthly_pnl_df['MONTH'] = pd.to_datetime(monthly_pnl_df['EXIT_DATE']).dt.to_period('M')\n",
        "        monthly_pnl_df = monthly_pnl_df.groupby('MONTH').agg({\n",
        "            \"PNL\": \"sum\"\n",
        "        }).reset_index()\n",
        "        monthly_pnl_df.to_excel(writer, sheet_name=\"Monthly_PnL\", index=False)\n",
        "        print(f\"üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No trades found, skipping additional sheets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vrn4g5BgBtH",
        "outputId": "2c34fc5b-d269-48d2-ca7d-fbfb41611832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with daily data\n",
            "Loading NIFTY500 minute data...\n",
            "‚úÖ Found 283 potential trade dates from symbol data\n",
            "‚úÖ After date filtering: 283 trade dates\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 989 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\n",
            "‚úÖ 989 signals selected for trading (all recommendations)\n",
            "‚úÖ Backtest completed. 853 trades executed.\n",
            "üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = -0.05      # -5% individual SL\n",
        "TARGET_PROFIT_PCT = 0.10       # +10% target profit\n",
        "START_DATE = None  # e.g., \"2020-01-01\" or None for full period\n",
        "END_DATE = None    # e.g., \"2025-01-01\" or None for full period\n",
        "LOOKBACK_PERIODS = 30          # Lookback for max close price\n",
        "END_TIME = \"15:29\"             # Daily close time\n",
        "ALLOCATION_LIMIT = 2500        # Per stock allocation limit based on signal day close price\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars, filter for 15:29 daily close.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    # Filter for 15:29\n",
        "    df = df.filter(pl.col(\"TradeTime\") == END_TIME)\n",
        "    pdf = df.select([\"TradeDate\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "    pdf['TradeDate'] = pd.to_datetime(pdf['TradeDate'])\n",
        "    pdf = pdf.set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    return symbol, pdf\n",
        "\n",
        "def load_minute_data(file_path):\n",
        "    \"\"\"Read CSV with polars, without time filter.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    pdf = df.select([\"TradeDate\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "    pdf['TradeDate'] = pd.to_datetime(pdf['TradeDate'])\n",
        "\n",
        "    return symbol, pdf\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_daily = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, daily = load_full_data(f)\n",
        "    symbol_daily[symbol] = daily\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily)} symbols with daily data\")\n",
        "\n",
        "# Load NIFTY500 minute data\n",
        "print(\"Loading NIFTY500 minute data...\")\n",
        "index_file = os.path.join(data_path, \"cash_NIFTY 500.csv\")\n",
        "if os.path.exists(index_file):\n",
        "    _, index_minute = load_minute_data(index_file)\n",
        "else:\n",
        "    raise ValueError(\"cash_NIFTY 500.csv not found\")\n",
        "index_1526 = index_minute[index_minute['TradeTime'] == \"15:26\"][['TradeDate', 'Close']].set_index('TradeDate').sort_index()\n",
        "index_1526.columns = [\"Close_1526\"]\n",
        "\n",
        "# Build list of unique trading dates from all symbols\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Filter unique_trade_dates based on START_DATE and END_DATE\n",
        "unique_trade_dates = [pd.Timestamp(dt) for dt in unique_trade_dates]\n",
        "if START_DATE:\n",
        "    start_dt = pd.to_datetime(START_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt >= start_dt]\n",
        "if END_DATE:\n",
        "    end_dt = pd.to_datetime(END_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt <= end_dt]\n",
        "print(f\"‚úÖ After date filtering: {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if d < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# Helper function to get n days back\n",
        "def get_n_days_back(trade_date, all_dates, n):\n",
        "    \"\"\"Return the trading day n days back from trade_date.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    try:\n",
        "        idx = all_dates.index(trade_date)\n",
        "        if idx < n:\n",
        "            return None\n",
        "        return all_dates[idx - n]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each trading day -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, BREAKOUT_STRENGTH\n",
        "all_breakdowns = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    # Check NIFTY500 condition\n",
        "    if date not in index_1526.index:\n",
        "        continue\n",
        "    current_1526 = index_1526[\"Close_1526\"][date]\n",
        "    lookback_start = get_n_days_back(date, unique_trade_dates, 30)\n",
        "    if lookback_start is None:\n",
        "        continue\n",
        "    if \"cash_NIFTY 500\" not in symbol_daily:\n",
        "        continue\n",
        "    index_daily = symbol_daily[\"cash_NIFTY 500\"]\n",
        "    if date not in index_daily.index:\n",
        "        continue\n",
        "    prev_data = index_daily.loc[lookback_start:date].iloc[:-1]\n",
        "    if len(prev_data) < 30:\n",
        "        continue\n",
        "    max_prev_close = prev_data[\"Close\"].max()\n",
        "    if current_1526 <= max_prev_close:\n",
        "        continue\n",
        "    # Condition passed, proceed with signals\n",
        "    for sym, daily in symbol_daily.items():\n",
        "        if sym == \"cash_NIFTY 500\":\n",
        "            continue\n",
        "        if date not in daily.index:\n",
        "            continue\n",
        "        df_up_to = daily.loc[:date]\n",
        "        if len(df_up_to) < LOOKBACK_PERIODS + 1:\n",
        "            continue\n",
        "        close = daily['Close'][date]\n",
        "        lookback_start = get_n_days_back(date, unique_trade_dates, LOOKBACK_PERIODS)\n",
        "        if lookback_start is None:\n",
        "            continue\n",
        "        lookback_data = df_up_to.loc[lookback_start:date].iloc[:-1]\n",
        "        if len(lookback_data) < LOOKBACK_PERIODS:\n",
        "            continue\n",
        "        max_close = lookback_data['Close'].max()\n",
        "        if close <= max_close:\n",
        "            continue\n",
        "        # Check for pullback and re-breakout\n",
        "        breakout_date = None\n",
        "        breakout_high = None\n",
        "        for check_date in df_up_to.index[::-1]:\n",
        "            if check_date >= date:\n",
        "                continue\n",
        "            if df_up_to['Close'][check_date] > max_close:\n",
        "                breakout_date = check_date\n",
        "                breakout_high = df_up_to['High'][check_date]\n",
        "                break\n",
        "        if breakout_date is None:\n",
        "            continue\n",
        "        post_breakout = df_up_to.loc[breakout_date:date].iloc[1:-1]\n",
        "        if post_breakout.empty:\n",
        "            continue\n",
        "        pullback_occurred = (post_breakout['Close'] < breakout_high).any()\n",
        "        if not pullback_occurred:\n",
        "            continue\n",
        "        if close <= breakout_high:\n",
        "            continue\n",
        "        # Calculate breakout strength as percentage gain of breakout candle\n",
        "        breakout_open = df_up_to['Open'][breakout_date]\n",
        "        if breakout_open <= 0:\n",
        "            continue\n",
        "        breakout_strength = (df_up_to['Close'][breakout_date] - breakout_open) / breakout_open\n",
        "        if close < ALLOCATION_LIMIT:\n",
        "            all_breakdowns.append([\n",
        "                date,\n",
        "                sym,\n",
        "                breakout_strength\n",
        "            ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"BREAKOUT_STRENGTH\"])\n",
        "breakdown_df[\"BREAKOUT_STRENGTH\"] = breakdown_df[\"BREAKOUT_STRENGTH\"].astype(float).round(6)\n",
        "breakdown_df['SIGNAL_DATE'] = pd.to_datetime(breakdown_df['SIGNAL_DATE'])\n",
        "breakdown_df = breakdown_df.sort_values(['SIGNAL_DATE', 'BREAKOUT_STRENGTH'], ascending=[True, False])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(f\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\")\n",
        "\n",
        "# ----- No ranking, enter all signals -----\n",
        "ranked_df = breakdown_df.copy()\n",
        "ranked_df[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "print(f\"‚úÖ {len(ranked_df)} signals selected for trading (all recommendations)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "open_positions = []\n",
        "output_trades = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    date = pd.Timestamp(date)\n",
        "    # Handle individual SL and target profit\n",
        "    if open_positions:\n",
        "        to_remove = []\n",
        "        for pos in open_positions:\n",
        "            if pos[\"sym\"] not in symbol_daily:\n",
        "                continue\n",
        "            daily = symbol_daily[pos[\"sym\"]]\n",
        "            if date not in daily.index:\n",
        "                continue\n",
        "            cur_price = daily[\"Close\"][date]\n",
        "            # Individual SL\n",
        "            if cur_price < pos[\"entry_price\"] * (1 + INDIVIDUAL_SL_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"INDIVIDUAL_SL\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "            # Target Profit\n",
        "            elif cur_price >= pos[\"entry_price\"] * (1 + TARGET_PROFIT_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"TARGET_PROFIT\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "        for pos in to_remove:\n",
        "            open_positions.remove(pos)\n",
        "\n",
        "    # Handle entries: if prev_date has signal, enter at next day's open\n",
        "    prev_date = get_prev_trading_day(date, unique_trade_dates)\n",
        "    if prev_date is not None:\n",
        "        day_signals = ranked_df[ranked_df[\"SIGNAL_DATE\"] == prev_date]\n",
        "        if not day_signals.empty:\n",
        "            for _, signal in day_signals.iterrows():\n",
        "                sym = signal[\"SYMBOL\"]\n",
        "                if sym not in symbol_daily:\n",
        "                    continue\n",
        "                daily = symbol_daily[sym]\n",
        "                if date not in daily.index:\n",
        "                    continue\n",
        "                entry_price = daily[\"Open\"][date]\n",
        "                if entry_price <= 0:\n",
        "                    continue\n",
        "                quantity = 1\n",
        "                open_positions.append({\n",
        "                    \"sym\": sym,\n",
        "                    \"signal_date\": prev_date,\n",
        "                    \"trade_date\": date,\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"entry_price\": entry_price,\n",
        "                    \"quantity\": quantity\n",
        "                })\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"EXIT_DATE\"])\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "\n",
        "# Save to Excel with multiple sheets\n",
        "with pd.ExcelWriter(\"OUTPUT_BACKTEST.xlsx\") as writer:\n",
        "    output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "\n",
        "    # Stock-wise summary sheet with success rate\n",
        "    if not output_df.empty:\n",
        "        stock_summary = output_df.groupby(\"SYMBOL\").agg(\n",
        "            TOTAL_PNL=(\"PNL\", \"sum\"),\n",
        "            AVG_TRADE_ROI=(\"TRADE_ROI%\", \"mean\"),\n",
        "            NUM_TRADES=(\"PNL\", \"count\"),\n",
        "            NUM_WIN_TRADES=(\"PNL\", lambda x: (x > 0).sum()),\n",
        "            TRADE_DATES=(\"TRADE_DATE\", lambda x: list(x)),\n",
        "            EXIT_DATES=(\"EXIT_DATE\", lambda x: list(x))\n",
        "        ).reset_index()\n",
        "        stock_summary[\"WIN_RATE%\"] = (stock_summary[\"NUM_WIN_TRADES\"] / stock_summary[\"NUM_TRADES\"]) * 100\n",
        "        stock_summary.to_excel(writer, sheet_name=\"Stock_Summary\", index=False)\n",
        "\n",
        "    # Generate Daily PnL from executed trades\n",
        "    if not output_df.empty:\n",
        "        output_df[\"EXIT_DATE\"] = output_df[\"EXIT_DATE\"].dt.date\n",
        "        daily_pnl_df = output_df.groupby(\"EXIT_DATE\").agg({\n",
        "            \"PNL\": \"sum\",\n",
        "            \"TRADE_ROI%\": \"mean\",\n",
        "            \"SYMBOL\": \"count\"\n",
        "        }).reset_index()\n",
        "\n",
        "        daily_pnl_df.rename(columns={\n",
        "            \"SYMBOL\": \"NUM_TRADES\",\n",
        "            \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "            \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "        }, inplace=True)\n",
        "\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"Daily_PnL\", index=False)\n",
        "\n",
        "    # Generate Monthly PnL\n",
        "    if not output_df.empty:\n",
        "        monthly_pnl_df = output_df.copy()\n",
        "        monthly_pnl_df['MONTH'] = pd.to_datetime(monthly_pnl_df['EXIT_DATE']).dt.to_period('M')\n",
        "        monthly_pnl_df = monthly_pnl_df.groupby('MONTH').agg({\n",
        "            \"PNL\": \"sum\"\n",
        "        }).reset_index()\n",
        "        monthly_pnl_df.to_excel(writer, sheet_name=\"Monthly_PnL\", index=False)\n",
        "        print(f\"üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No trades found, skipping additional sheets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aGzy5i228Ct",
        "outputId": "bf8f6852-d9a2-46a7-cfcc-5562afca3292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with daily data\n",
            "‚úÖ Found 283 potential trade dates from symbol data\n",
            "‚úÖ After date filtering: 283 trade dates\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 65295 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\n",
            "‚úÖ 65295 signals selected for trading (all recommendations)\n",
            "‚úÖ Backtest completed. 32950 trades executed.\n",
            "üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = -0.05      # -5% individual SL\n",
        "TARGET_PROFIT_PCT = 0.10       # +10% target profit\n",
        "START_DATE = None  # e.g., \"2020-01-01\" or None for full period\n",
        "END_DATE = None    # e.g., \"2025-01-01\" or None for full period\n",
        "LOOKBACK_PERIODS = 30          # Lookback for max close price\n",
        "END_TIME = \"15:29\"             # Daily close time\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars, aggregate daily OHLCV, and extract specific close times.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    # Aggregate to daily OHLCV\n",
        "    daily_agg = df.group_by(\"TradeDate\").agg([\n",
        "        pl.col(\"Open\").first(),\n",
        "        pl.col(\"High\").max(),\n",
        "        pl.col(\"Low\").min(),\n",
        "        pl.col(\"Close\").last(),\n",
        "        pl.col(\"Volume\").sum()\n",
        "    ])\n",
        "    daily_pdf = daily_agg.sort(\"TradeDate\").to_pandas()\n",
        "    daily_pdf['TradeDate'] = pd.to_datetime(daily_pdf['TradeDate'])\n",
        "    daily_pdf = daily_pdf.set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    # Extract 15:26 and 15:29 closes\n",
        "    close_times = df.filter(pl.col(\"TradeTime\").is_in([\"15:26\", \"15:29\"]))\n",
        "    close_pdf = close_times.select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    close_pdf['dt'] = pd.to_datetime(close_pdf['dt'])\n",
        "    close_pdf = close_pdf.set_index(\"dt\").sort_index()\n",
        "\n",
        "    return symbol, daily_pdf, close_pdf\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_daily = {}\n",
        "symbol_closes = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, daily, closes = load_full_data(f)\n",
        "    symbol_daily[symbol] = daily\n",
        "    symbol_closes[symbol] = closes\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily)} symbols with daily data\")\n",
        "\n",
        "# Build list of unique trading dates from all symbols\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Filter unique_trade_dates based on START_DATE and END_DATE\n",
        "unique_trade_dates = [pd.Timestamp(dt) for dt in unique_trade_dates]\n",
        "if START_DATE:\n",
        "    start_dt = pd.to_datetime(START_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt >= start_dt]\n",
        "if END_DATE:\n",
        "    end_dt = pd.to_datetime(END_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt <= end_dt]\n",
        "print(f\"‚úÖ After date filtering: {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if d < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# Helper function to get n days back\n",
        "def get_n_days_back(trade_date, all_dates, n):\n",
        "    \"\"\"Return the trading day n days back from trade_date.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    try:\n",
        "        idx = all_dates.index(trade_date)\n",
        "        if idx < n:\n",
        "            return None\n",
        "        return all_dates[idx - n]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each trading day -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, BREAKOUT_STRENGTH\n",
        "all_breakdowns = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    prev_date = get_prev_trading_day(date, unique_trade_dates)\n",
        "    if prev_date is None:\n",
        "        continue\n",
        "    for sym, daily in symbol_daily.items():\n",
        "        if date not in daily.index or prev_date not in daily.index:\n",
        "            continue\n",
        "        prev = daily.loc[prev_date]\n",
        "        PP = (prev['High'] + prev['Low'] + prev['Close']) / 3.0\n",
        "        close = daily['Close'][date]\n",
        "        if close <= PP:\n",
        "            continue\n",
        "        breakout_strength = (close - PP) / PP\n",
        "        all_breakdowns.append([\n",
        "            date,\n",
        "            sym,\n",
        "            breakout_strength\n",
        "        ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"BREAKOUT_STRENGTH\"])\n",
        "breakdown_df[\"BREAKOUT_STRENGTH\"] = breakdown_df[\"BREAKOUT_STRENGTH\"].astype(float).round(6)\n",
        "breakdown_df['SIGNAL_DATE'] = pd.to_datetime(breakdown_df['SIGNAL_DATE'])\n",
        "breakdown_df = breakdown_df.sort_values(['SIGNAL_DATE', 'BREAKOUT_STRENGTH'], ascending=[True, False])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(f\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\")\n",
        "\n",
        "# ----- No ranking, enter all signals -----\n",
        "ranked_df = breakdown_df.copy()\n",
        "ranked_df[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "print(f\"‚úÖ {len(ranked_df)} signals selected for trading (all recommendations)\")\n",
        "\n",
        "# ----- Backtest/execution loop -----\n",
        "open_positions = []\n",
        "output_trades = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    date = pd.Timestamp(date)\n",
        "    # Handle individual SL and target profit\n",
        "    if open_positions:\n",
        "        to_remove = []\n",
        "        for pos in open_positions:\n",
        "            if pos[\"sym\"] not in symbol_daily:\n",
        "                continue\n",
        "            daily = symbol_daily[pos[\"sym\"]]\n",
        "            if date not in daily.index:\n",
        "                continue\n",
        "            cur_price = daily[\"Close\"][date]\n",
        "            # Individual SL\n",
        "            if cur_price < pos[\"entry_price\"] * (1 + INDIVIDUAL_SL_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"INDIVIDUAL_SL\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "            # Target Profit\n",
        "            elif cur_price >= pos[\"entry_price\"] * (1 + TARGET_PROFIT_PCT):\n",
        "                exit_price = cur_price\n",
        "                exit_reason = \"TARGET_PROFIT\"\n",
        "                trade_pnl = (exit_price - pos[\"entry_price\"]) * pos[\"quantity\"]\n",
        "                invested = pos[\"quantity\"] * pos[\"entry_price\"]\n",
        "                roi_trade = (trade_pnl / invested) * 100 if invested != 0 else 0\n",
        "                output_trades.append([\n",
        "                    pos[\"sym\"],\n",
        "                    pos[\"signal_date\"],\n",
        "                    pos[\"trade_date\"],\n",
        "                    pos[\"side\"],\n",
        "                    pos[\"entry_price\"],\n",
        "                    exit_price,\n",
        "                    trade_pnl,\n",
        "                    roi_trade,\n",
        "                    exit_reason,\n",
        "                    date\n",
        "                ])\n",
        "                to_remove.append(pos)\n",
        "        for pos in to_remove:\n",
        "            open_positions.remove(pos)\n",
        "\n",
        "    # Handle entries: if prev_date has signal, enter at next day's 15:26 close if conditions met\n",
        "    prev_date = get_prev_trading_day(date, unique_trade_dates)\n",
        "    if prev_date is not None:\n",
        "        day_signals = ranked_df[ranked_df[\"SIGNAL_DATE\"] == prev_date]\n",
        "        if not day_signals.empty:\n",
        "            for _, signal in day_signals.iterrows():\n",
        "                sym = signal[\"SYMBOL\"]\n",
        "                if sym not in symbol_daily or sym not in symbol_closes:\n",
        "                    continue\n",
        "                daily = symbol_daily[sym]\n",
        "                closes = symbol_closes[sym]\n",
        "                if date not in daily.index:\n",
        "                    continue\n",
        "                # Get 15:26 datetime\n",
        "                entry_1526_dt = pd.Timestamp(year=date.year, month=date.month, day=date.day, hour=15, minute=26, second=0)\n",
        "                if entry_1526_dt not in closes.index:\n",
        "                    continue\n",
        "                entry_price = closes.loc[entry_1526_dt, \"Close\"]\n",
        "                if entry_price <= 0:\n",
        "                    continue\n",
        "                # Calculate pivot for entry day based on prev_date (signal day)\n",
        "                prev_for_pivot = daily.loc[prev_date]\n",
        "                PP = (prev_for_pivot[\"High\"] + prev_for_pivot[\"Low\"] + prev_for_pivot[\"Close\"]) / 3.0\n",
        "                if entry_price < PP:\n",
        "                    continue\n",
        "                quantity = 1\n",
        "                open_positions.append({\n",
        "                    \"sym\": sym,\n",
        "                    \"signal_date\": prev_date,\n",
        "                    \"trade_date\": date,\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"entry_price\": entry_price,\n",
        "                    \"quantity\": quantity\n",
        "                })\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\", \"EXIT_DATE\"])\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "\n",
        "# Save to Excel with multiple sheets\n",
        "with pd.ExcelWriter(\"OUTPUT_BACKTEST.xlsx\") as writer:\n",
        "    output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "\n",
        "    # Stock-wise summary sheet with success rate\n",
        "    if not output_df.empty:\n",
        "        stock_summary = output_df.groupby(\"SYMBOL\").agg(\n",
        "            TOTAL_PNL=(\"PNL\", \"sum\"),\n",
        "            AVG_TRADE_ROI=(\"TRADE_ROI%\", \"mean\"),\n",
        "            NUM_TRADES=(\"PNL\", \"count\"),\n",
        "            NUM_WIN_TRADES=(\"PNL\", lambda x: (x > 0).sum()),\n",
        "            TRADE_DATES=(\"TRADE_DATE\", lambda x: list(x)),\n",
        "            EXIT_DATES=(\"EXIT_DATE\", lambda x: list(x))\n",
        "        ).reset_index()\n",
        "        stock_summary[\"WIN_RATE%\"] = (stock_summary[\"NUM_WIN_TRADES\"] / stock_summary[\"NUM_TRADES\"]) * 100\n",
        "        stock_summary.to_excel(writer, sheet_name=\"Stock_Summary\", index=False)\n",
        "\n",
        "    # Generate Daily PnL from executed trades\n",
        "    if not output_df.empty:\n",
        "        output_df[\"EXIT_DATE\"] = output_df[\"EXIT_DATE\"].dt.date\n",
        "        daily_pnl_df = output_df.groupby(\"EXIT_DATE\").agg({\n",
        "            \"PNL\": \"sum\",\n",
        "            \"TRADE_ROI%\": \"mean\",\n",
        "            \"SYMBOL\": \"count\"\n",
        "        }).reset_index()\n",
        "\n",
        "        daily_pnl_df.rename(columns={\n",
        "            \"SYMBOL\": \"NUM_TRADES\",\n",
        "            \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "            \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "        }, inplace=True)\n",
        "\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"Daily_PnL\", index=False)\n",
        "\n",
        "    # Generate Monthly PnL\n",
        "    if not output_df.empty:\n",
        "        monthly_pnl_df = output_df.copy()\n",
        "        monthly_pnl_df['MONTH'] = pd.to_datetime(monthly_pnl_df['EXIT_DATE']).dt.to_period('M')\n",
        "        monthly_pnl_df = monthly_pnl_df.groupby('MONTH').agg({\n",
        "            \"PNL\": \"sum\"\n",
        "        }).reset_index()\n",
        "        monthly_pnl_df.to_excel(writer, sheet_name=\"Monthly_PnL\", index=False)\n",
        "        print(f\"üìÑ Backtest results saved in: OUTPUT_BACKTEST.xlsx (with sheets: Trades, Stock_Summary, Daily_PnL, Monthly_PnL)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No trades found, skipping additional sheets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzOGiV6ig1E3",
        "outputId": "02d67880-6f56-4d32-e45d-9b6b85dbf539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7kkCH06zdVb"
      },
      "source": [
        "# Live Signal PA Swing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIJczxKFqBMz",
        "outputId": "7af308c4-c0d0-4c1f-f91d-3caaf3c241d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 503 cash files...\n",
            "‚úÖ Processed 50/503 symbols\n",
            "‚úÖ Processed 100/503 symbols\n",
            "‚úÖ Processed 150/503 symbols\n",
            "‚úÖ Processed 200/503 symbols\n",
            "‚úÖ Processed 250/503 symbols\n",
            "‚úÖ Processed 300/503 symbols\n",
            "‚úÖ Processed 350/503 symbols\n",
            "‚úÖ Processed 400/503 symbols\n",
            "‚úÖ Processed 450/503 symbols\n",
            "‚úÖ Processed 500/503 symbols\n",
            "‚úÖ Loaded 503 symbols with daily data\n",
            "‚úÖ Found 283 potential trade dates from symbol data\n",
            "‚úÖ After date filtering: 283 trade dates\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 3642 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\n",
            "üìÖ Latest available trading date in database: 2025-09-19\n",
            "Live Signals (latest available date):\n",
            "SIGNAL_DATE          SYMBOL  QUANTITY  BREAKOUT_STRENGTH\n",
            " 2025-09-19     cash_TBOTEK         5           0.010937\n",
            " 2025-09-19       cash_CESC        58           0.004642\n",
            " 2025-09-19  cash_REDINGTON        33           0.004091\n",
            " 2025-09-19       cash_IDEA      1180           0.003659\n",
            " 2025-09-19 cash_ADANIPOWER        13           0.003169\n",
            " 2025-09-19   cash_JSWINFRA        29           0.002422\n",
            " 2025-09-19   cash_ANANTRAJ        15           0.002329\n",
            " 2025-09-19  cash_PPLPHARMA        48           0.002189\n",
            " 2025-09-19 cash_FLUOROCHEM         2           0.001873\n",
            " 2025-09-19        cash_IOB       243           0.001482\n",
            " 2025-09-19 cash_BHARTIARTL         5           0.001336\n",
            " 2025-09-19 cash_ADANIPORTS         6           0.001134\n",
            " 2025-09-19 cash_SAMMAANCAP        69           0.001069\n",
            " 2025-09-19      cash_HUDCO        43           0.000933\n",
            " 2025-09-19     cash_ALIVUS         9           0.000910\n",
            " 2025-09-19 cash_ZENSARTECH        11           0.000880\n",
            " 2025-09-19     cash_SWIGGY        21           0.000878\n",
            " 2025-09-19      cash_CANBK        84           0.000769\n",
            " 2025-09-19     cash_AFCONS        21           0.000659\n",
            " 2025-09-19 cash_SHRIRAMFIN        15           0.000634\n",
            " 2025-09-19       cash_ATGL        15           0.000621\n",
            " 2025-09-19   cash_USHAMART        23           0.000599\n",
            " 2025-09-19 cash_CHOLAHLDNG         5           0.000554\n",
            " 2025-09-19   cash_MAHABANK       174           0.000524\n",
            " 2025-09-19   cash_TATATECH        13           0.000418\n",
            " 2025-09-19       cash_SBIN        11           0.000407\n",
            " 2025-09-19 cash_GODREJPROP         4           0.000379\n",
            " 2025-09-19       cash_BPCL        30           0.000306\n",
            " 2025-09-19        cash_HAL         2           0.000286\n",
            " 2025-09-19       cash_NBCC        90           0.000272\n",
            " 2025-09-19      cash_IREDA        62           0.000128\n",
            " 2025-09-19    cash_POLYCAB         1           0.000094\n",
            " 2025-09-19    cash_KPRMILL         8           0.000088\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = -0.05      # -5% individual SL\n",
        "TARGET_PROFIT_PCT = 0.10       # +10% target profit\n",
        "START_DATE = None  # e.g., \"2020-01-01\" or None for full period\n",
        "END_DATE = None    # e.g., \"2025-01-01\" or None for full period\n",
        "LOOKBACK_PERIODS = 30          # Lookback for max close price\n",
        "END_TIME = \"15:29\"             # Daily close time\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars, filter for 15:29 daily close.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    # Filter for 15:29\n",
        "    df = df.filter(pl.col(\"TradeTime\") == END_TIME)\n",
        "    pdf = df.select([\"TradeDate\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "    pdf['TradeDate'] = pd.to_datetime(pdf['TradeDate'])\n",
        "    pdf = pdf.set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    return symbol, pdf\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_daily = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, daily = load_full_data(f)\n",
        "    symbol_daily[symbol] = daily\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily)} symbols with daily data\")\n",
        "\n",
        "# Build list of unique trading dates from all symbols\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Filter unique_trade_dates based on START_DATE and END_DATE\n",
        "unique_trade_dates = [pd.Timestamp(dt) for dt in unique_trade_dates]\n",
        "if START_DATE:\n",
        "    start_dt = pd.to_datetime(START_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt >= start_dt]\n",
        "if END_DATE:\n",
        "    end_dt = pd.to_datetime(END_DATE)\n",
        "    unique_trade_dates = [dt for dt in unique_trade_dates if dt <= end_dt]\n",
        "print(f\"‚úÖ After date filtering: {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if d < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# Helper function to get n days back\n",
        "def get_n_days_back(trade_date, all_dates, n):\n",
        "    \"\"\"Return the trading day n days back from trade_date.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    try:\n",
        "        idx = all_dates.index(trade_date)\n",
        "        if idx < n:\n",
        "            return None\n",
        "        return all_dates[idx - n]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each trading day -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, BREAKOUT_STRENGTH\n",
        "all_breakdowns = []\n",
        "\n",
        "for date in unique_trade_dates:\n",
        "    for sym, daily in symbol_daily.items():\n",
        "        if date not in daily.index:\n",
        "            continue\n",
        "        df_up_to = daily.loc[:date]\n",
        "        if len(df_up_to) < LOOKBACK_PERIODS + 1:\n",
        "            continue\n",
        "        close = daily['Close'][date]\n",
        "        lookback_start = get_n_days_back(date, unique_trade_dates, LOOKBACK_PERIODS)\n",
        "        if lookback_start is None:\n",
        "            continue\n",
        "        lookback_data = df_up_to.loc[lookback_start:date].iloc[:-1]\n",
        "        if len(lookback_data) < LOOKBACK_PERIODS:\n",
        "            continue\n",
        "        max_close = lookback_data['Close'].max()\n",
        "        if close <= max_close:\n",
        "            continue\n",
        "        # Check for pullback and re-breakout\n",
        "        breakout_date = None\n",
        "        breakout_high = None\n",
        "        for check_date in df_up_to.index[::-1]:\n",
        "            if check_date >= date:\n",
        "                continue\n",
        "            if df_up_to['Close'][check_date] > max_close:\n",
        "                breakout_date = check_date\n",
        "                breakout_high = df_up_to['High'][check_date]\n",
        "                break\n",
        "        if breakout_date is None:\n",
        "            continue\n",
        "        post_breakout = df_up_to.loc[breakout_date:date].iloc[1:-1]\n",
        "        if post_breakout.empty:\n",
        "            continue\n",
        "        pullback_occurred = (post_breakout['Close'] < breakout_high).any()\n",
        "        if not pullback_occurred:\n",
        "            continue\n",
        "        if close <= breakout_high:\n",
        "            continue\n",
        "        # Calculate breakout strength as percentage gain of breakout candle\n",
        "        breakout_open = df_up_to['Open'][breakout_date]\n",
        "        if breakout_open <= 0:\n",
        "            continue\n",
        "        breakout_strength = (df_up_to['Close'][breakout_date] - breakout_open) / breakout_open\n",
        "        all_breakdowns.append([\n",
        "            date,\n",
        "            sym,\n",
        "            breakout_strength\n",
        "        ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"BREAKOUT_STRENGTH\"])\n",
        "breakdown_df[\"BREAKOUT_STRENGTH\"] = breakdown_df[\"BREAKOUT_STRENGTH\"].astype(float).round(6)\n",
        "breakdown_df['SIGNAL_DATE'] = pd.to_datetime(breakdown_df['SIGNAL_DATE'])\n",
        "breakdown_df = breakdown_df.sort_values(['SIGNAL_DATE', 'BREAKOUT_STRENGTH'], ascending=[True, False])\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(f\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, breakout strength)\")\n",
        "\n",
        "# Live signals for today\n",
        "PER_STOCK_ALLOC = 10000\n",
        "# ‚úÖ Instead of using today's system date, use latest SIGNAL_DATE in data\n",
        "latest_date = breakdown_df['SIGNAL_DATE'].max()\n",
        "print(f\"üìÖ Latest available trading date in database: {latest_date.date()}\")\n",
        "\n",
        "# Filter signals for that latest date\n",
        "latest_signals = breakdown_df[\n",
        "    (breakdown_df['SIGNAL_DATE'] == latest_date) &\n",
        "    (breakdown_df['BREAKOUT_STRENGTH'] > 0)\n",
        "]\n",
        "\n",
        "live_signals = []\n",
        "for _, row in latest_signals.iterrows():\n",
        "    sym = row['SYMBOL']\n",
        "    strength = row['BREAKOUT_STRENGTH']\n",
        "    if sym in symbol_daily:\n",
        "        daily = symbol_daily[sym]\n",
        "        close_price = daily['Close'].get(latest_date, None)\n",
        "        if close_price is not None and close_price > 0:\n",
        "            quantity = int(PER_STOCK_ALLOC / close_price)\n",
        "            if quantity > 0:\n",
        "                live_signals.append([latest_date, sym, quantity, strength])\n",
        "\n",
        "if live_signals:\n",
        "    live_df = pd.DataFrame(live_signals, columns=['SIGNAL_DATE', 'SYMBOL', 'QUANTITY', 'BREAKOUT_STRENGTH'])\n",
        "    print(\"Live Signals (latest available date):\")\n",
        "    print(live_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"No signals on the latest available date.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axk9YepVkSe_"
      },
      "source": [
        "# VWAP+BB Intraday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atEEEotukRup",
        "outputId": "48d7c90b-23cf-42b6-b8d2-bc9c49097510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "üìä 15-min bars: ['09:30', '09:45', '10:00', '10:15', '10:30', '10:45', '11:00', '11:15', '11:30', '11:45', '12:00', '12:15', '12:30', '12:45', '13:00', '13:15', '13:30', '13:45', '14:00', '14:15', '14:30', '14:45', '15:00', '15:15', '15:30']\n",
            "‚úÖ Found 256 trade dates after 30-day skip\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Aggregated 15-min data for 540 symbols\n",
            "‚úÖ Backtest completed. 8905 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt_mod\n",
        "import numpy as np\n",
        "\n",
        "# User-configurable params\n",
        "LOOKBACK_DAYS_SKIP = 30\n",
        "BB_LENGTH = 20\n",
        "BB_MULT = 1.0\n",
        "MAX_LONGS = 2\n",
        "MAX_SHORTS = 2\n",
        "TRADING_HOURS_END = dt_mod.time(15, 30)\n",
        "\n",
        "# Path with cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Generate 15-min bar end times\n",
        "def generate_bar_ends():\n",
        "    bar_ends = []\n",
        "    current = dt_mod.time(9, 30)\n",
        "    while current <= TRADING_HOURS_END:\n",
        "        bar_ends.append(current.strftime(\"%H:%M\"))\n",
        "        current_dt = dt_mod.datetime.combine(dt_mod.date.today(), current) + timedelta(minutes=15)\n",
        "        current = current_dt.time()\n",
        "    return bar_ends\n",
        "\n",
        "bar_ends = generate_bar_ends()\n",
        "print(f\"üìä 15-min bars: {bar_ends}\")\n",
        "\n",
        "# Pre-compute bar end times as minutes since midnight\n",
        "bar_ends_times = [datetime.strptime(t, \"%H:%M\").time() for t in bar_ends]\n",
        "bar_ends_minutes = [t.hour * 60 + t.minute for t in bar_ends_times]\n",
        "bar_ends_str = {bar_ends_times[i]: bar_ends[i] for i in range(len(bar_ends))}\n",
        "\n",
        "# Collect all unique trade dates\n",
        "all_dates = set()\n",
        "for f in all_files:\n",
        "    df = pl.read_csv(f, try_parse_dates=False, low_memory=True).rename({\"date\": \"Timestamp\"})\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").dt.date().alias(\"TradeDate\")\n",
        "    )\n",
        "    all_dates.update(df[\"TradeDate\"].unique().to_list())\n",
        "unique_trade_dates = sorted(all_dates)[LOOKBACK_DAYS_SKIP:]\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} trade dates after {LOOKBACK_DAYS_SKIP}-day skip\")\n",
        "\n",
        "def process_symbol(file_path, valid_dates):\n",
        "    \"\"\"Process one symbol: load, aggregate to 15-min, compute indicators.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Read with lazy evaluation and filter early\n",
        "    df_min = pl.scan_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ]).filter(\n",
        "        pl.col(\"TradeDate\").is_in(valid_dates) &\n",
        "        pl.col(\"TradeTime\").is_in(bar_ends)\n",
        "    ).sort(\"dt\").collect()\n",
        "\n",
        "    if df_min.height == 0:\n",
        "        return symbol, None, None\n",
        "\n",
        "    # Assign to 15-min buckets\n",
        "    def assign_bar_time(t):\n",
        "        minutes = t.hour * 60 + t.minute\n",
        "        idx = min(range(len(bar_ends_minutes)), key=lambda i: bar_ends_minutes[i] if bar_ends_minutes[i] >= minutes else float('inf'))\n",
        "        return bar_ends[idx]\n",
        "\n",
        "    df_min = df_min.with_columns(\n",
        "        pl.col(\"dt\").dt.time().map_elements(\n",
        "            assign_bar_time,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"TradeTime\")\n",
        "    )\n",
        "\n",
        "    # Aggregate to 15-min bars\n",
        "    df_15 = df_min.group_by([\"TradeDate\", \"TradeTime\"]).agg([\n",
        "        pl.col(\"Open\").first().cast(pl.Float64),\n",
        "        pl.col(\"High\").max().cast(pl.Float64),\n",
        "        pl.col(\"Low\").min().cast(pl.Float64),\n",
        "        pl.col(\"Close\").last().cast(pl.Float64),\n",
        "        pl.col(\"Volume\").sum().cast(pl.Float64)\n",
        "    ]).sort([\"TradeDate\", \"TradeTime\"]).filter(\n",
        "        pl.col(\"Close\").is_not_null() & (pl.col(\"Open\") != 0)\n",
        "    )\n",
        "\n",
        "    if df_15.height == 0:\n",
        "        return symbol, None, df_min\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df_15 = df_15.with_columns([\n",
        "        pl.col(\"Close\").rolling_mean(window_size=BB_LENGTH).alias(\"bb_middle\"),\n",
        "        pl.col(\"Close\").rolling_std(window_size=BB_LENGTH).alias(\"bb_std\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"bb_middle\") + BB_MULT * pl.col(\"bb_std\")).alias(\"BB_upper\"),\n",
        "        (pl.col(\"bb_middle\") - BB_MULT * pl.col(\"bb_std\")).alias(\"BB_lower\")\n",
        "    ])\n",
        "\n",
        "    # Session VWAP\n",
        "    df_15 = df_15.with_columns([\n",
        "        (pl.col(\"Close\") * pl.col(\"Volume\")).cum_sum().over(\"TradeDate\").alias(\"cum_pv\"),\n",
        "        pl.col(\"Volume\").cum_sum().over(\"TradeDate\").alias(\"cum_v\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"cum_pv\") / pl.col(\"cum_v\")).alias(\"VWAP\")\n",
        "    ])\n",
        "\n",
        "    # Signals\n",
        "    df_15 = df_15.with_columns([\n",
        "        pl.when(\n",
        "            (pl.col(\"Close\") > pl.col(\"BB_upper\")) & (pl.col(\"Close\") > pl.col(\"VWAP\"))\n",
        "        ).then(pl.lit(\"LONG\")).when(\n",
        "            (pl.col(\"Close\") < pl.col(\"BB_lower\")) & (pl.col(\"Close\") < pl.col(\"VWAP\"))\n",
        "        ).then(pl.lit(\"SHORT\")).otherwise(None).alias(\"SIGNAL\"),\n",
        "        ((pl.col(\"Close\") - pl.col(\"Open\")) / pl.col(\"Open\") * 100).alias(\"body_pct\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df_15, df_min\n",
        "\n",
        "# Process symbols sequentially to save memory\n",
        "symbol_15min_data = {}\n",
        "symbol_min_data = {}  # Store minimal minute data for backtest\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df_15, df_min = process_symbol(f, unique_trade_dates)\n",
        "    if df_15 is not None:\n",
        "        symbol_15min_data[symbol] = df_15\n",
        "    if df_min is not None:\n",
        "        # Keep only necessary columns to reduce memory\n",
        "        symbol_min_data[symbol] = df_min.select([\"TradeDate\", \"TradeTime\", \"High\", \"Low\"])\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "print(f\"‚úÖ Aggregated 15-min data for {len(symbol_15min_data)} symbols\")\n",
        "\n",
        "# ----- Backtest -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "bar_ends_dict = {t: i for i, t in enumerate(bar_ends)}\n",
        "\n",
        "for trade_date_pl in unique_trade_dates:\n",
        "    trade_date_str = trade_date_pl.strftime(\"%Y-%m-%d\")\n",
        "    positions = []\n",
        "    current_longs = 0\n",
        "    current_shorts = 0\n",
        "    signaled_longs = set()\n",
        "    signaled_shorts = set()\n",
        "\n",
        "    # Collect all signals for the day\n",
        "    day_signals = []\n",
        "    for sym, df_15 in symbol_15min_data.items():\n",
        "        df15_day = df_15.filter(pl.col(\"TradeDate\") == trade_date_pl)\n",
        "        if df15_day.height == 0:\n",
        "            continue\n",
        "        df15_day = df15_day.select([\n",
        "            \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"SIGNAL\", \"body_pct\"\n",
        "        ]).filter(pl.col(\"SIGNAL\").is_not_null() & pl.col(\"body_pct\").is_not_null() & (pl.col(\"Close\") != 0))\n",
        "        for row in df15_day.to_dicts():\n",
        "            if row[\"SIGNAL\"] == \"LONG\" and sym not in signaled_longs:\n",
        "                day_signals.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"time\": row[\"TradeTime\"],\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"open\": row[\"Open\"],\n",
        "                    \"high\": row[\"High\"],\n",
        "                    \"low\": row[\"Low\"],\n",
        "                    \"close\": row[\"Close\"],\n",
        "                    \"strength\": row[\"body_pct\"]\n",
        "                })\n",
        "                signaled_longs.add(sym)\n",
        "            elif row[\"SIGNAL\"] == \"SHORT\" and sym not in signaled_shorts:\n",
        "                day_signals.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"time\": row[\"TradeTime\"],\n",
        "                    \"side\": \"SHORT\",\n",
        "                    \"open\": row[\"Open\"],\n",
        "                    \"high\": row[\"High\"],\n",
        "                    \"low\": row[\"Low\"],\n",
        "                    \"close\": row[\"Close\"],\n",
        "                    \"strength\": -row[\"body_pct\"]\n",
        "                })\n",
        "                signaled_shorts.add(sym)\n",
        "\n",
        "    # Process each bar\n",
        "    for bar_idx, bar_time_str in enumerate(bar_ends):\n",
        "        prev_bar_time = bar_ends[bar_idx - 1] if bar_idx > 0 else None\n",
        "\n",
        "        # Check existing positions\n",
        "        new_positions = []\n",
        "        for pos in positions:\n",
        "            sym = pos['sym']\n",
        "            side = pos['side']\n",
        "            entry_price = pos['entry_price']\n",
        "            sl_price = pos['sl_price']\n",
        "            target_price = pos['target_price']\n",
        "            entry_time_str = pos['entry_time']\n",
        "\n",
        "            hit = False\n",
        "            exit_price = None\n",
        "            reason = None\n",
        "\n",
        "            if prev_bar_time is not None:\n",
        "                df_min_day = symbol_min_data.get(sym, pl.DataFrame()).filter(pl.col(\"TradeDate\") == trade_date_pl)\n",
        "                mask = (pl.col(\"TradeTime\") > prev_bar_time) & (pl.col(\"TradeTime\") <= bar_time_str)\n",
        "                bar_data = df_min_day.filter(mask).select([\n",
        "                    pl.max(\"High\").alias(\"bar_high\"),\n",
        "                    pl.min(\"Low\").alias(\"bar_low\")\n",
        "                ])\n",
        "                if bar_data.height > 0:\n",
        "                    bar_high = bar_data[\"bar_high\"][0]\n",
        "                    bar_low = bar_data[\"bar_low\"][0]\n",
        "                    if not (pd.isna(bar_high) or pd.isna(bar_low)):\n",
        "                        if side == \"LONG\":\n",
        "                            if bar_low <= sl_price:\n",
        "                                exit_price = sl_price\n",
        "                                reason = \"SL\"\n",
        "                                hit = True\n",
        "                            elif bar_high >= target_price:\n",
        "                                exit_price = target_price\n",
        "                                reason = \"TARGET\"\n",
        "                                hit = True\n",
        "                        else:  # SHORT\n",
        "                            if bar_high >= sl_price:\n",
        "                                exit_price = sl_price\n",
        "                                reason = \"SL\"\n",
        "                                hit = True\n",
        "                            elif bar_low <= target_price:\n",
        "                                exit_price = target_price\n",
        "                                reason = \"TARGET\"\n",
        "                                hit = True\n",
        "\n",
        "            if not hit:\n",
        "                new_positions.append(pos)\n",
        "            else:\n",
        "                if side == \"LONG\":\n",
        "                    pnl = exit_price - entry_price\n",
        "                    roi_trade = (pnl / entry_price) * 100\n",
        "                else:\n",
        "                    pnl = entry_price - exit_price\n",
        "                    roi_trade = (pnl / entry_price) * 100\n",
        "                cumulative_portfolio_return += roi_trade\n",
        "                output_trades.append([\n",
        "                    sym, entry_time_str, trade_date_str, side,\n",
        "                    round(entry_price, 2), round(exit_price, 2),\n",
        "                    round(pnl, 2), round(roi_trade, 2),\n",
        "                    f\"{reason}_{bar_time_str}\", round(cumulative_portfolio_return, 2)\n",
        "                ])\n",
        "\n",
        "        positions = new_positions\n",
        "        current_longs = sum(1 for p in positions if p['side'] == 'LONG')\n",
        "        current_shorts = sum(1 for p in positions if p['side'] == 'SHORT')\n",
        "\n",
        "        # Enter new positions\n",
        "        bar_signals = [s for s in day_signals if s[\"time\"] == bar_time_str]\n",
        "        bar_signals.sort(key=lambda x: x[\"strength\"], reverse=True)\n",
        "\n",
        "        for sig in bar_signals:\n",
        "            sym = sig[\"symbol\"]\n",
        "            side = sig[\"side\"]\n",
        "            o, h, l, c = sig[\"open\"], sig[\"high\"], sig[\"low\"], sig[\"close\"]\n",
        "            t_str = sig[\"time\"]\n",
        "\n",
        "            if side == \"LONG\" and current_longs < MAX_LONGS:\n",
        "                length = c - o\n",
        "                target_p = c + 2 * length\n",
        "                sl_p = l\n",
        "                positions.append({\n",
        "                    'sym': sym, 'side': 'LONG', 'entry_price': c,\n",
        "                    'sl_price': sl_p, 'target_price': target_p, 'entry_time': t_str\n",
        "                })\n",
        "                current_longs += 1\n",
        "            elif side == \"SHORT\" and current_shorts < MAX_SHORTS:\n",
        "                length = o - c\n",
        "                target_p = c - 2 * length\n",
        "                sl_p = h\n",
        "                positions.append({\n",
        "                    'sym': sym, 'side': 'SHORT', 'entry_price': c,\n",
        "                    'sl_price': sl_p, 'target_price': target_p, 'entry_time': t_str\n",
        "                })\n",
        "                current_shorts += 1\n",
        "\n",
        "    # EOD exits\n",
        "    last_bar_str = bar_ends[-1]\n",
        "    for pos in positions:\n",
        "        sym = pos['sym']\n",
        "        side = pos['side']\n",
        "        entry_price = pos['entry_price']\n",
        "        entry_time_str = pos['entry_time']\n",
        "\n",
        "        df15_last = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "            (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == last_bar_str)\n",
        "        )\n",
        "        if df15_last.height == 0:\n",
        "            continue\n",
        "        exit_price = df15_last[\"Close\"][0]\n",
        "        reason = \"EOD\"\n",
        "\n",
        "        if side == \"LONG\":\n",
        "            pnl = exit_price - entry_price\n",
        "            roi_trade = (pnl / entry_price) * 100\n",
        "        else:\n",
        "            pnl = entry_price - exit_price\n",
        "            roi_trade = (pnl / entry_price) * 100\n",
        "        cumulative_portfolio_return += roi_trade\n",
        "        output_trades.append([\n",
        "            sym, entry_time_str, trade_date_str, side,\n",
        "            round(entry_price, 2), round(exit_price, 2),\n",
        "            round(pnl, 2), round(roi_trade, 2),\n",
        "            f\"{reason}_{last_bar_str}\", round(cumulative_portfolio_return, 2)\n",
        "        ])\n",
        "\n",
        "# Create output DataFrame\n",
        "if output_trades:\n",
        "    output_df = pd.DataFrame(output_trades, columns=[\n",
        "        \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "        \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"ROI%\", \"EXIT_REASON\",\n",
        "        \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "    ])\n",
        "    output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "    print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "    print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "    # Daily PnL\n",
        "    daily_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"ROI%\": \"sum\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "    daily_df.rename(columns={\"ROI%\": \"DAILY_ROI%\", \"SYMBOL\": \"NUM_TRADES\"}, inplace=True)\n",
        "    daily_df[\"CUMULATIVE_ROI%\"] = daily_df[\"DAILY_ROI%\"].cumsum()\n",
        "    daily_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found.\")\n",
        "    output_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqjY1qiRkWwQ",
        "outputId": "dc77a091-2bf7-4a4f-b4e9-2340aab12d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "üìä 15-min bars: ['09:30', '09:45', '10:00', '10:15', '10:30', '10:45', '11:00', '11:15', '11:30', '11:45', '12:00', '12:15', '12:30', '12:45', '13:00', '13:15', '13:30', '13:45', '14:00', '14:15', '14:30', '14:45', '15:00', '15:15', '15:30']\n",
            "‚úÖ Found 256 trade dates after 30-day skip\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Aggregated 15-min data for 540 symbols\n",
            "‚úÖ Backtest completed. 2901 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt_mod\n",
        "import numpy as np\n",
        "\n",
        "# User-configurable params\n",
        "LOOKBACK_DAYS_SKIP = 30\n",
        "BB_LENGTH = 20\n",
        "BB_MULT = 1.0\n",
        "MAX_LONGS = 2\n",
        "MAX_SHORTS = 2\n",
        "TRADING_HOURS_END = dt_mod.time(15, 30)\n",
        "\n",
        "# Path with cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Generate 15-min bar end times\n",
        "def generate_bar_ends():\n",
        "    bar_ends = []\n",
        "    current = dt_mod.time(9, 30)\n",
        "    while current <= TRADING_HOURS_END:\n",
        "        bar_ends.append(current.strftime(\"%H:%M\"))\n",
        "        current_dt = dt_mod.datetime.combine(dt_mod.date.today(), current) + timedelta(minutes=15)\n",
        "        current = current_dt.time()\n",
        "    return bar_ends\n",
        "\n",
        "bar_ends = generate_bar_ends()\n",
        "print(f\"üìä 15-min bars: {bar_ends}\")\n",
        "\n",
        "# Pre-compute bar end times as minutes since midnight\n",
        "bar_ends_times = [datetime.strptime(t, \"%H:%M\").time() for t in bar_ends]\n",
        "bar_ends_minutes = [t.hour * 60 + t.minute for t in bar_ends_times]\n",
        "bar_ends_str = {bar_ends_times[i]: bar_ends[i] for i in range(len(bar_ends))}\n",
        "\n",
        "# Collect all unique trade dates\n",
        "all_dates = set()\n",
        "for f in all_files:\n",
        "    df = pl.read_csv(f, try_parse_dates=False, low_memory=True).rename({\"date\": \"Timestamp\"})\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").dt.date().alias(\"TradeDate\")\n",
        "    )\n",
        "    all_dates.update(df[\"TradeDate\"].unique().to_list())\n",
        "unique_trade_dates = sorted(all_dates)[LOOKBACK_DAYS_SKIP:]\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} trade dates after {LOOKBACK_DAYS_SKIP}-day skip\")\n",
        "\n",
        "def process_symbol(file_path, valid_dates):\n",
        "    \"\"\"Process one symbol: load, aggregate to 15-min, compute indicators.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Read with lazy evaluation and filter early\n",
        "    df_min = pl.scan_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ]).filter(\n",
        "        pl.col(\"TradeDate\").is_in(valid_dates) &\n",
        "        pl.col(\"TradeTime\").is_in(bar_ends)\n",
        "    ).sort(\"dt\").collect()\n",
        "\n",
        "    if df_min.height == 0:\n",
        "        return symbol, None, None\n",
        "\n",
        "    # Assign to 15-min buckets\n",
        "    def assign_bar_time(t):\n",
        "        minutes = t.hour * 60 + t.minute\n",
        "        idx = min(range(len(bar_ends_minutes)), key=lambda i: bar_ends_minutes[i] if bar_ends_minutes[i] >= minutes else float('inf'))\n",
        "        return bar_ends[idx]\n",
        "\n",
        "    df_min = df_min.with_columns(\n",
        "        pl.col(\"dt\").dt.time().map_elements(\n",
        "            assign_bar_time,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"TradeTime\")\n",
        "    )\n",
        "\n",
        "    # Aggregate to 15-min bars\n",
        "    df_15 = df_min.group_by([\"TradeDate\", \"TradeTime\"]).agg([\n",
        "        pl.col(\"Open\").first().cast(pl.Float64),\n",
        "        pl.col(\"High\").max().cast(pl.Float64),\n",
        "        pl.col(\"Low\").min().cast(pl.Float64),\n",
        "        pl.col(\"Close\").last().cast(pl.Float64),\n",
        "        pl.col(\"Volume\").sum().cast(pl.Float64)\n",
        "    ]).sort([\"TradeDate\", \"TradeTime\"]).filter(\n",
        "        pl.col(\"Close\").is_not_null() & (pl.col(\"Open\") != 0)\n",
        "    )\n",
        "\n",
        "    if df_15.height == 0:\n",
        "        return symbol, None, df_min\n",
        "\n",
        "    # Session VWAP\n",
        "    df_15 = df_15.with_columns([\n",
        "        (pl.col(\"Close\") * pl.col(\"Volume\")).cum_sum().over(\"TradeDate\").alias(\"cum_pv\"),\n",
        "        pl.col(\"Volume\").cum_sum().over(\"TradeDate\").alias(\"cum_v\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"cum_pv\") / pl.col(\"cum_v\")).alias(\"VWAP\")\n",
        "    ])\n",
        "\n",
        "    # Signals: two consecutive closes above/below VWAP\n",
        "    df_15 = df_15.with_columns([\n",
        "        (pl.col(\"Close\") > pl.col(\"VWAP\")).alias(\"is_above\")\n",
        "    ]).with_columns([\n",
        "        pl.when(\n",
        "            pl.col(\"is_above\") & pl.col(\"is_above\").shift(1)\n",
        "        ).then(pl.lit(\"LONG\")).when(\n",
        "            ~pl.col(\"is_above\") & ~pl.col(\"is_above\").shift(1)\n",
        "        ).then(pl.lit(\"SHORT\")).otherwise(None).alias(\"SIGNAL\")\n",
        "    ]).with_columns([\n",
        "        ((pl.col(\"Close\") - pl.col(\"Open\")) / pl.col(\"Open\") * 100).alias(\"body_pct\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df_15, df_min\n",
        "\n",
        "# Process symbols sequentially to save memory\n",
        "symbol_15min_data = {}\n",
        "symbol_min_data = {}  # Store minimal minute data for backtest\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df_15, df_min = process_symbol(f, unique_trade_dates)\n",
        "    if df_15 is not None:\n",
        "        symbol_15min_data[symbol] = df_15\n",
        "    if df_min is not None:\n",
        "        # Keep only necessary columns to reduce memory\n",
        "        symbol_min_data[symbol] = df_min.select([\"TradeDate\", \"TradeTime\", \"High\", \"Low\"])\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "print(f\"‚úÖ Aggregated 15-min data for {len(symbol_15min_data)} symbols\")\n",
        "\n",
        "# ----- Backtest -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "bar_ends_dict = {t: i for i, t in enumerate(bar_ends)}\n",
        "\n",
        "for trade_date_pl in unique_trade_dates:\n",
        "    trade_date_str = trade_date_pl.strftime(\"%Y-%m-%d\")\n",
        "    positions = []\n",
        "    current_longs = 0\n",
        "    current_shorts = 0\n",
        "    signaled_longs = set()\n",
        "    signaled_shorts = set()\n",
        "\n",
        "    # Collect all signals for the day\n",
        "    day_signals = []\n",
        "    for sym, df_15 in symbol_15min_data.items():\n",
        "        df15_day = df_15.filter(pl.col(\"TradeDate\") == trade_date_pl)\n",
        "        if df15_day.height == 0:\n",
        "            continue\n",
        "        df15_day = df15_day.select([\n",
        "            \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"SIGNAL\", \"body_pct\"\n",
        "        ]).filter(pl.col(\"SIGNAL\").is_not_null() & pl.col(\"body_pct\").is_not_null() & (pl.col(\"Close\") != 0))\n",
        "        for row in df15_day.to_dicts():\n",
        "            if row[\"SIGNAL\"] == \"LONG\" and sym not in signaled_longs:\n",
        "                day_signals.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"time\": row[\"TradeTime\"],\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"open\": row[\"Open\"],\n",
        "                    \"high\": row[\"High\"],\n",
        "                    \"low\": row[\"Low\"],\n",
        "                    \"close\": row[\"Close\"],\n",
        "                    \"strength\": row[\"body_pct\"]\n",
        "                })\n",
        "                signaled_longs.add(sym)\n",
        "            elif row[\"SIGNAL\"] == \"SHORT\" and sym not in signaled_shorts:\n",
        "                day_signals.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"time\": row[\"TradeTime\"],\n",
        "                    \"side\": \"SHORT\",\n",
        "                    \"open\": row[\"Open\"],\n",
        "                    \"high\": row[\"High\"],\n",
        "                    \"low\": row[\"Low\"],\n",
        "                    \"close\": row[\"Close\"],\n",
        "                    \"strength\": -row[\"body_pct\"]\n",
        "                })\n",
        "                signaled_shorts.add(sym)\n",
        "\n",
        "    # Process each bar\n",
        "    for bar_idx, bar_time_str in enumerate(bar_ends):\n",
        "        prev_bar_time = bar_ends[bar_idx - 1] if bar_idx > 0 else None\n",
        "\n",
        "        # Check existing positions for exit conditions (two consecutive red/green)\n",
        "        new_positions = []\n",
        "        for pos in positions:\n",
        "            sym = pos['sym']\n",
        "            side = pos['side']\n",
        "            entry_price = pos['entry_price']\n",
        "            entry_time_str = pos['entry_time']\n",
        "\n",
        "            hit = False\n",
        "            exit_price = None\n",
        "            reason = None\n",
        "\n",
        "            # Get current bar's close for exit if hit\n",
        "            df15_bar = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "                (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == bar_time_str)\n",
        "            )\n",
        "            if df15_bar.height == 0:\n",
        "                new_positions.append(pos)\n",
        "                continue\n",
        "            current_close = df15_bar[\"Close\"][0]\n",
        "            current_is_red = df15_bar[\"Close\"][0] < df15_bar[\"Open\"][0]\n",
        "            current_is_green = df15_bar[\"Close\"][0] > df15_bar[\"Open\"][0]\n",
        "\n",
        "            # Get previous bar's color\n",
        "            if prev_bar_time is not None:\n",
        "                df15_prev = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "                    (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == prev_bar_time)\n",
        "                )\n",
        "                if df15_prev.height > 0:\n",
        "                    prev_is_red = df15_prev[\"Close\"][0] < df15_prev[\"Open\"][0]\n",
        "                    prev_is_green = df15_prev[\"Close\"][0] > df15_prev[\"Open\"][0]\n",
        "\n",
        "                    if side == \"LONG\" and current_is_red and prev_is_red:\n",
        "                        exit_price = current_close\n",
        "                        reason = \"TWO_RED\"\n",
        "                        hit = True\n",
        "                    elif side == \"SHORT\" and current_is_green and prev_is_green:\n",
        "                        exit_price = current_close\n",
        "                        reason = \"TWO_GREEN\"\n",
        "                        hit = True\n",
        "\n",
        "            if not hit:\n",
        "                new_positions.append(pos)\n",
        "            else:\n",
        "                if side == \"LONG\":\n",
        "                    pnl = exit_price - entry_price\n",
        "                    roi_trade = (pnl / entry_price) * 100\n",
        "                else:\n",
        "                    pnl = entry_price - exit_price\n",
        "                    roi_trade = (pnl / entry_price) * 100\n",
        "                cumulative_portfolio_return += roi_trade\n",
        "                output_trades.append([\n",
        "                    sym, entry_time_str, trade_date_str, side,\n",
        "                    round(entry_price, 2), round(exit_price, 2),\n",
        "                    round(pnl, 2), round(roi_trade, 2),\n",
        "                    f\"{reason}_{bar_time_str}\", round(cumulative_portfolio_return, 2)\n",
        "                ])\n",
        "\n",
        "        positions = new_positions\n",
        "        current_longs = sum(1 for p in positions if p['side'] == 'LONG')\n",
        "        current_shorts = sum(1 for p in positions if p['side'] == 'SHORT')\n",
        "\n",
        "        # Enter new positions\n",
        "        bar_signals = [s for s in day_signals if s[\"time\"] == bar_time_str]\n",
        "        bar_signals.sort(key=lambda x: x[\"strength\"], reverse=True)\n",
        "\n",
        "        for sig in bar_signals:\n",
        "            sym = sig[\"symbol\"]\n",
        "            side = sig[\"side\"]\n",
        "            o, h, l, c = sig[\"open\"], sig[\"high\"], sig[\"low\"], sig[\"close\"]\n",
        "            t_str = sig[\"time\"]\n",
        "\n",
        "            if side == \"LONG\" and current_longs < MAX_LONGS:\n",
        "                positions.append({\n",
        "                    'sym': sym, 'side': 'LONG', 'entry_price': c,\n",
        "                    'entry_time': t_str\n",
        "                })\n",
        "                current_longs += 1\n",
        "            elif side == \"SHORT\" and current_shorts < MAX_SHORTS:\n",
        "                positions.append({\n",
        "                    'sym': sym, 'side': 'SHORT', 'entry_price': c,\n",
        "                    'entry_time': t_str\n",
        "                })\n",
        "                current_shorts += 1\n",
        "\n",
        "    # EOD exits\n",
        "    last_bar_str = bar_ends[-1]\n",
        "    for pos in positions:\n",
        "        sym = pos['sym']\n",
        "        side = pos['side']\n",
        "        entry_price = pos['entry_price']\n",
        "        entry_time_str = pos['entry_time']\n",
        "\n",
        "        df15_last = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "            (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == last_bar_str)\n",
        "        )\n",
        "        if df15_last.height == 0:\n",
        "            continue\n",
        "        exit_price = df15_last[\"Close\"][0]\n",
        "        reason = \"EOD\"\n",
        "\n",
        "        if side == \"LONG\":\n",
        "            pnl = exit_price - entry_price\n",
        "            roi_trade = (pnl / entry_price) * 100\n",
        "        else:\n",
        "            pnl = entry_price - exit_price\n",
        "            roi_trade = (pnl / entry_price) * 100\n",
        "        cumulative_portfolio_return += roi_trade\n",
        "        output_trades.append([\n",
        "            sym, entry_time_str, trade_date_str, side,\n",
        "            round(entry_price, 2), round(exit_price, 2),\n",
        "            round(pnl, 2), round(roi_trade, 2),\n",
        "            f\"{reason}_{last_bar_str}\", round(cumulative_portfolio_return, 2)\n",
        "        ])\n",
        "\n",
        "# Create output DataFrame\n",
        "if output_trades:\n",
        "    output_df = pd.DataFrame(output_trades, columns=[\n",
        "        \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "        \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"ROI%\", \"EXIT_REASON\",\n",
        "        \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "    ])\n",
        "    output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "    print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "    print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "    # Daily PnL\n",
        "    daily_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"ROI%\": \"sum\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "    daily_df.rename(columns={\"ROI%\": \"DAILY_ROI%\", \"SYMBOL\": \"NUM_TRADES\"}, inplace=True)\n",
        "    daily_df[\"CUMULATIVE_ROI%\"] = daily_df[\"DAILY_ROI%\"].cumsum()\n",
        "    daily_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found.\")\n",
        "    output_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6xSimGtWL9P",
        "outputId": "98fc3388-a472-4aeb-a9b9-ada2d802ddaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "üìä 15-min bars: ['09:30', '09:45', '10:00', '10:15', '10:30', '10:45', '11:00', '11:15', '11:30', '11:45', '12:00', '12:15', '12:30', '12:45', '13:00', '13:15', '13:30', '13:45', '14:00', '14:15', '14:30', '14:45', '15:00', '15:15', '15:30']\n",
            "‚úÖ Found 256 trade dates after 30-day skip\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Aggregated 15-min data for 540 symbols\n",
            "‚úÖ Backtest completed. 940 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt_mod\n",
        "import numpy as np\n",
        "\n",
        "# User-configurable params\n",
        "LOOKBACK_DAYS_SKIP = 30\n",
        "MAX_LONGS = 2\n",
        "MAX_SHORTS = 2\n",
        "TRADING_HOURS_END = dt_mod.time(15, 30)\n",
        "ENTRY_TIME = \"09:45\"  # Restrict entries to 9:30 bar\n",
        "\n",
        "# Path with cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Generate 15-min bar end times\n",
        "def generate_bar_ends():\n",
        "    bar_ends = []\n",
        "    current = dt_mod.time(9, 30)\n",
        "    while current <= TRADING_HOURS_END:\n",
        "        bar_ends.append(current.strftime(\"%H:%M\"))\n",
        "        current_dt = dt_mod.datetime.combine(dt_mod.date.today(), current) + timedelta(minutes=15)\n",
        "        current = current_dt.time()\n",
        "    return bar_ends\n",
        "\n",
        "bar_ends = generate_bar_ends()\n",
        "print(f\"üìä 15-min bars: {bar_ends}\")\n",
        "\n",
        "# Pre-compute bar end times as minutes since midnight\n",
        "bar_ends_times = [datetime.strptime(t, \"%H:%M\").time() for t in bar_ends]\n",
        "bar_ends_minutes = [t.hour * 60 + t.minute for t in bar_ends_times]\n",
        "bar_ends_str = {bar_ends_times[i]: bar_ends[i] for i in range(len(bar_ends))}\n",
        "\n",
        "# Collect all unique trade dates\n",
        "all_dates = set()\n",
        "for f in all_files:\n",
        "    df = pl.read_csv(f, try_parse_dates=False, low_memory=True).rename({\"date\": \"Timestamp\"})\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").dt.date().alias(\"TradeDate\")\n",
        "    )\n",
        "    all_dates.update(df[\"TradeDate\"].unique().to_list())\n",
        "unique_trade_dates = sorted(all_dates)[LOOKBACK_DAYS_SKIP:]\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} trade dates after {LOOKBACK_DAYS_SKIP}-day skip\")\n",
        "\n",
        "def process_symbol(file_path, valid_dates):\n",
        "    \"\"\"Process one symbol: load, aggregate to 15-min, compute indicators.\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Read with lazy evaluation and filter early\n",
        "    df_min = pl.scan_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    }).with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ]).filter(\n",
        "        pl.col(\"TradeDate\").is_in(valid_dates) &\n",
        "        pl.col(\"TradeTime\").is_in(bar_ends)\n",
        "    ).sort(\"dt\").collect()\n",
        "\n",
        "    if df_min.height == 0:\n",
        "        return symbol, None, None\n",
        "\n",
        "    # Assign to 15-min buckets\n",
        "    def assign_bar_time(t):\n",
        "        minutes = t.hour * 60 + t.minute\n",
        "        idx = min(range(len(bar_ends_minutes)), key=lambda i: bar_ends_minutes[i] if bar_ends_minutes[i] >= minutes else float('inf'))\n",
        "        return bar_ends[idx]\n",
        "\n",
        "    df_min = df_min.with_columns(\n",
        "        pl.col(\"dt\").dt.time().map_elements(\n",
        "            assign_bar_time,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"TradeTime\")\n",
        "    )\n",
        "\n",
        "    # Aggregate to 15-min bars\n",
        "    df_15 = df_min.group_by([\"TradeDate\", \"TradeTime\"]).agg([\n",
        "        pl.col(\"Open\").first().cast(pl.Float64),\n",
        "        pl.col(\"High\").max().cast(pl.Float64),\n",
        "        pl.col(\"Low\").min().cast(pl.Float64),\n",
        "        pl.col(\"Close\").last().cast(pl.Float64),\n",
        "        pl.col(\"Volume\").sum().cast(pl.Float64)\n",
        "    ]).sort([\"TradeDate\", \"TradeTime\"]).filter(\n",
        "        pl.col(\"Close\").is_not_null() & (pl.col(\"Open\") != 0)\n",
        "    )\n",
        "\n",
        "    if df_15.height == 0:\n",
        "        return symbol, None, df_min\n",
        "\n",
        "    # Session VWAP\n",
        "    df_15 = df_15.with_columns([\n",
        "        (pl.col(\"Close\") * pl.col(\"Volume\")).cum_sum().over(\"TradeDate\").alias(\"cum_pv\"),\n",
        "        pl.col(\"Volume\").cum_sum().over(\"TradeDate\").alias(\"cum_v\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"cum_pv\") / pl.col(\"cum_v\")).alias(\"VWAP\")\n",
        "    ])\n",
        "\n",
        "    # Signals: two consecutive closes above/below VWAP\n",
        "    df_15 = df_15.with_columns([\n",
        "        (pl.col(\"Close\") > pl.col(\"VWAP\")).alias(\"is_above\")\n",
        "    ]).with_columns([\n",
        "        pl.when(\n",
        "            pl.col(\"is_above\") & pl.col(\"is_above\").shift(1)\n",
        "        ).then(pl.lit(\"LONG\")).when(\n",
        "            ~pl.col(\"is_above\") & ~pl.col(\"is_above\").shift(1)\n",
        "        ).then(pl.lit(\"SHORT\")).otherwise(None).alias(\"SIGNAL\")\n",
        "    ]).with_columns([\n",
        "        ((pl.col(\"Close\") - pl.col(\"Open\")) / pl.col(\"Open\") * 100).alias(\"body_pct\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df_15, df_min\n",
        "\n",
        "# Process symbols sequentially to save memory\n",
        "symbol_15min_data = {}\n",
        "symbol_min_data = {}  # Store minimal minute data for backtest\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df_15, df_min = process_symbol(f, unique_trade_dates)\n",
        "    if df_15 is not None:\n",
        "        symbol_15min_data[symbol] = df_15\n",
        "    if df_min is not None:\n",
        "        # Keep only necessary columns to reduce memory\n",
        "        symbol_min_data[symbol] = df_min.select([\"TradeDate\", \"TradeTime\", \"High\", \"Low\"])\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "print(f\"‚úÖ Aggregated 15-min data for {len(symbol_15min_data)} symbols\")\n",
        "\n",
        "# ----- Backtest -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "bar_ends_dict = {t: i for i, t in enumerate(bar_ends)}\n",
        "\n",
        "for trade_date_pl in unique_trade_dates:\n",
        "    trade_date_str = trade_date_pl.strftime(\"%Y-%m-%d\")\n",
        "    positions = []\n",
        "    current_longs = 0\n",
        "    current_shorts = 0\n",
        "    signaled_longs = set()\n",
        "    signaled_shorts = set()\n",
        "\n",
        "    # Collect all signals for the day (only at 9:30)\n",
        "    day_signals = []\n",
        "    for sym, df_15 in symbol_15min_data.items():\n",
        "        df15_day = df_15.filter(\n",
        "            (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == ENTRY_TIME)\n",
        "        )\n",
        "        if df15_day.height == 0:\n",
        "            continue\n",
        "        df15_day = df15_day.select([\n",
        "            \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"SIGNAL\", \"body_pct\"\n",
        "        ]).filter(pl.col(\"SIGNAL\").is_not_null() & pl.col(\"body_pct\").is_not_null() & (pl.col(\"Close\") != 0))\n",
        "        for row in df15_day.to_dicts():\n",
        "            if row[\"SIGNAL\"] == \"LONG\" and sym not in signaled_longs:\n",
        "                day_signals.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"time\": row[\"TradeTime\"],\n",
        "                    \"side\": \"LONG\",\n",
        "                    \"open\": row[\"Open\"],\n",
        "                    \"high\": row[\"High\"],\n",
        "                    \"low\": row[\"Low\"],\n",
        "                    \"close\": row[\"Close\"],\n",
        "                    \"strength\": row[\"body_pct\"]\n",
        "                })\n",
        "                signaled_longs.add(sym)\n",
        "            elif row[\"SIGNAL\"] == \"SHORT\" and sym not in signaled_shorts:\n",
        "                day_signals.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"time\": row[\"TradeTime\"],\n",
        "                    \"side\": \"SHORT\",\n",
        "                    \"open\": row[\"Open\"],\n",
        "                    \"high\": row[\"High\"],\n",
        "                    \"low\": row[\"Low\"],\n",
        "                    \"close\": row[\"Close\"],\n",
        "                    \"strength\": -row[\"body_pct\"]\n",
        "                })\n",
        "                signaled_shorts.add(sym)\n",
        "\n",
        "    # Process each bar\n",
        "    for bar_idx, bar_time_str in enumerate(bar_ends):\n",
        "        prev_bar_time = bar_ends[bar_idx - 1] if bar_idx > 0 else None\n",
        "\n",
        "        # Check existing positions for exit conditions (two consecutive red/green)\n",
        "        new_positions = []\n",
        "        for pos in positions:\n",
        "            sym = pos['sym']\n",
        "            side = pos['side']\n",
        "            entry_price = pos['entry_price']\n",
        "            entry_time_str = pos['entry_time']\n",
        "\n",
        "            hit = False\n",
        "            exit_price = None\n",
        "            reason = None\n",
        "\n",
        "            # Get current bar's close for exit if hit\n",
        "            df15_bar = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "                (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == bar_time_str)\n",
        "            )\n",
        "            if df15_bar.height == 0:\n",
        "                new_positions.append(pos)\n",
        "                continue\n",
        "            current_close = df15_bar[\"Close\"][0]\n",
        "            current_is_red = df15_bar[\"Close\"][0] < df15_bar[\"Open\"][0]\n",
        "            current_is_green = df15_bar[\"Close\"][0] > df15_bar[\"Open\"][0]\n",
        "\n",
        "            # Get previous bar's color\n",
        "            if prev_bar_time is not None:\n",
        "                df15_prev = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "                    (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == prev_bar_time)\n",
        "                )\n",
        "                if df15_prev.height > 0:\n",
        "                    prev_is_red = df15_prev[\"Close\"][0] < df15_prev[\"Open\"][0]\n",
        "                    prev_is_green = df15_prev[\"Close\"][0] > df15_prev[\"Open\"][0]\n",
        "\n",
        "                    if side == \"LONG\" and current_is_red and prev_is_red:\n",
        "                        exit_price = current_close\n",
        "                        reason = \"TWO_RED\"\n",
        "                        hit = True\n",
        "                    elif side == \"SHORT\" and current_is_green and prev_is_green:\n",
        "                        exit_price = current_close\n",
        "                        reason = \"TWO_GREEN\"\n",
        "                        hit = True\n",
        "\n",
        "            if not hit:\n",
        "                new_positions.append(pos)\n",
        "            else:\n",
        "                if side == \"LONG\":\n",
        "                    pnl = exit_price - entry_price\n",
        "                    roi_trade = (pnl / entry_price) * 100\n",
        "                else:\n",
        "                    pnl = entry_price - exit_price\n",
        "                    roi_trade = (pnl / entry_price) * 100\n",
        "                cumulative_portfolio_return += roi_trade\n",
        "                output_trades.append([\n",
        "                    sym, entry_time_str, trade_date_str, side,\n",
        "                    round(entry_price, 2), round(exit_price, 2),\n",
        "                    round(pnl, 2), round(roi_trade, 2),\n",
        "                    f\"{reason}_{bar_time_str}\", round(cumulative_portfolio_return, 2)\n",
        "                ])\n",
        "\n",
        "        positions = new_positions\n",
        "        current_longs = sum(1 for p in positions if p['side'] == 'LONG')\n",
        "        current_shorts = sum(1 for p in positions if p['side'] == 'SHORT')\n",
        "\n",
        "        # Enter new positions only at 9:30\n",
        "        if bar_time_str == ENTRY_TIME:\n",
        "            bar_signals = [s for s in day_signals if s[\"time\"] == bar_time_str]\n",
        "            # Sort by lowest strength for LONG (body_pct), lowest strength for SHORT (-body_pct)\n",
        "            bar_signals.sort(key=lambda x: x[\"strength\"], reverse=False)\n",
        "\n",
        "            for sig in bar_signals:\n",
        "                sym = sig[\"symbol\"]\n",
        "                side = sig[\"side\"]\n",
        "                c = sig[\"close\"]\n",
        "                t_str = sig[\"time\"]\n",
        "\n",
        "                if side == \"LONG\" and current_longs < MAX_LONGS:\n",
        "                    positions.append({\n",
        "                        'sym': sym, 'side': 'LONG', 'entry_price': c,\n",
        "                        'entry_time': t_str\n",
        "                    })\n",
        "                    current_longs += 1\n",
        "                elif side == \"SHORT\" and current_shorts < MAX_SHORTS:\n",
        "                    positions.append({\n",
        "                        'sym': sym, 'side': 'SHORT', 'entry_price': c,\n",
        "                        'entry_time': t_str\n",
        "                    })\n",
        "                    current_shorts += 1\n",
        "\n",
        "    # EOD exits\n",
        "    last_bar_str = bar_ends[-1]\n",
        "    for pos in positions:\n",
        "        sym = pos['sym']\n",
        "        side = pos['side']\n",
        "        entry_price = pos['entry_price']\n",
        "        entry_time_str = pos['entry_time']\n",
        "\n",
        "        df15_last = symbol_15min_data.get(sym, pl.DataFrame()).filter(\n",
        "            (pl.col(\"TradeDate\") == trade_date_pl) & (pl.col(\"TradeTime\") == last_bar_str)\n",
        "        )\n",
        "        if df15_last.height == 0:\n",
        "            continue\n",
        "        exit_price = df15_last[\"Close\"][0]\n",
        "        reason = \"EOD\"\n",
        "\n",
        "        if side == \"LONG\":\n",
        "            pnl = exit_price - entry_price\n",
        "            roi_trade = (pnl / entry_price) * 100\n",
        "        else:\n",
        "            pnl = entry_price - exit_price\n",
        "            roi_trade = (pnl / entry_price) * 100\n",
        "        cumulative_portfolio_return += roi_trade\n",
        "        output_trades.append([\n",
        "            sym, entry_time_str, trade_date_str, side,\n",
        "            round(entry_price, 2), round(exit_price, 2),\n",
        "            round(pnl, 2), round(roi_trade, 2),\n",
        "            f\"{reason}_{last_bar_str}\", round(cumulative_portfolio_return, 2)\n",
        "        ])\n",
        "\n",
        "# Create output DataFrame\n",
        "if output_trades:\n",
        "    output_df = pd.DataFrame(output_trades, columns=[\n",
        "        \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "        \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\", \"ROI%\", \"EXIT_REASON\",\n",
        "        \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "    ])\n",
        "    output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "    print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "    print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "    # Daily PnL\n",
        "    daily_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"ROI%\": \"sum\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "    daily_df.rename(columns={\"ROI%\": \"DAILY_ROI%\", \"SYMBOL\": \"NUM_TRADES\"}, inplace=True)\n",
        "    daily_df[\"CUMULATIVE_ROI%\"] = daily_df[\"DAILY_ROI%\"].cumsum()\n",
        "    daily_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found.\")\n",
        "    output_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tSPWyF6-GuV"
      },
      "source": [
        "#Gap intraday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxawzTC7bvo1",
        "outputId": "a92d87ba-b115-4892-aaa4-46ebe468b80c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Loaded 540 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 286 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 151665 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 5700 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 1080 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.01      # 0.4% individual SL\n",
        "START_TIME = \"09:16\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:16\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"13:30\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME))\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Resample to 15min candles\n",
        "        day_df = day_df.sort(\"dt\")\n",
        "        day_df = day_df.with_columns(pl.col(\"dt\").dt.truncate(\"15m\").alias(\"candle_start\"))\n",
        "        candles = day_df.group_by(\"candle_start\").agg([\n",
        "            pl.col(\"Open\").first().alias(\"open\"),\n",
        "            pl.col(\"High\").max().alias(\"high\"),\n",
        "            pl.col(\"Low\").min().alias(\"low\"),\n",
        "            pl.col(\"Close\").last().alias(\"close\"),\n",
        "            pl.col(\"Volume\").sum().alias(\"volume\")\n",
        "        ]).sort(\"candle_start\")\n",
        "\n",
        "        if len(candles) < 3:\n",
        "            continue\n",
        "\n",
        "        # Get first two candles\n",
        "        first_two = candles[0:2]\n",
        "\n",
        "        if side == \"LONG\":\n",
        "            breakout_level = first_two[\"high\"].max()\n",
        "            # Find first later candle where close > breakout_level\n",
        "            later_candles = candles[2:]\n",
        "            triggered = later_candles.filter(pl.col(\"close\") > breakout_level)\n",
        "            if triggered.is_empty():\n",
        "                continue\n",
        "            trigger_candle = triggered[0]\n",
        "        else:  # SHORT\n",
        "            breakdown_level = first_two[\"low\"].min()\n",
        "            later_candles = candles[2:]\n",
        "            triggered = later_candles.filter(pl.col(\"close\") < breakdown_level)\n",
        "            if triggered.is_empty():\n",
        "                continue\n",
        "            trigger_candle = triggered[0]\n",
        "\n",
        "        # Trigger time is end of the trigger candle\n",
        "        trigger_start = trigger_candle[\"candle_start\"][0]\n",
        "        trigger_dt = trigger_start + timedelta(minutes=15)\n",
        "        trigger_time_str = trigger_dt.strftime(\"%H:%M\")\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        # Entry price approx as candle close\n",
        "        entry_price = trigger_candle[\"close\"][0]\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # Determine SL price (with tick rounding approximation)\n",
        "        if side == \"LONG\":\n",
        "            sl_trigger = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "            indiv_sl_price = math.ceil(sl_trigger / TICK_SIZE) * TICK_SIZE\n",
        "        else:\n",
        "            sl_trigger = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "            indiv_sl_price = math.ceil(sl_trigger / TICK_SIZE) * TICK_SIZE\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "\n",
        "        for _, minute_row in day_prices.iterrows():\n",
        "            cur_price = minute_row[\"Close\"]\n",
        "            cur_low = minute_row[\"Low\"]\n",
        "            cur_high = minute_row[\"High\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Immediate SL activation\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                # Approximate hit if low <= sl\n",
        "                if cur_low <= indiv_sl_price:\n",
        "                    hit_condition = True\n",
        "            else:\n",
        "                # For short, if high >= sl\n",
        "                if cur_high >= indiv_sl_price:\n",
        "                    hit_condition = True\n",
        "\n",
        "            if hit_condition:\n",
        "                exit_price = indiv_sl_price  # Assume exit at SL price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (exit_price - entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (entry_price - exit_price)\n",
        "\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWmtGQC_d8ia"
      },
      "source": [
        "# First min Candle Breakout/breakdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zImhPiyX-J9Z",
        "outputId": "53a151ea-5e55-4027-bf17-0e9bca37b769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Loaded 540 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 289 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 153272 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 5760 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 1018 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.01      # 0.4% individual SL\n",
        "START_TIME = \"09:16\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:16\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"13:30\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df_pl = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME))\n",
        "        if day_df_pl.is_empty():\n",
        "            continue\n",
        "\n",
        "        day_df = day_df_pl.sort(\"dt\").to_pandas()\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        o915_mask = day_df['TradeTime'] == '09:15'\n",
        "        if not o915_mask.any():\n",
        "            continue\n",
        "        o915_high = day_df.loc[o915_mask, 'High'].iloc[0]\n",
        "        o915_low = day_df.loc[o915_mask, 'Low'].iloc[0]\n",
        "\n",
        "        later_df = day_df[day_df['TradeTime'] > '09:15'].reset_index(drop=True)\n",
        "        if later_df.empty:\n",
        "            continue\n",
        "\n",
        "        trigger_row = None\n",
        "        if side == \"LONG\":\n",
        "            breakout_level = o915_high\n",
        "            for i in range(len(later_df)):\n",
        "                if later_df.iloc[i]['High'] > breakout_level:\n",
        "                    # Found breakout candle at i\n",
        "                    if i + 1 < len(later_df):\n",
        "                        next_candle = later_df.iloc[i + 1]\n",
        "                        if next_candle['Close'] > breakout_level:\n",
        "                            trigger_row = next_candle\n",
        "                            break\n",
        "                    break  # if no next, no trigger\n",
        "        else:  # SHORT\n",
        "            breakdown_level = o915_low\n",
        "            for i in range(len(later_df)):\n",
        "                if later_df.iloc[i]['Low'] < breakdown_level:\n",
        "                    if i + 1 < len(later_df):\n",
        "                        next_candle = later_df.iloc[i + 1]\n",
        "                        if next_candle['Close'] < breakdown_level:\n",
        "                            trigger_row = next_candle\n",
        "                            break\n",
        "                    break  # if no next, no trigger\n",
        "\n",
        "        if trigger_row is None:\n",
        "            continue\n",
        "\n",
        "        # Trigger time is end of the trigger candle\n",
        "        entry_price = trigger_row['Close']\n",
        "        trigger_time_str = trigger_row['TradeTime']\n",
        "        trigger_dt = trigger_row['dt']\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # Determine SL price (with tick rounding approximation)\n",
        "        if side == \"LONG\":\n",
        "            sl_trigger = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "            indiv_sl_price = math.ceil(sl_trigger / TICK_SIZE) * TICK_SIZE\n",
        "        else:\n",
        "            sl_trigger = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "            indiv_sl_price = math.ceil(sl_trigger / TICK_SIZE) * TICK_SIZE\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "\n",
        "        for _, minute_row in day_prices.iterrows():\n",
        "            cur_price = minute_row[\"Close\"]\n",
        "            cur_low = minute_row[\"Low\"]\n",
        "            cur_high = minute_row[\"High\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Immediate SL activation\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                # Approximate hit if low <= sl\n",
        "                if cur_low <= indiv_sl_price:\n",
        "                    hit_condition = True\n",
        "            else:\n",
        "                # For short, if high >= sl\n",
        "                if cur_high >= indiv_sl_price:\n",
        "                    hit_condition = True\n",
        "\n",
        "            if hit_condition:\n",
        "                exit_price = indiv_sl_price  # Assume exit at SL price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (exit_price - entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (entry_price - exit_price)\n",
        "\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpKj5wsweApD",
        "outputId": "db2b96c7-28ce-4e3d-d0b8-c3a135591a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 540 cash files...\n",
            "‚úÖ Processed 50/540 symbols\n",
            "‚úÖ Processed 100/540 symbols\n",
            "‚úÖ Processed 150/540 symbols\n",
            "‚úÖ Processed 200/540 symbols\n",
            "‚úÖ Processed 250/540 symbols\n",
            "‚úÖ Processed 300/540 symbols\n",
            "‚úÖ Processed 350/540 symbols\n",
            "‚úÖ Processed 400/540 symbols\n",
            "‚úÖ Processed 450/540 symbols\n",
            "‚úÖ Processed 500/540 symbols\n",
            "‚úÖ Loaded 540 symbols with required times\n",
            "‚úÖ Loaded NIFTY500 reference series\n",
            "‚úÖ Found 289 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 153369 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 5760 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 1152 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "START_TIME = \"09:15\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:20\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get range from 9:16 to 9:20\n",
        "        range_df = day_df.filter((pl.col(\"TradeTime\") >= \"09:16\") & (pl.col(\"TradeTime\") <= \"09:20\"))\n",
        "        if len(range_df) < 5:\n",
        "            continue  # Expect at least 5 minutes\n",
        "\n",
        "        range_high = range_df[\"High\"].max()\n",
        "        range_low = range_df[\"Low\"].min()\n",
        "\n",
        "        # Get later minutes after 9:20\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:20\").sort(\"dt\")\n",
        "        if len(later_df) < 2:\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(later_pd) - 1):\n",
        "            cur_high = later_pd.iloc[i][\"High\"]\n",
        "            cur_low = later_pd.iloc[i][\"Low\"]\n",
        "            next_close = later_pd.iloc[i + 1][\"Close\"]\n",
        "            next_dt = later_pd.iloc[i + 1][\"dt\"]\n",
        "            next_time_str = later_pd.iloc[i + 1][\"TradeTime\"]\n",
        "\n",
        "            if side == \"LONG\":\n",
        "                if cur_high > range_high and next_close > range_high:\n",
        "                    entry_price = next_close\n",
        "                    trigger_dt = next_dt\n",
        "                    trigger_time_str = next_time_str\n",
        "                    triggered = True\n",
        "                    break\n",
        "            else:  # SHORT\n",
        "                if cur_low < range_low and next_close < range_low:\n",
        "                    entry_price = next_close\n",
        "                    trigger_dt = next_dt\n",
        "                    trigger_time_str = next_time_str\n",
        "                    triggered = True\n",
        "                    break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # Determine SL price (with tick rounding approximation)\n",
        "        if side == \"LONG\":\n",
        "            sl_trigger = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "            indiv_sl_price = math.ceil(sl_trigger / TICK_SIZE) * TICK_SIZE\n",
        "        else:\n",
        "            sl_trigger = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "            indiv_sl_price = math.ceil(sl_trigger / TICK_SIZE) * TICK_SIZE\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "\n",
        "        for _, minute_row in day_prices.iterrows():\n",
        "            cur_price = minute_row[\"Close\"]\n",
        "            cur_low = minute_row[\"Low\"]\n",
        "            cur_high = minute_row[\"High\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Immediate SL activation\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                # Approximate hit if low <= sl\n",
        "                if cur_low <= indiv_sl_price:\n",
        "                    hit_condition = True\n",
        "            else:\n",
        "                # For short, if high >= sl\n",
        "                if cur_high >= indiv_sl_price:\n",
        "                    hit_condition = True\n",
        "\n",
        "            if hit_condition:\n",
        "                exit_price = indiv_sl_price  # Assume exit at SL price\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (exit_price - entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (entry_price - exit_price)\n",
        "\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNRjoqtAVK91"
      },
      "source": [
        "# Dynamic SL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYjceJGdWRaI",
        "outputId": "2028ab51-1ad4-4ecc-8ed5-5fec83e80789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 316 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 160052 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 6300 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 1260 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "START_TIME = \"09:15\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get range from 9:16 to 9:20\n",
        "        range_df = day_df.filter((pl.col(\"TradeTime\") >= \"09:16\") & (pl.col(\"TradeTime\") <= \"09:20\"))\n",
        "        if len(range_df) < 5:\n",
        "            continue  # Expect at least 5 minutes\n",
        "\n",
        "        range_high = range_df[\"High\"].max()\n",
        "        range_low = range_df[\"Low\"].min()\n",
        "\n",
        "        # Get later minutes after 9:20\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:20\").sort(\"dt\")\n",
        "        if len(later_df) < 2:\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(later_pd) - 1):\n",
        "            cur_high = later_pd.iloc[i][\"High\"]\n",
        "            cur_low = later_pd.iloc[i][\"Low\"]\n",
        "            next_close = later_pd.iloc[i + 1][\"Close\"]\n",
        "            next_dt = later_pd.iloc[i + 1][\"dt\"]\n",
        "            next_time_str = later_pd.iloc[i + 1][\"TradeTime\"]\n",
        "\n",
        "            if side == \"LONG\":\n",
        "                if cur_high > range_high and next_close > range_high:\n",
        "                    entry_price = next_close\n",
        "                    trigger_dt = next_dt\n",
        "                    trigger_time_str = next_time_str\n",
        "                    triggered = True\n",
        "                    break\n",
        "            else:  # SHORT\n",
        "                if cur_low < range_low and next_close < range_low:\n",
        "                    entry_price = next_close\n",
        "                    trigger_dt = next_dt\n",
        "                    trigger_time_str = next_time_str\n",
        "                    triggered = True\n",
        "                    break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"range_high\": range_high,\n",
        "            \"range_low\": range_low\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        range_high = entry[\"range_high\"]\n",
        "        range_low = entry[\"range_low\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "        prev_is_sl_condition = False\n",
        "\n",
        "        for _, minute_row in day_prices.iterrows():\n",
        "            cur_close = minute_row[\"Close\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Check SL condition for two consecutive closes\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                is_below = cur_close < range_low\n",
        "                if is_below and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_below\n",
        "            else:  # SHORT\n",
        "                is_above = cur_close > range_high\n",
        "                if is_above and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_above\n",
        "\n",
        "            if hit_condition:\n",
        "                exit_price = cur_close\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (exit_price - entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (entry_price - exit_price)\n",
        "\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzB-3uIKgu-H"
      },
      "source": [
        "#xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GvJnZltSDAC",
        "outputId": "4a614ec4-b060-4800-a9df-e586e87a1e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR1lDiyPOAuF",
        "outputId": "09d1d9cb-f664-4eb9-a35e-e37488330976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 330 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 167278 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 6580 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 1316 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Backtest results with equity curve saved in: BACKTEST_RESULTS.xlsx\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "START_TIME = \"09:15\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "SLIPPAGE_PCT = 0.0005          # 0.05% slippage\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get range from 9:16 to 9:20\n",
        "        range_df = day_df.filter((pl.col(\"TradeTime\") >= \"09:16\") & (pl.col(\"TradeTime\") <= \"09:20\"))\n",
        "        if len(range_df) < 5:\n",
        "            continue  # Expect at least 5 minutes\n",
        "\n",
        "        range_high = range_df[\"High\"].max()\n",
        "        range_low = range_df[\"Low\"].min()\n",
        "\n",
        "        # Get later minutes after 9:20\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:20\").sort(\"dt\")\n",
        "        if len(later_df) < 2:\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(later_pd) - 1):\n",
        "            cur_high = later_pd.iloc[i][\"High\"]\n",
        "            cur_low = later_pd.iloc[i][\"Low\"]\n",
        "            next_close = later_pd.iloc[i + 1][\"Close\"]\n",
        "            next_dt = later_pd.iloc[i + 1][\"dt\"]\n",
        "            next_time_str = later_pd.iloc[i + 1][\"TradeTime\"]\n",
        "\n",
        "            if side == \"LONG\":\n",
        "                if cur_high > range_high and next_close > range_high:\n",
        "                    entry_price = next_close\n",
        "                    trigger_dt = next_dt\n",
        "                    trigger_time_str = next_time_str\n",
        "                    triggered = True\n",
        "                    break\n",
        "            else:  # SHORT\n",
        "                if cur_low < range_low and next_close < range_low:\n",
        "                    entry_price = next_close\n",
        "                    trigger_dt = next_dt\n",
        "                    trigger_time_str = next_time_str\n",
        "                    triggered = True\n",
        "                    break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"range_high\": range_high,\n",
        "            \"range_low\": range_low\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        range_high = entry[\"range_high\"]\n",
        "        range_low = entry[\"range_low\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "        prev_is_sl_condition = False\n",
        "\n",
        "        for _, minute_row in day_prices.iterrows():\n",
        "            cur_close = minute_row[\"Close\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Check SL condition for two consecutive closes\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                is_below = cur_close < range_low\n",
        "                if is_below and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_below\n",
        "            else:  # SHORT\n",
        "                is_above = cur_close > range_high\n",
        "                if is_above and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_above\n",
        "\n",
        "            if hit_condition:\n",
        "                exit_price = cur_close\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Apply slippage\n",
        "        adjusted_entry_price = entry_price * (1 + SLIPPAGE_PCT if side == \"LONG\" else 1 - SLIPPAGE_PCT)\n",
        "        adjusted_exit_price = exit_price * (1 - SLIPPAGE_PCT if side == \"LONG\" else 1 + SLIPPAGE_PCT)\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (adjusted_exit_price - adjusted_entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (adjusted_entry_price - adjusted_exit_price)\n",
        "\n",
        "        position_value = qty * adjusted_entry_price\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            adjusted_entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            adjusted_exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    # Compute equity curve and drawdown\n",
        "    daily_pnl_df['TRADE_DATE'] = pd.to_datetime(daily_pnl_df['TRADE_DATE'])\n",
        "    daily_pnl_df = daily_pnl_df.sort_values('TRADE_DATE')\n",
        "    daily_pnl_df['CUMULATIVE_PNL_ABS'] = CAPITAL * daily_pnl_df['CUMULATIVE_RETURN%'] / 100\n",
        "    daily_pnl_df['Equity'] = CAPITAL + daily_pnl_df['CUMULATIVE_PNL_ABS']\n",
        "    daily_pnl_df['Peak_Equity'] = daily_pnl_df['Equity'].cummax()\n",
        "    daily_pnl_df['Drawdown_Abs'] = daily_pnl_df['Equity'] - daily_pnl_df['Peak_Equity']\n",
        "    daily_pnl_df['Drawdown_Pct'] = (daily_pnl_df['Drawdown_Abs'] / daily_pnl_df['Peak_Equity']) * 100\n",
        "\n",
        "    # Save to Excel with charts\n",
        "    out_path = \"BACKTEST_RESULTS.xlsx\"\n",
        "    with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
        "        output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"EquityCurve\", index=False)\n",
        "\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['EquityCurve']\n",
        "\n",
        "        # ‚úÖ Cumulative PnL Chart\n",
        "        chart1 = workbook.add_chart({'type': 'line'})\n",
        "        chart1.add_series({\n",
        "            'name': 'Cumulative Return %',\n",
        "            'categories': ['EquityCurve', 1, 0, len(daily_pnl_df), 0],\n",
        "            'values': ['EquityCurve', 1, 8, len(daily_pnl_df), 8],  # Assuming CUMULATIVE_RETURN% is column index 8 (I)\n",
        "        })\n",
        "        chart1.set_title({'name': 'Cumulative Return %'})\n",
        "        chart1.set_x_axis({'name': 'Date', 'date_axis': True, 'num_format': 'yyyy-mm-dd'})\n",
        "        chart1.set_y_axis({'name': 'Cumulative Return %'})\n",
        "        worksheet.insert_chart('J2', chart1)\n",
        "\n",
        "        # ‚úÖ Drawdown Chart\n",
        "        chart2 = workbook.add_chart({'type': 'line'})\n",
        "        chart2.add_series({\n",
        "            'name': 'Drawdown %',\n",
        "            'categories': ['EquityCurve', 1, 0, len(daily_pnl_df), 0],\n",
        "            'values': ['EquityCurve', 1, 11, len(daily_pnl_df), 11],  # Assuming Drawdown_Pct is column index 11 (L)\n",
        "        })\n",
        "        chart2.set_title({'name': 'Drawdown %'})\n",
        "        chart2.set_x_axis({'name': 'Date', 'date_axis': True, 'num_format': 'yyyy-mm-dd'})\n",
        "        chart2.set_y_axis({'name': 'Drawdown %'})\n",
        "        worksheet.insert_chart('J20', chart2)\n",
        "\n",
        "    print(f\"üìÑ Backtest results with equity curve saved in: {out_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE08KtdTjRIO"
      },
      "source": [
        "# all time 4 live pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sD_f1oKjU6H",
        "outputId": "4395f255-a700-4664-f1a6-d279a47cb681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 521 cash files...\n",
            "Loading symbol data...\n",
            "   Processed 100/521\n",
            "   Processed 200/521\n",
            "   Processed 300/521\n",
            "   Processed 400/521\n",
            "   Processed 500/521\n",
            "Loaded 521 symbols with required times\n",
            "Found 318 trade dates\n",
            "Building ALL_BREAKDOWNS...\n",
            "Saved ALL_BREAKDOWNS.csv ‚Üí 161075 rows\n",
            "Ranked ‚Üí 6340 potential signals\n",
            "Precomputing daily minute data for signals...\n",
            "Cached 6340 symbol-day minute datasets\n",
            "Starting backtest with position replacement...\n",
            "Backtest complete: 1619 trades ‚Üí OUTPUT_BACKTEST.csv\n",
            "Results saved ‚Üí BACKTEST_RESULTS.xlsx\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------- CONFIG -------------------\n",
        "INDIVIDUAL_SL_PCT = 0.004\n",
        "START_TIME = \"09:15\"\n",
        "END_TIME = \"15:15\"\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"\n",
        "CAPITAL = 50000.0\n",
        "LEVERAGE = 2.5\n",
        "MAX_POSITIONS = 4\n",
        "SLIPPAGE_PCT = 0.0005\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"Found {len(all_files)} cash files...\")\n",
        "\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ------------------- LOAD FULL DATA -------------------\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\", \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"),\n",
        "    ]).with_columns([\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\"),\n",
        "    ]).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "print(\"Loading symbol data...\")\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"]\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"]\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"   Processed {i}/{len(all_files)}\")\n",
        "\n",
        "print(f\"Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# ------------------- NIFTY500 -------------------\n",
        "nifty500_close_1529 = nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"]\n",
        "        nifty500_close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"]\n",
        "        print(\"Loaded NIFTY500 reference\")\n",
        "\n",
        "# ------------------- BUILD ALL_BREAKDOWNS -------------------\n",
        "all_dates = set()\n",
        "for d in symbol_close_start_end.values():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"Found {len(unique_trade_dates)} trade dates\")\n",
        "\n",
        "def get_prev_trading_day(date, dates):\n",
        "    prev = [d for d in dates if d < date]\n",
        "    return max(prev) if prev else None\n",
        "\n",
        "all_breakdowns = []\n",
        "print(\"Building ALL_BREAKDOWNS...\")\n",
        "for trade_date in unique_trade_dates:\n",
        "    prev_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "    nifty_roi = None\n",
        "    if nifty500_close_1529 is not None and prev_date:\n",
        "        try:\n",
        "            nifty_roi = ((nifty500_close_start.loc[trade_date] - nifty500_close_1529.loc[prev_date]) /\n",
        "                         nifty500_close_1529.loc[prev_date]) * 100\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            prev_close = d[\"close_1529\"].loc[prev_date] if prev_date else None\n",
        "            start_close = d[\"close_start\"].loc[trade_date]\n",
        "            if prev_close and prev_close > 0:\n",
        "                roi = (start_close - prev_close) / prev_close * 100\n",
        "                all_breakdowns.append([trade_date, sym, prev_close, start_close, roi, nifty_roi])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "    columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(f\"Saved ALL_BREAKDOWNS.csv ‚Üí {len(breakdown_df)} rows\")\n",
        "\n",
        "# ------------------- RANKING -------------------\n",
        "potential_df = []\n",
        "for date, group in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    longs = group.nsmallest(10, \"ROI_%\").copy()\n",
        "    shorts = group.nlargest(10, \"ROI_%\").copy()\n",
        "    longs[\"SIDE\"] = \"LONG\"\n",
        "    shorts[\"SIDE\"] = \"SHORT\"\n",
        "    potential_df.append(pd.concat([longs, shorts], ignore_index=True))\n",
        "\n",
        "potential_df = pd.concat(potential_df, ignore_index=True) if potential_df else pd.DataFrame()\n",
        "print(f\"Ranked ‚Üí {len(potential_df)} potential signals\")\n",
        "\n",
        "# ------------------- PRECOMPUTE DAY DATA (CRITICAL OPTIMIZATION) -------------------\n",
        "print(\"Precomputing daily minute data for signals...\")\n",
        "signal_day_cache = {}\n",
        "\n",
        "for _, row in potential_df.iterrows():\n",
        "    key = (row[\"SIGNAL_DATE\"], row[\"SYMBOL\"])\n",
        "    if key in signal_day_cache:\n",
        "        continue\n",
        "    df = symbol_full_data.get(row[\"SYMBOL\"])\n",
        "    if df is None:\n",
        "        continue\n",
        "    day_df = df.filter(\n",
        "        (pl.col(\"TradeDate\") == row[\"SIGNAL_DATE\"]) &\n",
        "        (pl.col(\"TradeTime\") >= \"09:15\") &\n",
        "        (pl.col(\"TradeTime\") <= END_TIME)\n",
        "    ).sort(\"dt\").select([\"dt\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
        "    if day_df.height > 0:\n",
        "        signal_day_cache[key] = day_df.to_pandas()\n",
        "\n",
        "print(f\"Cached {len(signal_day_cache)} symbol-day minute datasets\")\n",
        "\n",
        "# ------------------- BACKTEST ENGINE (EVENT-DRIVEN, FAST) -------------------\n",
        "output_trades = []\n",
        "cumulative_pnl = 0.0\n",
        "\n",
        "print(\"Starting backtest with position replacement...\")\n",
        "\n",
        "for signal_date, day_signals in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_signals.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "        key = (signal_date, sym)\n",
        "        day_pd = signal_day_cache.get(key)\n",
        "        if day_pd is None or len(day_pd) < 10:\n",
        "            continue\n",
        "\n",
        "        # 9:16‚Äì9:20 range\n",
        "        range_df = day_pd[(day_pd[\"TradeTime\"] >= \"09:16\") & (day_pd[\"TradeTime\"] <= \"09:20\")]\n",
        "        if len(range_df) < 5:\n",
        "            continue\n",
        "        range_high, range_low = range_df[\"High\"].max(), range_df[\"Low\"].min()\n",
        "\n",
        "        # Post 9:20 data\n",
        "        post_df = day_pd[day_pd[\"TradeTime\"] > \"09:20\"]\n",
        "        if len(post_df) < 2:\n",
        "            continue\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(post_df) - 1):\n",
        "            cur = post_df.iloc[i]\n",
        "            nxt = post_df.iloc[i + 1]\n",
        "            if side == \"LONG\" and cur[\"High\"] > range_high and nxt[\"Close\"] > range_high:\n",
        "                entry_price = nxt[\"Close\"]\n",
        "                trigger_dt = nxt[\"dt\"]\n",
        "                trigger_time = nxt[\"TradeTime\"]\n",
        "                triggered = True\n",
        "                break\n",
        "            elif side == \"SHORT\" and cur[\"Low\"] < range_low and nxt[\"Close\"] < range_low:\n",
        "                entry_price = nxt[\"Close\"]\n",
        "                trigger_dt = nxt[\"dt\"]\n",
        "                trigger_time = nxt[\"TradeTime\"]\n",
        "                triggered = True\n",
        "                break\n",
        "\n",
        "        if not triggered or trigger_time > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        # SL detection: two consecutive closes beyond range\n",
        "        from_entry = day_pd[day_pd[\"TradeTime\"] >= trigger_time]\n",
        "        if len(from_entry) == 0:\n",
        "            continue\n",
        "\n",
        "        exit_price = exit_time = exit_dt = exit_reason = None\n",
        "        prev_breach = False\n",
        "        for _, r in from_entry.iterrows():\n",
        "            breach = (r[\"Close\"] < range_low) if side == \"LONG\" else (r[\"Close\"] > range_high)\n",
        "            if breach and prev_breach:\n",
        "                exit_price = r[\"Close\"]\n",
        "                exit_time = r[\"TradeTime\"]\n",
        "                exit_dt = r[\"dt\"]\n",
        "                exit_reason = f\"INDIV_SL_{exit_time}\"\n",
        "                break\n",
        "            prev_breach = breach\n",
        "\n",
        "        if exit_price is None:\n",
        "            end_row = from_entry[from_entry[\"TradeTime\"] == END_TIME]\n",
        "            if not end_row.empty:\n",
        "                exit_price = end_row.iloc[0][\"Close\"]\n",
        "                exit_dt = end_row.iloc[0][\"dt\"]\n",
        "                exit_time = END_TIME\n",
        "            else:\n",
        "                exit_price = from_entry.iloc[-1][\"Close\"]\n",
        "                exit_dt = from_entry.iloc[-1][\"dt\"]\n",
        "                exit_time = from_entry.iloc[-1][\"TradeTime\"]\n",
        "            exit_reason = \"END_TIME\" if exit_reason is None else exit_reason\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": pd.to_datetime(trigger_dt),\n",
        "            \"exit_dt\": pd.to_datetime(exit_dt),\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"exit_price\": exit_price,\n",
        "            \"trigger_time\": trigger_time,\n",
        "            \"exit_time\": exit_time,\n",
        "            \"exit_reason\": exit_reason\n",
        "        })\n",
        "\n",
        "    if not potential_entries:\n",
        "        continue\n",
        "\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "    open_positions = []\n",
        "    pot_idx = 0\n",
        "\n",
        "    while pot_idx < len(potential_entries) or open_positions:\n",
        "        next_entry = potential_entries[pot_idx][\"trigger_dt\"] if pot_idx < len(potential_entries) else None\n",
        "        next_exit = min((p[\"exit_dt\"] for p in open_positions), default=None)\n",
        "        advance_to = min(next_entry, next_exit) if next_entry and next_exit else (next_entry or next_exit)\n",
        "\n",
        "        if advance_to is None:\n",
        "            break\n",
        "\n",
        "        # Close positions at exit time\n",
        "        closed = [p for p in open_positions if p[\"exit_dt\"] == advance_to]\n",
        "        for pos in closed:\n",
        "            qty = math.floor(PER_STOCK_ALLOC / pos[\"entry_price\"])\n",
        "            if qty <= 0:\n",
        "                open_positions.remove(pos)\n",
        "                continue\n",
        "\n",
        "            adj_entry = pos[\"entry_price\"] * (1 + SLIPPAGE_PCT if pos[\"side\"] == \"LONG\" else 1 - SLIPPAGE_PCT)\n",
        "            adj_exit = pos[\"exit_price\"] * (1 - SLIPPAGE_PCT if pos[\"side\"] == \"LONG\" else 1 + SLIPPAGE_PCT)\n",
        "\n",
        "            pnl = qty * (adj_exit - adj_entry) if pos[\"side\"] == \"LONG\" else qty * (adj_entry - adj_exit)\n",
        "            cumulative_pnl += pnl\n",
        "            cum_ret = cumulative_pnl / CAPITAL * 100\n",
        "\n",
        "            output_trades.append([\n",
        "                pos[\"symbol\"], signal_date, signal_date, pos[\"side\"],\n",
        "                round(adj_entry, 2), qty, round(qty * adj_entry, 2), round(adj_exit, 2),\n",
        "                round(pnl, 2), round(pnl / (qty * adj_entry) * 100, 4),\n",
        "                round(pnl / CAPITAL * 100, 4), round(cum_ret, 4),\n",
        "                pos[\"exit_reason\"], pos[\"trigger_time\"]\n",
        "            ])\n",
        "            open_positions.remove(pos)\n",
        "\n",
        "        # Enter new positions\n",
        "        while pot_idx < len(potential_entries) and potential_entries[pot_idx][\"trigger_dt\"] == advance_to:\n",
        "            if len(open_positions) < MAX_POSITIONS:\n",
        "                open_positions.append(potential_entries[pot_idx])\n",
        "            pot_idx += 1\n",
        "\n",
        "# ------------------- SAVE RESULTS -------------------\n",
        "output_df = pd.DataFrame(output_trades, columns=[\n",
        "    \"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\", \"ENTRY_PRICE\", \"QTY\",\n",
        "    \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\", \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\",\n",
        "    \"CUMULATIVE_PORTFOLIO_RETURN%\", \"EXIT_REASON\", \"ENTRY_TIME\"\n",
        "])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest complete: {len(output_df)} trades ‚Üí OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ------------------- DAILY PNL & EQUITY CURVE -------------------\n",
        "if not output_df.empty:\n",
        "    daily_pnl = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "    daily_pnl.rename(columns={\"SYMBOL\": \"NUM_TRADES\", \"POSITION_PNL\": \"DAILY_PNL\"}, inplace=True)\n",
        "    daily_pnl[\"DAILY_RETURN%\"] = daily_pnl[\"PORTFOLIO_RETURN%\"]\n",
        "    daily_pnl[\"CUM_RETURN%\"] = daily_pnl[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    daily_pnl[\"Equity\"] = CAPITAL * (1 + daily_pnl[\"CUM_RETURN%\"] / 100)\n",
        "    daily_pnl[\"Peak\"] = daily_pnl[\"Equity\"].cummax()\n",
        "    daily_pnl[\"Drawdown\"] = daily_pnl[\"Equity\"] - daily_pnl[\"Peak\"]\n",
        "    daily_pnl[\"DD%\"] = daily_pnl[\"Drawdown\"] / daily_pnl[\"Peak\"] * 100\n",
        "\n",
        "    with pd.ExcelWriter(\"BACKTEST_RESULTS.xlsx\", engine='xlsxwriter') as writer:\n",
        "        output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "        daily_pnl.to_excel(writer, sheet_name=\"EquityCurve\", index=False)\n",
        "\n",
        "        wb = writer.book\n",
        "        ws = writer.sheets['EquityCurve']\n",
        "        chart1 = wb.add_chart({'type': 'line'})\n",
        "        chart1.add_series({\n",
        "            'name': 'Cum Return %', 'categories': ['EquityCurve', 1, 0, len(daily_pnl), 0],\n",
        "            'values': ['EquityCurve', 1, daily_pnl.columns.get_loc(\"CUM_RETURN%\"), len(daily_pnl), daily_pnl.columns.get_loc(\"CUM_RETURN%\")]\n",
        "        })\n",
        "        chart1.set_title({'name': 'Cumulative Return %'})\n",
        "        ws.insert_chart('K2', chart1)\n",
        "\n",
        "        chart2 = wb.add_chart({'type': 'line'})\n",
        "        chart2.add_series({\n",
        "            'name': 'Drawdown %', 'categories': ['EquityCurve', 1, 0, len(daily_pnl), 0],\n",
        "            'values': ['EquityCurve', 1, daily_pnl.columns.get_loc(\"DD%\"), len(daily_pnl), daily_pnl.columns.get_loc(\"DD%\")]\n",
        "        })\n",
        "        chart2.set_title({'name': 'Drawdown %'})\n",
        "        ws.insert_chart('K20', chart2)\n",
        "\n",
        "    print(f\"Results saved ‚Üí BACKTEST_RESULTS.xlsx\")\n",
        "else:\n",
        "    print(\"No trades executed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEuDNEoIJgtd"
      },
      "source": [
        "# SL hit reverse position take\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXBBeqKeZO0q",
        "outputId": "78b428ca-1e78-48fd-92e4-2f1b3e41e216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 521 cash files...\n",
            "Processed 50/521 symbols\n",
            "Processed 100/521 symbols\n",
            "Processed 150/521 symbols\n",
            "Processed 200/521 symbols\n",
            "Processed 250/521 symbols\n",
            "Processed 300/521 symbols\n",
            "Processed 350/521 symbols\n",
            "Processed 400/521 symbols\n",
            "Processed 450/521 symbols\n",
            "Processed 500/521 symbols\n",
            "Loaded 521 symbols with required times\n",
            "NIFTY500 file not found\n",
            "Found 318 potential trade dates\n",
            "Built ALL_BREAKDOWNS ‚Üí 161075 rows\n",
            "Saved ALL_BREAKDOWNS.csv\n",
            "After ranking ‚Üí 6340 potential signals\n",
            "Backtest completed. 1803 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Backtest results with equity curve saved in: BACKTEST_RESULTS.xlsx\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# USER-CONFIGURABLE PARAMETERS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "INDIVIDUAL_SL_PCT   = 0.004      # 0.4 % individual SL\n",
        "START_TIME          = \"09:15\"\n",
        "SL_ACTIVATION_TIME = \"09:15\"\n",
        "END_TIME            = \"15:15\"\n",
        "ENTRY_CUTOFF_TIME   = \"15:15\"\n",
        "CAPITAL             = 50000.0\n",
        "LEVERAGE            = 2.5\n",
        "MAX_POSITIONS       = 4\n",
        "TICK_SIZE           = 0.05\n",
        "SLIPPAGE_PCT        = 0.0005\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# DATA PATHS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "data_path    = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files    = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"Found {len(all_files)} cash files...\")\n",
        "\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# HELPERS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def load_full_data(file_path):\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    ).with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    ).with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    return max(prev) if prev else None\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# LOAD ALL SYMBOLS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "symbol_full_data      = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not sel.is_empty():\n",
        "        pdf = sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# NIFTY-500 (optional)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "nifty500_close_1529 = nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not sel.is_empty():\n",
        "        pdf = sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"NIFTY500 file found but missing required times\")\n",
        "else:\n",
        "    print(\"NIFTY500 file not found\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# BUILD ALL_BREAKDOWNS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "all_dates = set()\n",
        "for d in symbol_close_start_end.values():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"Found {len(unique_trade_dates)} potential trade dates\")\n",
        "\n",
        "all_breakdowns = []\n",
        "for trade_date in unique_trade_dates:\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    nifty_roi = None\n",
        "    if nifty500_close_1529 is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_roi = ((nifty500_close_start.loc[trade_date] -\n",
        "                         nifty500_close_1529.loc[prev_trade_date]) /\n",
        "                         nifty500_close_1529.loc[prev_trade_date]) * 100.0\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            prev_close = d[\"close_1529\"].loc[prev_trade_date] if prev_trade_date else None\n",
        "            start_close = d[\"close_start\"].loc[trade_date]\n",
        "        except Exception:\n",
        "            continue\n",
        "        if prev_close is None or prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "        all_breakdowns.append([trade_date, sym, prev_close, start_close, roi_pct, nifty_roi])\n",
        "\n",
        "print(f\"Built ALL_BREAKDOWNS ‚Üí {len(all_breakdowns)} rows\")\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\",\"SYMBOL\",\"PREV_CLOSE_1529\",\n",
        "                                     \"START_CLOSE_0916\",\"ROI_%\",\"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"Saved ALL_BREAKDOWNS.csv\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# RANKING ‚Äì TOP-10 SHORT / BOTTOM-10 LONG\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "potential_signals = []\n",
        "for signal_date, day_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    bottom = day_df.sort_values(\"ROI_%\").head(10).copy()\n",
        "    top    = day_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "    if not bottom.empty: bottom[\"SIDE\"] = \"LONG\"\n",
        "    if not top.empty:    top[\"SIDE\"]    = \"SHORT\"\n",
        "    day_pot = pd.concat([bottom, top], ignore_index=True)\n",
        "    if not day_pot.empty:\n",
        "        potential_signals.append(day_pot)\n",
        "\n",
        "potential_df = pd.concat(potential_signals, ignore_index=True) if potential_signals else pd.DataFrame()\n",
        "print(f\"After ranking ‚Üí {len(potential_df)} potential signals\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# BACKTEST LOOP (with reversal on SL)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "output_trades          = []\n",
        "cumulative_portfolio_pnl = 0.0          # <-- defined at module level\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym, side = row[\"SYMBOL\"], row[\"SIDE\"]\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None: continue\n",
        "\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) &\n",
        "                                (pl.col(\"TradeTime\") >= \"09:15\") &\n",
        "                                (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty(): continue\n",
        "\n",
        "        range_df = day_df.filter((pl.col(\"TradeTime\") >= \"09:16\") &\n",
        "                                 (pl.col(\"TradeTime\") <= \"09:20\"))\n",
        "        if len(range_df) < 5: continue\n",
        "        range_high, range_low = range_df[\"High\"].max(), range_df[\"Low\"].min()\n",
        "\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:20\").sort(\"dt\")\n",
        "        if len(later_df) < 2: continue\n",
        "        later_pd = later_df.select([\"dt\",\"TradeTime\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(later_pd)-1):\n",
        "            cur_high, cur_low = later_pd.iloc[i][\"High\"], later_pd.iloc[i][\"Low\"]\n",
        "            nxt_close, nxt_dt, nxt_time = later_pd.iloc[i+1][\"Close\"], later_pd.iloc[i+1][\"dt\"], later_pd.iloc[i+1][\"TradeTime\"]\n",
        "\n",
        "            if side == \"LONG\" and cur_high > range_high and nxt_close > range_high:\n",
        "                entry_price, trigger_dt, trigger_time = nxt_close, nxt_dt, nxt_time\n",
        "                triggered = True; break\n",
        "            if side == \"SHORT\" and cur_low < range_low and nxt_close < range_low:\n",
        "                entry_price, trigger_dt, trigger_time = nxt_close, nxt_dt, nxt_time\n",
        "                triggered = True; break\n",
        "\n",
        "        if not triggered or trigger_time > ENTRY_CUTOFF_TIME: continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt, \"symbol\": sym, \"side\": side,\n",
        "            \"entry_price\": entry_price, \"trigger_time_str\": trigger_time,\n",
        "            \"range_high\": range_high, \"range_low\": range_low\n",
        "        })\n",
        "\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    entered = 0\n",
        "    for entry in potential_entries:\n",
        "        if entered >= MAX_POSITIONS: break\n",
        "        sym   = entry[\"symbol\"]\n",
        "        side  = entry[\"side\"]\n",
        "        eprice= entry[\"entry_price\"]\n",
        "        etime = entry[\"trigger_time_str\"]\n",
        "        rhigh = entry[\"range_high\"]\n",
        "        rlow  = entry[\"range_low\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / eprice)\n",
        "        if qty <= 0: continue\n",
        "\n",
        "        # minute data from entry onward\n",
        "        day_prices = symbol_full_data[sym].filter(pl.col(\"TradeDate\") == signal_date) \\\n",
        "                        .select([\"dt\",\"TradeTime\",\"Close\",\"Low\",\"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= etime) &\n",
        "                                (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        # ---- position loop (allows multiple reversals) ----\n",
        "        cur_side   = side\n",
        "        cur_eprice = eprice\n",
        "        cur_etime  = etime\n",
        "        cur_adj_e  = cur_eprice * (1 + SLIPPAGE_PCT if cur_side==\"LONG\" else 1 - SLIPPAGE_PCT)\n",
        "        pos_val    = qty * cur_adj_e\n",
        "        prev_cond  = False\n",
        "        i = 0\n",
        "\n",
        "        # *** GLOBAL DECLARATION MOVED HERE (before any use) ***\n",
        "        global cumulative_portfolio_pnl\n",
        "\n",
        "        while i < len(day_prices):\n",
        "            row = day_prices.iloc[i]\n",
        "            close, ctime, cdt = row[\"Close\"], row[\"TradeTime\"], row[\"dt\"]\n",
        "\n",
        "            # ---- EOD exit ----\n",
        "            if ctime == END_TIME or i == len(day_prices)-1:\n",
        "                adj_exit = close * (1 - SLIPPAGE_PCT if cur_side==\"LONG\" else 1 + SLIPPAGE_PCT)\n",
        "                pnl = qty * (adj_exit - cur_adj_e) if cur_side==\"LONG\" else qty * (cur_adj_e - adj_exit)\n",
        "                roi = (pnl / pos_val) * 100 if pos_val else 0\n",
        "                port_ret = (pnl / CAPITAL) * 100\n",
        "                cumulative_portfolio_pnl += pnl\n",
        "                cum_ret = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "                reason = END_TIME if ctime == END_TIME else \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "                output_trades.append([sym, signal_date, signal_date, cur_side,\n",
        "                                      cur_adj_e, qty, pos_val, adj_exit, pnl,\n",
        "                                      roi, port_ret, cum_ret, reason, cur_etime])\n",
        "                break\n",
        "\n",
        "            # ---- SL check (two consecutive closes) ----\n",
        "            cond = (cur_side==\"LONG\" and close < rlow) or (cur_side==\"SHORT\" and close > rhigh)\n",
        "            hit  = cond and prev_cond\n",
        "            prev_cond = cond\n",
        "\n",
        "            if hit:\n",
        "                adj_exit = close * (1 - SLIPPAGE_PCT if cur_side==\"LONG\" else 1 + SLIPPAGE_PCT)\n",
        "                pnl = qty * (adj_exit - cur_adj_e) if cur_side==\"LONG\" else qty * (cur_adj_e - adj_exit)\n",
        "                roi = (pnl / pos_val) * 100 if pos_val else 0\n",
        "                port_ret = (pnl / CAPITAL) * 100\n",
        "                cumulative_portfolio_pnl += pnl\n",
        "                cum_ret = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "                reason = f\"REVERSAL_SL_{ctime}\"\n",
        "\n",
        "                output_trades.append([sym, signal_date, signal_date, cur_side,\n",
        "                                      cur_adj_e, qty, pos_val, adj_exit, pnl,\n",
        "                                      roi, port_ret, cum_ret, reason, cur_etime])\n",
        "\n",
        "                # ---- REVERSE ----\n",
        "                cur_side   = \"SHORT\" if cur_side==\"LONG\" else \"LONG\"\n",
        "                cur_eprice = close\n",
        "                cur_etime  = ctime\n",
        "                cur_adj_e  = cur_eprice * (1 + SLIPPAGE_PCT if cur_side==\"LONG\" else 1 - SLIPPAGE_PCT)\n",
        "                pos_val    = qty * cur_adj_e\n",
        "                prev_cond  = False\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        entered += 1\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# SAVE TRADES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "    columns=[\"SYMBOL\",\"SIGNAL_DATE\",\"TRADE_DATE\",\"SIDE\",\n",
        "             \"ENTRY_PRICE\",\"QTY\",\"POSITION_VALUE\",\"EXIT_PRICE\",\"POSITION_PNL\",\n",
        "             \"TRADE_ROI%\",\"PORTFOLIO_RETURN%\",\"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "             \"EXIT_REASON\",\"ENTRY_TIME\"])\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# DAILY PnL ‚Äì CORRECT AVG_TRADE_ROI%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if not output_df.empty:\n",
        "    # 1. average ROI per symbol per day\n",
        "    sym_avg = (output_df.groupby([\"TRADE_DATE\",\"SYMBOL\"])[\"TRADE_ROI%\"]\n",
        "                       .mean()\n",
        "                       .reset_index(name=\"SYMBOL_AVG_ROI\"))\n",
        "\n",
        "    # 2. daily aggregates\n",
        "    daily = (output_df.groupby(\"TRADE_DATE\")\n",
        "             .agg({\"POSITION_PNL\":\"sum\",\n",
        "                   \"PORTFOLIO_RETURN%\":\"sum\",\n",
        "                   \"SYMBOL\":\"nunique\"})   # unique symbols = number of averaged trades\n",
        "             .reset_index())\n",
        "\n",
        "    # merge the per-symbol average ROI and then average those\n",
        "    daily = daily.merge(sym_avg.groupby(\"TRADE_DATE\")[\"SYMBOL_AVG_ROI\"]\n",
        "                            .mean()\n",
        "                            .reset_index(name=\"AVG_TRADE_ROI%\"),\n",
        "                        on=\"TRADE_DATE\", how=\"left\")\n",
        "\n",
        "    daily.rename(columns={\n",
        "        \"SYMBOL\":\"NUM_TRADES\",           # now counts unique symbols (after averaging)\n",
        "        \"POSITION_PNL\":\"DAILY_TOTAL_PNL\",\n",
        "        \"PORTFOLIO_RETURN%\":\"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily[\"CUMULATIVE_RETURN%\"] = daily[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    # equity curve & drawdown\n",
        "    daily['TRADE_DATE'] = pd.to_datetime(daily['TRADE_DATE'])\n",
        "    daily = daily.sort_values('TRADE_DATE')\n",
        "    daily['CUMULATIVE_PNL_ABS'] = CAPITAL * daily['CUMULATIVE_RETURN%'] / 100\n",
        "    daily['Equity'] = CAPITAL + daily['CUMULATIVE_PNL_ABS']\n",
        "    daily['Peak_Equity'] = daily['Equity'].cummax()\n",
        "    daily['Drawdown_Abs'] = daily['Equity'] - daily['Peak_Equity']\n",
        "    daily['Drawdown_Pct'] = daily['Drawdown_Abs'] / daily['Peak_Equity'] * 100\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ save to Excel with charts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    out_path = \"BACKTEST_RESULTS.xlsx\"\n",
        "    with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
        "        output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "        daily.to_excel(writer, sheet_name=\"EquityCurve\", index=False)\n",
        "\n",
        "        wb = writer.book\n",
        "        ws = writer.sheets['EquityCurve']\n",
        "\n",
        "        # Cumulative Return %\n",
        "        ch1 = wb.add_chart({'type':'line'})\n",
        "        ch1.add_series({\n",
        "            'name':'Cumulative Return %',\n",
        "            'categories':['EquityCurve',1,0,len(daily),0],\n",
        "            'values':['EquityCurve',1,daily.columns.get_loc(\"CUMULATIVE_RETURN%\"),len(daily),daily.columns.get_loc(\"CUMULATIVE_RETURN%\")]\n",
        "        })\n",
        "        ch1.set_title({'name':'Cumulative Return %'})\n",
        "        ch1.set_x_axis({'name':'Date','date_axis':True,'num_format':'yyyy-mm-dd'})\n",
        "        ch1.set_y_axis({'name':'%'})\n",
        "        ws.insert_chart('J2', ch1)\n",
        "\n",
        "        # Drawdown %\n",
        "        ch2 = wb.add_chart({'type':'line'})\n",
        "        ch2.add_series({\n",
        "            'name':'Drawdown %',\n",
        "            'categories':['EquityCurve',1,0,len(daily),0],\n",
        "            'values':['EquityCurve',1,daily.columns.get_loc(\"Drawdown_Pct\"),len(daily),daily.columns.get_loc(\"Drawdown_Pct\")]\n",
        "        })\n",
        "        ch2.set_title({'name':'Drawdown %'})\n",
        "        ch2.set_x_axis({'name':'Date','date_axis':True,'num_format':'yyyy-mm-dd'})\n",
        "        ch2.set_y_axis({'name':'%'})\n",
        "        ws.insert_chart('J20', ch2)\n",
        "\n",
        "    print(f\"Backtest results with equity curve saved in: {out_path}\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH3CKPpNf5aT"
      },
      "source": [
        "next min close entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2AcpUD1f4_d",
        "outputId": "90c12cf3-906c-49a8-dc3f-4fc2589aa5e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 559 cash files...\n",
            "‚úÖ Processed 50/559 symbols\n",
            "‚úÖ Processed 100/559 symbols\n",
            "‚úÖ Processed 150/559 symbols\n",
            "‚úÖ Processed 200/559 symbols\n",
            "‚úÖ Processed 250/559 symbols\n",
            "‚úÖ Processed 300/559 symbols\n",
            "‚úÖ Processed 350/559 symbols\n",
            "‚úÖ Processed 400/559 symbols\n",
            "‚úÖ Processed 450/559 symbols\n",
            "‚úÖ Processed 500/559 symbols\n",
            "‚úÖ Processed 550/559 symbols\n",
            "‚úÖ Loaded 559 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 247 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 135459 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 4920 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 984 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Backtest results with equity curve saved in: BACKTEST_RESULTS.xlsx\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "START_TIME = \"09:15\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "SLIPPAGE_PCT = 0.0005          # 0.05% slippage\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data22\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get range from 9:16 to 9:20\n",
        "        range_df = day_df.filter((pl.col(\"TradeTime\") >= \"09:16\") & (pl.col(\"TradeTime\") <= \"09:20\"))\n",
        "        if len(range_df) < 5:\n",
        "            continue  # Expect at least 5 minutes\n",
        "\n",
        "        range_high = range_df[\"High\"].max()\n",
        "        range_low = range_df[\"Low\"].min()\n",
        "\n",
        "        # Get later minutes after 9:20\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:20\").sort(\"dt\")\n",
        "        if len(later_df) < 2:\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(later_pd) - 1):\n",
        "            cur_high = later_pd.iloc[i][\"High\"]\n",
        "            cur_low = later_pd.iloc[i][\"Low\"]\n",
        "            next_close = later_pd.iloc[i + 1][\"Close\"]\n",
        "            next_dt = later_pd.iloc[i + 1][\"dt\"]\n",
        "            next_time_str = later_pd.iloc[i + 1][\"TradeTime\"]\n",
        "\n",
        "            if side == \"LONG\":\n",
        "                if cur_high > range_high and next_close > range_high:\n",
        "                    # Use next minute's close price as entry price\n",
        "                    if i + 2 < len(later_pd):  # Ensure there's a next minute\n",
        "                        entry_price = later_pd.iloc[i + 2][\"Close\"]\n",
        "                        trigger_dt = later_pd.iloc[i + 2][\"dt\"]\n",
        "                        trigger_time_str = later_pd.iloc[i + 2][\"TradeTime\"]\n",
        "                    else:\n",
        "                        continue  # Skip if no next minute available\n",
        "                    triggered = True\n",
        "                    break\n",
        "            else:  # SHORT\n",
        "                if cur_low < range_low and next_close < range_low:\n",
        "                    # Use next minute's close price as entry price\n",
        "                    if i + 2 < len(later_pd):  # Ensure there's a next minute\n",
        "                        entry_price = later_pd.iloc[i + 2][\"Close\"]\n",
        "                        trigger_dt = later_pd.iloc[i + 2][\"dt\"]\n",
        "                        trigger_time_str = later_pd.iloc[i + 2][\"TradeTime\"]\n",
        "                    else:\n",
        "                        continue  # Skip if no next minute available\n",
        "                    triggered = True\n",
        "                    break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"range_high\": range_high,\n",
        "            \"range_low\": range_low\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        range_high = entry[\"range_high\"]\n",
        "        range_low = entry[\"range_low\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "        prev_is_sl_condition = False\n",
        "\n",
        "        for _, minute_row in day_prices.iterrows():\n",
        "            cur_close = minute_row[\"Close\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Check SL condition for two consecutive closes\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                is_below = cur_close < range_low\n",
        "                if is_below and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_below\n",
        "            else:  # SHORT\n",
        "                is_above = cur_close > range_high\n",
        "                if is_above and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_above\n",
        "\n",
        "            if hit_condition:\n",
        "                exit_price = cur_close\n",
        "                exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Apply slippage\n",
        "        adjusted_entry_price = entry_price * (1 + SLIPPAGE_PCT if side == \"LONG\" else 1 - SLIPPAGE_PCT)\n",
        "        adjusted_exit_price = exit_price * (1 - SLIPPAGE_PCT if side == \"LONG\" else 1 + SLIPPAGE_PCT)\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (adjusted_exit_price - adjusted_entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (adjusted_entry_price - adjusted_exit_price)\n",
        "\n",
        "        position_value = qty * adjusted_entry_price\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            adjusted_entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            adjusted_exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    # Compute equity curve and drawdown\n",
        "    daily_pnl_df['TRADE_DATE'] = pd.to_datetime(daily_pnl_df['TRADE_DATE'])\n",
        "    daily_pnl_df = daily_pnl_df.sort_values('TRADE_DATE')\n",
        "    daily_pnl_df['CUMULATIVE_PNL_ABS'] = CAPITAL * daily_pnl_df['CUMULATIVE_RETURN%'] / 100\n",
        "    daily_pnl_df['Equity'] = CAPITAL + daily_pnl_df['CUMULATIVE_PNL_ABS']\n",
        "    daily_pnl_df['Peak_Equity'] = daily_pnl_df['Equity'].cummax()\n",
        "    daily_pnl_df['Drawdown_Abs'] = daily_pnl_df['Equity'] - daily_pnl_df['Peak_Equity']\n",
        "    daily_pnl_df['Drawdown_Pct'] = (daily_pnl_df['Drawdown_Abs'] / daily_pnl_df['Peak_Equity']) * 100\n",
        "\n",
        "    # Save to Excel with charts\n",
        "    out_path = \"BACKTEST_RESULTS.xlsx\"\n",
        "    with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
        "        output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"EquityCurve\", index=False)\n",
        "\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['EquityCurve']\n",
        "\n",
        "        # ‚úÖ Cumulative PnL Chart\n",
        "        chart1 = workbook.add_chart({'type': 'line'})\n",
        "        chart1.add_series({\n",
        "            'name': 'Cumulative Return %',\n",
        "            'categories': ['EquityCurve', 1, 0, len(daily_pnl_df), 0],\n",
        "            'values': ['EquityCurve', 1, 8, len(daily_pnl_df), 8],  # Assuming CUMULATIVE_RETURN% is column index 8 (I)\n",
        "        })\n",
        "        chart1.set_title({'name': 'Cumulative Return %'})\n",
        "        chart1.set_x_axis({'name': 'Date', 'date_axis': True, 'num_format': 'yyyy-mm-dd'})\n",
        "        chart1.set_y_axis({'name': 'Cumulative Return %'})\n",
        "        worksheet.insert_chart('J2', chart1)\n",
        "\n",
        "        # ‚úÖ Drawdown Chart\n",
        "        chart2 = workbook.add_chart({'type': 'line'})\n",
        "        chart2.add_series({\n",
        "            'name': 'Drawdown %',\n",
        "            'categories': ['EquityCurve', 1, 0, len(daily_pnl_df), 0],\n",
        "            'values': ['EquityCurve', 1, 11, len(daily_pnl_df), 11],  # Assuming Drawdown_Pct is column index 11 (L)\n",
        "        })\n",
        "        chart2.set_title({'name': 'Drawdown %'})\n",
        "        chart2.set_x_axis({'name': 'Date', 'date_axis': True, 'num_format': 'yyyy-mm-dd'})\n",
        "        chart2.set_y_axis({'name': 'Drawdown %'})\n",
        "        worksheet.insert_chart('J20', chart2)\n",
        "\n",
        "    print(f\"üìÑ Backtest results with equity curve saved in: {out_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UeO7L98Vgc-",
        "outputId": "b1d23292-cd22-4d8d-f637-f309c07325c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n",
            "üöÄ Found 559 cash files...\n",
            "‚úÖ Processed 50/559 symbols\n",
            "‚úÖ Processed 100/559 symbols\n",
            "‚úÖ Processed 150/559 symbols\n",
            "‚úÖ Processed 200/559 symbols\n",
            "‚úÖ Processed 250/559 symbols\n",
            "‚úÖ Processed 300/559 symbols\n",
            "‚úÖ Processed 350/559 symbols\n",
            "‚úÖ Processed 400/559 symbols\n",
            "‚úÖ Processed 450/559 symbols\n",
            "‚úÖ Processed 500/559 symbols\n",
            "‚úÖ Processed 550/559 symbols\n",
            "‚úÖ Loaded 559 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 247 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 135459 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\n",
            "‚úÖ After ranking ‚Üí 4920 potential signals (up to 20 per date)\n",
            "‚úÖ Backtest completed. 984 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Backtest results with equity curve saved in: BACKTEST_RESULTS.xlsx\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% individual SL\n",
        "START_TIME = \"09:15\"           # Snapshot time for ROI\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding\n",
        "SLIPPAGE_PCT = 0.0005          # 0.05% slippage\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data22\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:16): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date -----\n",
        "# Each row: SIGNAL_DATE, SYMBOL, PREV_CLOSE_1529, START_CLOSE_0916, ROI_%, NIFTY500_ROI_%\n",
        "all_breakdowns = []\n",
        "\n",
        "for trade_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(trade_date, unique_trade_dates)\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None and prev_trade_date is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[trade_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            # Fetch prev close (15:29 from previous trading day) and start close (09:16 on trade_date)\n",
        "            prev_close = None\n",
        "            start_close = None\n",
        "            try:\n",
        "                if prev_trade_date is not None:\n",
        "                    prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_close = None\n",
        "            try:\n",
        "                start_close = float(d[\"close_start\"].loc[trade_date])\n",
        "            except Exception:\n",
        "                start_close = None\n",
        "\n",
        "            # Require both to compute ROI\n",
        "            if prev_close is None or start_close is None or prev_close == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((start_close - prev_close) / prev_close) * 100.0\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                trade_date,\n",
        "                sym,\n",
        "                prev_close,\n",
        "                start_close,\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date\n",
        "            ])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv\n",
        "breakdown_df = pd.DataFrame(all_breakdowns,\n",
        "                            columns=[\"SIGNAL_DATE\", \"SYMBOL\", \"PREV_CLOSE_1529\", \"START_CLOSE_0916\", \"ROI_%\", \"NIFTY500_ROI_%\"])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, ROI vs prev and NIFTY500 ROI)\")\n",
        "\n",
        "# ----- Ranking logic: for each SIGNAL_DATE pick top10 (highest ROI) for shorts and bottom10 (lowest ROI) for longs -----\n",
        "potential_signals = []\n",
        "\n",
        "for signal_date, daily_df in breakdown_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # Pick top10 (highest ROI_%) for SHORT and bottom10 (lowest ROI_%) for LONG\n",
        "    try:\n",
        "        bottom10 = daily_df.sort_values(\"ROI_%\", ascending=True).head(10).copy()\n",
        "        if not bottom10.empty:\n",
        "            bottom10[\"SIDE\"] = \"LONG\"\n",
        "        top10 = daily_df.sort_values(\"ROI_%\", ascending=False).head(10).copy()\n",
        "        if not top10.empty:\n",
        "            top10[\"SIDE\"] = \"SHORT\"\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Combine into day's potentials\n",
        "    day_potential = pd.concat([bottom10, top10], ignore_index=True) if (not bottom10.empty or not top10.empty) else pd.DataFrame()\n",
        "    if not day_potential.empty:\n",
        "        potential_signals.append(day_potential)\n",
        "\n",
        "if potential_signals:\n",
        "    potential_df = pd.concat(potential_signals, ignore_index=True)\n",
        "else:\n",
        "    potential_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After ranking ‚Üí {len(potential_df)} potential signals (up to 20 per date)\")\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date, day_potentials in potential_df.groupby(\"SIGNAL_DATE\"):\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for _, row in day_potentials.iterrows():\n",
        "        sym = row[\"SYMBOL\"]\n",
        "        side = row[\"SIDE\"]\n",
        "\n",
        "        # Pull full-day minute prices for trade_date\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get range from 9:16 to 9:20\n",
        "        range_df = day_df.filter((pl.col(\"TradeTime\") >= \"09:16\") & (pl.col(\"TradeTime\") <= \"09:20\"))\n",
        "        if len(range_df) < 5:\n",
        "            continue  # Expect at least 5 minutes\n",
        "\n",
        "        range_high = range_df[\"High\"].max()\n",
        "        range_low = range_df[\"Low\"].min()\n",
        "\n",
        "        # Get later minutes after 9:20\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:20\").sort(\"dt\")\n",
        "        if len(later_df) < 3:\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for i in range(len(later_pd) - 2):\n",
        "            cur_high = later_pd.iloc[i][\"High\"]\n",
        "            cur_low = later_pd.iloc[i][\"Low\"]\n",
        "            next_close = later_pd.iloc[i + 1][\"Close\"]\n",
        "            next_dt = later_pd.iloc[i + 1][\"dt\"]\n",
        "            next_time_str = later_pd.iloc[i + 1][\"TradeTime\"]\n",
        "\n",
        "            if side == \"LONG\":\n",
        "                if cur_high > range_high and next_close > range_high:\n",
        "                    # Use next next minute's close price as entry price\n",
        "                    entry_price = later_pd.iloc[i + 2][\"Close\"]\n",
        "                    trigger_dt = later_pd.iloc[i + 2][\"dt\"]\n",
        "                    trigger_time_str = later_pd.iloc[i + 2][\"TradeTime\"]\n",
        "                    triggered = True\n",
        "                    break\n",
        "            else:  # SHORT\n",
        "                if cur_low < range_low and next_close < range_low:\n",
        "                    # Use next next minute's close price as entry price\n",
        "                    entry_price = later_pd.iloc[i + 2][\"Close\"]\n",
        "                    trigger_dt = later_pd.iloc[i + 2][\"dt\"]\n",
        "                    trigger_time_str = later_pd.iloc[i + 2][\"TradeTime\"]\n",
        "                    triggered = True\n",
        "                    break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        # Skip if after entry cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"range_high\": range_high,\n",
        "            \"range_low\": range_low\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "    day_pnl = 0.0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        range_high = entry[\"range_high\"]\n",
        "        range_low = entry[\"range_low\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        # Pull minute prices from trigger time onward\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter((pl.col(\"TradeDate\") == signal_date)).select([\"dt\", \"TradeTime\", \"Close\", \"Low\", \"High\"]).to_pandas()\n",
        "        day_prices = day_prices[(day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)]\n",
        "        day_prices[\"dt\"] = pd.to_datetime(day_prices[\"dt\"])\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = END_TIME\n",
        "        exit_dt = None\n",
        "        prev_is_sl_condition = False\n",
        "\n",
        "        for idx, minute_row in day_prices.iterrows():\n",
        "            cur_close = minute_row[\"Close\"]\n",
        "            cur_time = minute_row[\"TradeTime\"]\n",
        "            cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "            # Check SL condition for two consecutive closes\n",
        "            hit_condition = False\n",
        "            if side == \"LONG\":\n",
        "                is_below = cur_close < range_low\n",
        "                if is_below and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_below\n",
        "            else:  # SHORT\n",
        "                is_above = cur_close > range_high\n",
        "                if is_above and prev_is_sl_condition:\n",
        "                    hit_condition = True\n",
        "                prev_is_sl_condition = is_above\n",
        "\n",
        "            if hit_condition:\n",
        "                if idx + 1 < len(day_prices):\n",
        "                    next_row = day_prices.iloc[idx + 1]\n",
        "                    exit_price = next_row[\"Close\"]\n",
        "                    exit_reason = f\"INDIV_SL_{next_row['TradeTime']}\"\n",
        "                    exit_dt = next_row[\"dt\"]\n",
        "                else:\n",
        "                    exit_price = cur_close\n",
        "                    exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "                    exit_dt = cur_dt\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Use END_TIME price if no SL triggered\n",
        "            end_time_prices = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            if not end_time_prices.empty:\n",
        "                exit_price = end_time_prices[\"Close\"].values[0]\n",
        "            else:\n",
        "                # Fallback to last available price\n",
        "                exit_price = day_prices[\"Close\"].iloc[-1] if not day_prices.empty else entry_price\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Apply slippage\n",
        "        adjusted_entry_price = entry_price * (1 + SLIPPAGE_PCT if side == \"LONG\" else 1 - SLIPPAGE_PCT)\n",
        "        adjusted_exit_price = exit_price * (1 - SLIPPAGE_PCT if side == \"LONG\" else 1 + SLIPPAGE_PCT)\n",
        "\n",
        "        # Compute PnL and ROI\n",
        "        if side == \"LONG\":\n",
        "            position_pnl = qty * (adjusted_exit_price - adjusted_entry_price)\n",
        "        else:  # SHORT\n",
        "            position_pnl = qty * (adjusted_entry_price - adjusted_exit_price)\n",
        "\n",
        "        position_value = qty * adjusted_entry_price\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,\n",
        "            signal_date,  # TRADE_DATE same\n",
        "            side,\n",
        "            adjusted_entry_price,\n",
        "            qty,\n",
        "            position_value,\n",
        "            adjusted_exit_price,\n",
        "            position_pnl,\n",
        "            trade_roi_pct,\n",
        "            portfolio_return_pct,\n",
        "            cumulative_return_pct,\n",
        "            exit_reason,\n",
        "            trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "\n",
        "    # Compute equity curve and drawdown\n",
        "    daily_pnl_df['TRADE_DATE'] = pd.to_datetime(daily_pnl_df['TRADE_DATE'])\n",
        "    daily_pnl_df = daily_pnl_df.sort_values('TRADE_DATE')\n",
        "    daily_pnl_df['CUMULATIVE_PNL_ABS'] = CAPITAL * daily_pnl_df['CUMULATIVE_RETURN%'] / 100\n",
        "    daily_pnl_df['Equity'] = CAPITAL + daily_pnl_df['CUMULATIVE_PNL_ABS']\n",
        "    daily_pnl_df['Peak_Equity'] = daily_pnl_df['Equity'].cummax()\n",
        "    daily_pnl_df['Drawdown_Abs'] = daily_pnl_df['Equity'] - daily_pnl_df['Peak_Equity']\n",
        "    daily_pnl_df['Drawdown_Pct'] = (daily_pnl_df['Drawdown_Abs'] / daily_pnl_df['Peak_Equity']) * 100\n",
        "\n",
        "    # Save to Excel with charts\n",
        "    out_path = \"BACKTEST_RESULTS.xlsx\"\n",
        "    with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
        "        output_df.to_excel(writer, sheet_name=\"Trades\", index=False)\n",
        "        daily_pnl_df.to_excel(writer, sheet_name=\"EquityCurve\", index=False)\n",
        "\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['EquityCurve']\n",
        "\n",
        "        # ‚úÖ Cumulative PnL Chart\n",
        "        chart1 = workbook.add_chart({'type': 'line'})\n",
        "        chart1.add_series({\n",
        "            'name': 'Cumulative Return %',\n",
        "            'categories': ['EquityCurve', 1, 0, len(daily_pnl_df), 0],\n",
        "            'values': ['EquityCurve', 1, 8, len(daily_pnl_df), 8],  # Assuming CUMULATIVE_RETURN% is column index 8 (I)\n",
        "        })\n",
        "        chart1.set_title({'name': 'Cumulative Return %'})\n",
        "        chart1.set_x_axis({'name': 'Date', 'date_axis': True, 'num_format': 'yyyy-mm-dd'})\n",
        "        chart1.set_y_axis({'name': 'Cumulative Return %'})\n",
        "        worksheet.insert_chart('J2', chart1)\n",
        "\n",
        "        # ‚úÖ Drawdown Chart\n",
        "        chart2 = workbook.add_chart({'type': 'line'})\n",
        "        chart2.add_series({\n",
        "            'name': 'Drawdown %',\n",
        "            'categories': ['EquityCurve', 1, 0, len(daily_pnl_df), 0],\n",
        "            'values': ['EquityCurve', 1, 11, len(daily_pnl_df), 11],  # Assuming Drawdown_Pct is column index 11 (L)\n",
        "        })\n",
        "        chart2.set_title({'name': 'Drawdown %'})\n",
        "        chart2.set_x_axis({'name': 'Date', 'date_axis': True, 'num_format': 'yyyy-mm-dd'})\n",
        "        chart2.set_y_axis({'name': 'Drawdown %'})\n",
        "        worksheet.insert_chart('J20', chart2)\n",
        "\n",
        "    print(f\"üìÑ Backtest results with equity curve saved in: {out_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbPr5kKYvfQj"
      },
      "source": [
        "# GOLDCASE INTRADAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyRbhuj-VOHM",
        "outputId": "aaf73630-7264-415f-9713-f2de73f566ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ GOLDCASE file found: /content/drive/MyDrive/Cash_data/cash_GOLDCASE.csv\n",
            "‚úÖ GOLDCASE trade dates loaded: 297\n",
            "‚úÖ Backtest completed. 297 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST_GOLDCASE.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL_GOLDCASE.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "ALLOCATION_PER_TRADE = 25000     # Rupees per trade\n",
        "TARGET_PCT = 0.02                # 2% target\n",
        "INDIVIDUAL_SL_PCT = 0.004        # 0.4% individual SL applied at entry\n",
        "ENTRY_TIME = \"09:15\"\n",
        "EXIT_TIME = \"09:17\"\n",
        "DATA_PATH = \"/content/drive/MyDrive/Cash_data\"\n",
        "\n",
        "# ------------------ LOAD FILE ------------------\n",
        "all_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n",
        "gold_file = [f for f in all_files if \"GOLDCASE\" in os.path.basename(f)]\n",
        "if not gold_file:\n",
        "    raise FileNotFoundError(\"‚ö†Ô∏è GOLDCASE CSV not found in the data path\")\n",
        "gold_file = gold_file[0]\n",
        "print(f\"üöÄ GOLDCASE file found: {gold_file}\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "    return symbol, df\n",
        "\n",
        "symbol, symbol_df = load_full_data(gold_file)\n",
        "symbol_full_data = {symbol: symbol_df}\n",
        "\n",
        "# ------------------ PREP ENTRY AND EXIT PRICES ------------------\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "df_sel = symbol_df.filter(pl.col(\"TradeTime\").is_in([ENTRY_TIME, EXIT_TIME]))\n",
        "if not df_sel.is_empty():\n",
        "    pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    entry_price_series = pdf[pdf[\"TradeTime\"] == ENTRY_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    exit_price_series = pdf[pdf[\"TradeTime\"] == EXIT_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "    symbol_close_start_end[symbol] = {\"entry\": entry_price_series, \"exit\": exit_price_series}\n",
        "\n",
        "trade_dates = sorted(symbol_close_start_end[symbol][\"entry\"].index)\n",
        "print(f\"‚úÖ GOLDCASE trade dates loaded: {len(trade_dates)}\")\n",
        "\n",
        "# ------------------ BACKTEST LOOP ------------------\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for trade_date in trade_dates:\n",
        "    try:\n",
        "        entry_price = float(symbol_close_start_end[symbol][\"entry\"].loc[trade_date])\n",
        "        exit_price = float(symbol_close_start_end[symbol][\"exit\"].loc[trade_date])\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # Quantity based on allocation\n",
        "    quantity = int(ALLOCATION_PER_TRADE / entry_price)\n",
        "    if quantity <= 0:\n",
        "        continue\n",
        "\n",
        "    side = \"SHORT\"\n",
        "    target_price = entry_price * (1 - TARGET_PCT)   # SHORT target\n",
        "    sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)  # SHORT SL applied immediately\n",
        "\n",
        "    # Pull minute-level prices between ENTRY and EXIT\n",
        "    df_full = symbol_full_data[symbol]\n",
        "    day_prices = df_full.filter(\n",
        "        (pl.col(\"TradeDate\") == trade_date) &\n",
        "        (pl.col(\"TradeTime\") >= ENTRY_TIME) &\n",
        "        (pl.col(\"TradeTime\") <= EXIT_TIME)\n",
        "    ).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "    trade_exit_price = None\n",
        "    exit_reason = EXIT_TIME\n",
        "\n",
        "    for _, minute_row in day_prices.iterrows():\n",
        "        cur_price = minute_row[\"Close\"]\n",
        "        cur_time = minute_row[\"TradeTime\"]\n",
        "\n",
        "        # Check SL first\n",
        "        if cur_price >= sl_price:\n",
        "            trade_exit_price = cur_price\n",
        "            exit_reason = f\"INDIV_SL_{cur_time}\"\n",
        "            break\n",
        "\n",
        "        # Check target\n",
        "        if cur_price <= target_price:\n",
        "            trade_exit_price = cur_price\n",
        "            exit_reason = f\"TARGET_HIT_{cur_time}\"\n",
        "            break\n",
        "\n",
        "    if trade_exit_price is None:\n",
        "        # Use 09:19 price if neither hit\n",
        "        trade_exit_price = exit_price\n",
        "        exit_reason = \"END_TIME\"\n",
        "\n",
        "    # Compute PnL and ROI\n",
        "    trade_pnl = round((entry_price - trade_exit_price) * quantity, 2)  # SHORT PnL\n",
        "    roi_trade = round((trade_pnl / ALLOCATION_PER_TRADE) * 100, 2)\n",
        "    cumulative_portfolio_return += roi_trade\n",
        "\n",
        "    output_trades.append([\n",
        "        symbol,\n",
        "        trade_date,\n",
        "        trade_date,\n",
        "        side,\n",
        "        entry_price,\n",
        "        trade_exit_price,\n",
        "        trade_pnl,\n",
        "        roi_trade,\n",
        "        exit_reason,\n",
        "        round(roi_trade, 2),\n",
        "        round(cumulative_portfolio_return, 2)\n",
        "    ])\n",
        "\n",
        "# ------------------ SAVE OUTPUT ------------------\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST_GOLDCASE.csv\", index=False)\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST_GOLDCASE.csv\")\n",
        "\n",
        "# ------------------ DAILY PNL ------------------\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL_GOLDCASE.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL_GOLDCASE.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ijAclvdMMT"
      },
      "source": [
        "# Chirag Intraday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KocDhz9hvt4C",
        "outputId": "93cfa6cc-213f-4442-dcd5-568167d7f6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 252 cash files that match F&O symbols\n",
            "‚úÖ Processed 50/252 symbols\n",
            "‚úÖ Processed 100/252 symbols\n",
            "‚úÖ Processed 150/252 symbols\n",
            "‚úÖ Processed 200/252 symbols\n",
            "‚úÖ Processed 250/252 symbols\n",
            "‚úÖ Loaded 252 symbols with required times and precomputed indicators\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 302 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 75816 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, revised daily setups)\n",
            "‚úÖ After selection ‚Üí 984 signals selected for trading (up to 4 per entry date)\n",
            "‚úÖ Backtest completed. 984 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================\n",
        "# üîπ CONFIG (modified per your instructions)\n",
        "# ==========================\n",
        "INDIVIDUAL_SL_PCT = 0.005      # 0.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"10:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"10:15\"   # SL activation time\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "\n",
        "# ‚úÖ Load filtered symbols\n",
        "common_symbols = pd.read_csv(\"common_symbols.csv\")[\"SYMBOL\"].tolist()\n",
        "\n",
        "# ‚úÖ Filter CSVs to only these symbols\n",
        "all_files = [\n",
        "    f for f in glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "    if os.path.splitext(os.path.basename(f))[0].replace(\"cash_\", \"\") in common_symbols\n",
        "]\n",
        "print(f\"üöÄ Found {len(all_files)} cash files that match F&O symbols\")\n",
        "\n",
        "# Path to NIFTY 500 cash file\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper to parse datetime strings used across the script\n",
        "# ----------------------------\n",
        "def parse_ts_to_dt(ts_str):\n",
        "    try:\n",
        "        return datetime.strptime(ts_str[:19], \"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Function to read each CSV and return symbol + polars DataFrame (unchanged)\n",
        "# ----------------------------\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN LOAD + PRECOMPUTE (revised: only daily, add daily High/Low series)\n",
        "# ----------------------------\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}   # holds 15:29 and START_TIME closes keyed by TradeDate\n",
        "# <<< PERFORMANCE: caches for precomputed indicators per symbol >>>\n",
        "symbol_daily_20sma = {}\n",
        "symbol_prev5day_high = {}\n",
        "symbol_prev5day_low = {}\n",
        "symbol_daily_high = {}  # new: daily High series\n",
        "symbol_daily_low = {}   # new: daily Low series\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for daily calculations)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # ensure TradeDate is datetime.date\n",
        "        pdf[\"TradeDate\"] = pd.to_datetime(pdf[\"TradeDate\"]).dt.date\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    # <<< NEW: Compute daily High and Low series >>>\n",
        "    daily_ohlc = df.group_by(\"TradeDate\").agg([\n",
        "        pl.col(\"High\").max().alias(\"DailyHigh\"),\n",
        "        pl.col(\"Low\").min().alias(\"DailyLow\")\n",
        "    ]).to_pandas()\n",
        "    if not daily_ohlc.empty:\n",
        "        daily_ohlc[\"TradeDate\"] = pd.to_datetime(daily_ohlc[\"TradeDate\"]).dt.date\n",
        "        symbol_daily_high[symbol] = daily_ohlc.set_index(\"TradeDate\")[\"DailyHigh\"].sort_index()\n",
        "        symbol_daily_low[symbol] = daily_ohlc.set_index(\"TradeDate\")[\"DailyLow\"].sort_index()\n",
        "\n",
        "    # <<< PERFORMANCE: precompute daily indicators (vectorized, using actual High/Low) >>>\n",
        "    # compute daily series from 15:29 closes (if available)\n",
        "    if symbol in symbol_close_start_end:\n",
        "        close_series = symbol_close_start_end[symbol]['close_1529']\n",
        "        if isinstance(close_series, pd.Series) and not close_series.empty:\n",
        "            # ensure index is datetime.date objects already (we set above)\n",
        "            # daily 20 SMA\n",
        "            daily_20 = close_series.rolling(window=20, min_periods=1).mean()\n",
        "            symbol_daily_20sma[symbol] = daily_20\n",
        "\n",
        "            # NEW: prev-5-day high/low using actual daily High/Low: shift(1) then rolling on previous 5\n",
        "            daily_high = symbol_daily_high.get(symbol)\n",
        "            daily_low = symbol_daily_low.get(symbol)\n",
        "            if daily_high is not None and not daily_high.empty:\n",
        "                # align indices if needed\n",
        "                common_idx = close_series.index.intersection(daily_high.index)\n",
        "                if len(common_idx) > 0:\n",
        "                    daily_high_aligned = daily_high.loc[common_idx]\n",
        "                    prev5_high = daily_high_aligned.shift(1).rolling(window=5, min_periods=1).max()\n",
        "                    symbol_prev5day_high[symbol] = prev5_high.reindex(close_series.index).ffill().bfill()\n",
        "            if daily_low is not None and not daily_low.empty:\n",
        "                common_idx = close_series.index.intersection(daily_low.index)\n",
        "                if len(common_idx) > 0:\n",
        "                    daily_low_aligned = daily_low.loc[common_idx]\n",
        "                    prev5_low = daily_low_aligned.shift(1).rolling(window=5, min_periods=1).min()\n",
        "                    symbol_prev5day_low[symbol] = prev5_low.reindex(close_series.index).ffill().bfill()\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times and precomputed indicators\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time START_TIME) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty_pdf[\"TradeDate\"] = pd.to_datetime(nifty_pdf[\"TradeDate\"]).dt.date\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the next trading day\n",
        "def get_next_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    next_dates = [d for d in all_dates if pd.Timestamp(d).date() > trade_date]\n",
        "    if not next_dates:\n",
        "        return None\n",
        "    return min(next_dates)\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d).date() < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date with revised daily-only indicators -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    entry_date = get_next_trading_day(signal_date, unique_trade_dates)\n",
        "    if entry_date is None:\n",
        "        continue\n",
        "\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)  # for crossover in bearish\n",
        "\n",
        "    # Compute NIFTY500 ROI for entry (overnight from signal close to entry open)\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None:\n",
        "        try:\n",
        "            nifty_signal_close = float(nifty500_close_1529.loc[signal_date])\n",
        "            nifty_entry_open = float(nifty500_open_start.loc[entry_date])\n",
        "            if nifty_entry_open != 0:\n",
        "                nifty_roi_for_date = ((nifty_entry_open - nifty_signal_close) / nifty_entry_open) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            signal_close = None\n",
        "            entry_open = None\n",
        "            try:\n",
        "                signal_close = float(d[\"close_1529\"].loc[signal_date])\n",
        "            except Exception:\n",
        "                signal_close = None\n",
        "            try:\n",
        "                entry_open = float(d[\"open_start\"].loc[entry_date])\n",
        "            except Exception:\n",
        "                entry_open = None\n",
        "\n",
        "            if signal_close is None or entry_open is None or entry_open == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((entry_open - signal_close) / entry_open) * 100.0\n",
        "\n",
        "            # fetch precomputed daily indicators for signal_date\n",
        "            daily_20s = symbol_daily_20sma.get(sym)\n",
        "            today_daily_20sma = None\n",
        "            prev_daily_20sma = None\n",
        "            prev_daily_close = None\n",
        "            if daily_20s is not None and signal_date in daily_20s.index:\n",
        "                today_daily_20sma = float(daily_20s.loc[signal_date])\n",
        "            if prev_trade_date is not None and daily_20s is not None and prev_trade_date in daily_20s.index:\n",
        "                prev_daily_20sma = float(daily_20s.loc[prev_trade_date])\n",
        "            # prev daily close (for crossover):\n",
        "            try:\n",
        "                series_15_29 = d[\"close_1529\"]\n",
        "                if prev_trade_date is not None and prev_trade_date in series_15_29.index:\n",
        "                    prev_daily_close = float(series_15_29.loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_daily_close = None\n",
        "\n",
        "            prev_5day_high_val = None\n",
        "            prev_5day_low_val = None\n",
        "            prev5_high_series = symbol_prev5day_high.get(sym)\n",
        "            prev5_low_series = symbol_prev5day_low.get(sym)\n",
        "            if prev5_high_series is not None and signal_date in prev5_high_series.index:\n",
        "                prev_5day_high_val = float(prev5_high_series.loc[signal_date])\n",
        "            if prev5_low_series is not None and signal_date in prev5_low_series.index:\n",
        "                prev_5day_low_val = float(prev5_low_series.loc[signal_date])\n",
        "\n",
        "            # Evaluate bullish/bearish (revised: daily only, no hourly/VWAP, actual High/Low for prev5)\n",
        "            bullish = False\n",
        "            try:\n",
        "                cond1 = (prev_5day_high_val is not None) and (signal_close > prev_5day_high_val)\n",
        "                cond2 = (today_daily_20sma is not None) and (signal_close > today_daily_20sma)\n",
        "                bullish = all([cond1, cond2])\n",
        "            except Exception:\n",
        "                bullish = False\n",
        "\n",
        "            bearish = False\n",
        "            try:\n",
        "                bcond1 = (prev_5day_low_val is not None) and (signal_close < prev_5day_low_val)\n",
        "                bcond2 = (today_daily_20sma is not None) and (signal_close < today_daily_20sma)\n",
        "                bcond3 = (prev_daily_close is not None) and (prev_daily_20sma is not None) and (prev_daily_close >= prev_daily_20sma)\n",
        "                bearish = all([bcond1, bcond2, bcond3])\n",
        "            except Exception:\n",
        "                bearish = False\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                signal_date,\n",
        "                entry_date,\n",
        "                sym,\n",
        "                prev_daily_close,  # previous close before signal\n",
        "                signal_close,      # signal close (used as \"prev_close\" for entry)\n",
        "                entry_open,        # entry open\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date,\n",
        "                today_daily_20sma,\n",
        "                prev_5day_high_val,\n",
        "                prev_5day_low_val,\n",
        "                bullish,\n",
        "                bearish\n",
        "            ])\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with revised columns (no hourly/VWAP)\n",
        "breakdown_df = pd.DataFrame(all_breakdowns, columns=[\n",
        "    \"SIGNAL_DATE\", \"ENTRY_DATE\", \"SYMBOL\", \"PREV_CLOSE_BEFORE_SIGNAL\", \"SIGNAL_CLOSE\", \"ENTRY_OPEN\", \"ROI_%\", \"NIFTY500_ROI_%\",\n",
        "    \"DAILY_20SMA\", \"PREV_5DAY_HIGH\", \"PREV_5DAY_LOW\", \"BULLISH_SETUP\", \"BEARISH_SETUP\"\n",
        "])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df[\"DAILY_20SMA\"] = pd.to_numeric(breakdown_df[\"DAILY_20SMA\"], errors='coerce').round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, revised daily setups)\")\n",
        "\n",
        "# ----- Ranking logic: For each ENTRY_DATE pick up to 2 LONG and 2 SHORT (no ranking, just head(2)) -----\n",
        "ranked_signals = []\n",
        "\n",
        "for entry_date, daily_df in breakdown_df.groupby(\"ENTRY_DATE\"):\n",
        "    # Compute NIFTY ROI for the entry day if present\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    if len(nifty_vals) == 0:\n",
        "        nifty_roi_for_date = None\n",
        "    else:\n",
        "        nifty_roi_for_date = float(nifty_vals[0])\n",
        "\n",
        "    bullish_candidates = daily_df[daily_df[\"BULLISH_SETUP\"] == True].copy()\n",
        "    bearish_candidates = daily_df[daily_df[\"BEARISH_SETUP\"] == True].copy()\n",
        "\n",
        "    top2_long = bullish_candidates.head(2).copy()\n",
        "    if not top2_long.empty:\n",
        "        top2_long[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "    top2_short = bearish_candidates.head(2).copy()\n",
        "    if not top2_short.empty:\n",
        "        top2_short[\"SIDE\"] = \"SHORT\"\n",
        "\n",
        "    day_selected = pd.concat([top2_long, top2_short], ignore_index=True) if (not top2_long.empty or not top2_short.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After selection ‚Üí {len(ranked_df)} signals selected for trading (up to 4 per entry date)\")\n",
        "\n",
        "# ----- Backtest/execution loop (revised: entry on ENTRY_DATE, prev_close is SIGNAL_CLOSE) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for entry_date, day_group in ranked_df.groupby(\"ENTRY_DATE\"):\n",
        "    # For each entry day, collect the symbols and sides\n",
        "    signals = day_group.set_index(\"SYMBOL\")[\"SIDE\"].to_dict()\n",
        "    num_signals = len(signals)\n",
        "    if num_signals == 0:\n",
        "        continue\n",
        "    weight = 1.0 / num_signals  # equal weight\n",
        "\n",
        "    # Get entry prices (ENTRY_OPEN), indiv SL prices\n",
        "    entries = {}\n",
        "    indiv_sls = {}\n",
        "    for sym, side in list(signals.items()):\n",
        "        entry_price = symbol_close_start_end.get(sym, {}).get(\"open_start\", {}).get(entry_date, None)\n",
        "        if entry_price is None or entry_price == 0:\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entries[sym] = entry_price\n",
        "        if side == \"LONG\":\n",
        "            indiv_sls[sym] = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "        else:\n",
        "            indiv_sls[sym] = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    if not signals:\n",
        "        continue\n",
        "\n",
        "    num_signals = len(signals)\n",
        "    weight = 1.0 / num_signals\n",
        "\n",
        "    # Collect day prices for symbols on entry_date\n",
        "    all_times = set()\n",
        "    day_prices = {}\n",
        "    for sym in signals:\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == entry_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_df = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)].copy()\n",
        "        if day_df.empty:\n",
        "            continue\n",
        "        day_df = day_df.set_index(\"TradeTime\")\n",
        "        day_df = day_df[~day_df.index.duplicated(keep='last')]\n",
        "        day_prices[sym] = day_df[\"Close\"]\n",
        "        all_times.update(day_df.index)\n",
        "\n",
        "    all_times = sorted(all_times)\n",
        "\n",
        "    sim_df = pd.DataFrame(index=all_times)\n",
        "    for sym in signals:\n",
        "        sym_prices = day_prices.get(sym)\n",
        "        if sym_prices is None:\n",
        "            sim_df[sym] = entries[sym]\n",
        "            continue\n",
        "        sym_prices_reindexed = sym_prices.reindex(all_times).ffill().bfill()\n",
        "        sim_df[sym] = sym_prices_reindexed\n",
        "\n",
        "    exit_times = {sym: None for sym in signals}\n",
        "    exit_prices = {sym: None for sym in signals}\n",
        "    exit_reasons = {sym: END_TIME for sym in signals}\n",
        "\n",
        "    for t in sim_df.index:\n",
        "        if t < SL_ACTIVATION_TIME:\n",
        "            continue\n",
        "\n",
        "        current_rois = {}\n",
        "        portfolio_pnl_decimal = 0.0\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if side == \"LONG\":\n",
        "                current_roi = (cur_price - entries[sym]) / entries[sym] * 100\n",
        "            else:\n",
        "                current_roi = (entries[sym] - cur_price) / entries[sym] * 100\n",
        "            current_rois[sym] = current_roi\n",
        "            portfolio_pnl_decimal += weight * (current_roi / 100)\n",
        "\n",
        "        if portfolio_pnl_decimal >= PORTFOLIO_TARGET_PCT:\n",
        "            for sym in open_trades:\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = sim_df.at[t, sym]\n",
        "                exit_reasons[sym] = f\"PORTFOLIO_TARGET_{t}\"\n",
        "            continue\n",
        "\n",
        "        if portfolio_pnl_decimal <= PORTFOLIO_SL_PCT:\n",
        "            for sym in open_trades:\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = sim_df.at[t, sym]\n",
        "                exit_reasons[sym] = f\"PORTFOLIO_SL_{t}\"\n",
        "            continue\n",
        "\n",
        "        # Individual SL check\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if (side == \"LONG\" and cur_price <= indiv_sls[sym]) or (side == \"SHORT\" and cur_price >= indiv_sls[sym]):\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = cur_price\n",
        "                exit_reasons[sym] = f\"INDIV_SL_{t}\"\n",
        "\n",
        "    final_time = END_TIME if END_TIME in sim_df.index else (all_times[-1] if all_times else END_TIME)\n",
        "    for sym in signals:\n",
        "        if exit_times[sym] is None:\n",
        "            exit_times[sym] = final_time\n",
        "            try:\n",
        "                exit_prices[sym] = sim_df.at[final_time, sym]\n",
        "            except Exception:\n",
        "                exit_prices[sym] = entries[sym]\n",
        "            exit_reasons[sym] = f\"END_TIME_{final_time}\"\n",
        "\n",
        "    # Get signal_date for this sym (assuming one per sym per entry_date)\n",
        "    signal_date_for_sym = day_group[day_group[\"SYMBOL\"] == sym][\"SIGNAL_DATE\"].iloc[0] if not day_group.empty else entry_date\n",
        "\n",
        "    # Compute day results\n",
        "    day_portfolio_return = 0.0\n",
        "    for sym, side in signals.items():\n",
        "        exit_price = exit_prices[sym]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entries[sym], 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entries[sym] - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        day_portfolio_return += weight * roi_trade\n",
        "\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date_for_sym,  # SIGNAL_DATE\n",
        "            entry_date,           # TRADE_DATE\n",
        "            side,\n",
        "            entries[sym],\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reasons[sym],\n",
        "            round(day_portfolio_return, 2),\n",
        "            None,  # cumulative placeholder\n",
        "        ])\n",
        "\n",
        "    cumulative_portfolio_return += day_portfolio_return\n",
        "\n",
        "    for i in range(len(output_trades) - num_signals, len(output_trades)):\n",
        "        output_trades[i][10] = round(cumulative_portfolio_return, 2)\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "Kb51ZKNYJs9t",
        "outputId": "4ae2ddbf-374c-48b9-e5b2-d60c51c39e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 523 cash files...\n",
            "‚úÖ Processed 50/523 symbols\n",
            "‚úÖ Processed 100/523 symbols\n",
            "‚úÖ Processed 150/523 symbols\n",
            "‚úÖ Processed 200/523 symbols\n",
            "‚úÖ Processed 250/523 symbols\n",
            "‚úÖ Processed 300/523 symbols\n",
            "‚úÖ Processed 350/523 symbols\n",
            "‚úÖ Processed 400/523 symbols\n",
            "‚úÖ Processed 450/523 symbols\n",
            "‚úÖ Processed 500/523 symbols\n",
            "‚úÖ Loaded 523 symbols with required times and precomputed indicators\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 302 potential trade dates from symbol data\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4153452495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mhour_lows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_low\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstart_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 h_high = df_sym.filter(\n\u001b[0m\u001b[1;32m    298\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TradeDate\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprev_day_entry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TradeTime\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mstart_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/dataframe/frame.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, *predicates, **constraints)\u001b[0m\n\u001b[1;32m   5099\u001b[0m         \u001b[0;31m‚îî\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚î¥\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚î¥\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îÄ\u001b[0m\u001b[0;31m‚îò\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5100\u001b[0m         \"\"\"\n\u001b[0;32m-> 5101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5103\u001b[0m     def remove(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"engine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"old-streaming\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/frame.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, _check_order, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0;31m# Only for testing purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_opt_callback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================\n",
        "# üîπ CONFIG (modified per your instructions)\n",
        "# ==========================\n",
        "INDIVIDUAL_SL_PCT = 0.005      # 0.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"10:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"10:15\"   # SL activation time\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "\n",
        "# Hourly time-points\n",
        "HOURLY_TIMES = [\"09:15\", \"10:15\", \"11:15\", \"12:15\", \"13:15\", \"14:15\", \"15:15\"]\n",
        "\n",
        "# Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper to parse datetime strings used across the script\n",
        "# ----------------------------\n",
        "def parse_ts_to_dt(ts_str):\n",
        "    try:\n",
        "        return datetime.strptime(ts_str[:19], \"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Function to read each CSV and return symbol + polars DataFrame (unchanged)\n",
        "# ----------------------------\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN LOAD + PRECOMPUTE (revised: only daily, add daily High/Low series)\n",
        "# ----------------------------\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}   # holds 15:29 and START_TIME closes keyed by TradeDate\n",
        "symbol_hourly_series = {}\n",
        "# <<< PERFORMANCE: caches for precomputed indicators per symbol >>>\n",
        "symbol_daily_20sma = {}\n",
        "symbol_prev5day_high = {}\n",
        "symbol_prev5day_low = {}\n",
        "symbol_daily_high = {}  # new: daily High series\n",
        "symbol_daily_low = {}   # new: daily Low series\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for daily calculations)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # ensure TradeDate is datetime.date\n",
        "        pdf[\"TradeDate\"] = pd.to_datetime(pdf[\"TradeDate\"]).dt.date\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    # <<< NEW: Compute daily High and Low series >>>\n",
        "    daily_ohlc = df.group_by(\"TradeDate\").agg([\n",
        "        pl.col(\"High\").max().alias(\"DailyHigh\"),\n",
        "        pl.col(\"Low\").min().alias(\"DailyLow\")\n",
        "    ]).to_pandas()\n",
        "    if not daily_ohlc.empty:\n",
        "        daily_ohlc[\"TradeDate\"] = pd.to_datetime(daily_ohlc[\"TradeDate\"]).dt.date\n",
        "        symbol_daily_high[symbol] = daily_ohlc.set_index(\"TradeDate\")[\"DailyHigh\"].sort_index()\n",
        "        symbol_daily_low[symbol] = daily_ohlc.set_index(\"TradeDate\")[\"DailyLow\"].sort_index()\n",
        "\n",
        "    # <<< PERFORMANCE: precompute daily indicators (vectorized, using actual High/Low) >>>\n",
        "    # compute daily series from 15:29 closes (if available)\n",
        "    if symbol in symbol_close_start_end:\n",
        "        close_series = symbol_close_start_end[symbol]['close_1529']\n",
        "        if isinstance(close_series, pd.Series) and not close_series.empty:\n",
        "            # ensure index is datetime.date objects already (we set above)\n",
        "            # daily 20 SMA\n",
        "            daily_20 = close_series.rolling(window=20, min_periods=1).mean()\n",
        "            symbol_daily_20sma[symbol] = daily_20\n",
        "\n",
        "            # NEW: prev-5-day high/low using actual daily High/Low: shift(1) then rolling on previous 5\n",
        "            daily_high = symbol_daily_high.get(symbol)\n",
        "            daily_low = symbol_daily_low.get(symbol)\n",
        "            if daily_high is not None and not daily_high.empty:\n",
        "                # align indices if needed\n",
        "                common_idx = close_series.index.intersection(daily_high.index)\n",
        "                if len(common_idx) > 0:\n",
        "                    daily_high_aligned = daily_high.loc[common_idx]\n",
        "                    prev5_high = daily_high_aligned.shift(1).rolling(window=5, min_periods=1).max()\n",
        "                    symbol_prev5day_high[symbol] = prev5_high.reindex(close_series.index).ffill().bfill()\n",
        "            if daily_low is not None and not daily_low.empty:\n",
        "                common_idx = close_series.index.intersection(daily_low.index)\n",
        "                if len(common_idx) > 0:\n",
        "                    daily_low_aligned = daily_low.loc[common_idx]\n",
        "                    prev5_low = daily_low_aligned.shift(1).rolling(window=5, min_periods=1).min()\n",
        "                    symbol_prev5day_low[symbol] = prev5_low.reindex(close_series.index).ffill().bfill()\n",
        "\n",
        "    # Precompute hourly closes for SMA\n",
        "    hr_df = df.filter(pl.col(\"TradeTime\").is_in(HOURLY_TIMES)).select([\"dt\", \"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    if not hr_df.empty:\n",
        "        if 'dt' in hr_df and hr_df['dt'].dtype == 'object':\n",
        "            hr_df['dt_ts'] = hr_df['dt'].apply(parse_ts_to_dt)\n",
        "        else:\n",
        "            hr_df['dt_ts'] = hr_df['dt']\n",
        "        hr_df['TradeDate'] = pd.to_datetime(hr_df['TradeDate']).dt.date\n",
        "        hr_df['dt_index'] = hr_df.apply(lambda r: pd.Timestamp(str(r['TradeDate']) + ' ' + r['TradeTime']), axis=1)\n",
        "        hr_df = hr_df.sort_values('dt_index')\n",
        "        symbol_hourly_series[symbol] = hr_df.set_index('dt_index')['Close']\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times and precomputed indicators\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time START_TIME) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty_pdf[\"TradeDate\"] = pd.to_datetime(nifty_pdf[\"TradeDate\"]).dt.date\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the next trading day\n",
        "def get_next_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    next_dates = [d for d in all_dates if pd.Timestamp(d).date() > trade_date]\n",
        "    if not next_dates:\n",
        "        return None\n",
        "    return min(next_dates)\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d).date() < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date with revised daily-only indicators -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    entry_date = get_next_trading_day(signal_date, unique_trade_dates)\n",
        "    if entry_date is None:\n",
        "        continue\n",
        "\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)  # for crossover in bearish\n",
        "\n",
        "    # Compute NIFTY500 ROI for entry (overnight from signal close to entry open)\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None:\n",
        "        try:\n",
        "            nifty_signal_close = float(nifty500_close_1529.loc[signal_date])\n",
        "            nifty_entry_open = float(nifty500_open_start.loc[entry_date])\n",
        "            if nifty_entry_open != 0:\n",
        "                nifty_roi_for_date = ((nifty_entry_open - nifty_signal_close) / nifty_entry_open) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            signal_close = None\n",
        "            entry_open = None\n",
        "            try:\n",
        "                signal_close = float(d[\"close_1529\"].loc[signal_date])\n",
        "            except Exception:\n",
        "                signal_close = None\n",
        "            try:\n",
        "                entry_open = float(d[\"open_start\"].loc[entry_date])\n",
        "            except Exception:\n",
        "                entry_open = None\n",
        "\n",
        "            if signal_close is None or entry_open is None or entry_open == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((entry_open - signal_close) / entry_open) * 100.0\n",
        "\n",
        "            # fetch precomputed daily indicators for signal_date\n",
        "            daily_20s = symbol_daily_20sma.get(sym)\n",
        "            today_daily_20sma = None\n",
        "            prev_daily_20sma = None\n",
        "            prev_daily_close = None\n",
        "            if daily_20s is not None and signal_date in daily_20s.index:\n",
        "                today_daily_20sma = float(daily_20s.loc[signal_date])\n",
        "            if prev_trade_date is not None and daily_20s is not None and prev_trade_date in daily_20s.index:\n",
        "                prev_daily_20sma = float(daily_20s.loc[prev_trade_date])\n",
        "            # prev daily close (for crossover):\n",
        "            try:\n",
        "                series_15_29 = d[\"close_1529\"]\n",
        "                if prev_trade_date is not None and prev_trade_date in series_15_29.index:\n",
        "                    prev_daily_close = float(series_15_29.loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_daily_close = None\n",
        "\n",
        "            prev_5day_high_val = None\n",
        "            prev_5day_low_val = None\n",
        "            prev5_high_series = symbol_prev5day_high.get(sym)\n",
        "            prev5_low_series = symbol_prev5day_low.get(sym)\n",
        "            if prev5_high_series is not None and signal_date in prev5_high_series.index:\n",
        "                prev_5day_high_val = float(prev5_high_series.loc[signal_date])\n",
        "            if prev5_low_series is not None and signal_date in prev5_low_series.index:\n",
        "                prev_5day_low_val = float(prev5_low_series.loc[signal_date])\n",
        "\n",
        "            # Compute hourly conditions\n",
        "            entry_dt = pd.Timestamp(str(entry_date) + \" \" + START_TIME)\n",
        "            hr_series = symbol_hourly_series.get(sym)\n",
        "            if hr_series is None or entry_dt not in hr_series.index:\n",
        "                continue\n",
        "            sma_series = hr_series.rolling(window=20, min_periods=1).mean()\n",
        "            sma_entry = float(sma_series.loc[entry_dt])\n",
        "            prev_candidates = hr_series[hr_series.index < entry_dt]\n",
        "            if prev_candidates.empty:\n",
        "                continue\n",
        "            prev_dt = prev_candidates.index[-1]\n",
        "            prev_close = float(hr_series.loc[prev_dt])\n",
        "            sma_prev = float(sma_series.loc[prev_dt])\n",
        "            cross_above = (entry_open > sma_entry) and (prev_close <= sma_prev)\n",
        "            cross_below = (entry_open < sma_entry) and (prev_close >= sma_prev)\n",
        "\n",
        "            # Compute prev 5 hour high/low\n",
        "            prev_day_entry = get_prev_trading_day(entry_date, unique_trade_dates)\n",
        "            if prev_day_entry is None:\n",
        "                continue\n",
        "            df_sym = symbol_full_data[sym]\n",
        "            # current partial\n",
        "            partial_high = df_sym.filter(\n",
        "                (pl.col(\"TradeDate\") == entry_date) & (pl.col(\"TradeTime\") < START_TIME)\n",
        "            ).select(pl.max(\"High\")).item()\n",
        "            partial_low = df_sym.filter(\n",
        "                (pl.col(\"TradeDate\") == entry_date) & (pl.col(\"TradeTime\") < START_TIME)\n",
        "            ).select(pl.min(\"Low\")).item()\n",
        "            # prev day periods\n",
        "            periods = [\n",
        "                (\"11:15\", \"12:15\"),\n",
        "                (\"12:15\", \"13:15\"),\n",
        "                (\"13:15\", \"14:15\"),\n",
        "                (\"14:15\", \"15:30\")\n",
        "            ]\n",
        "            hour_highs = []\n",
        "            hour_lows = []\n",
        "            if partial_high is not None:\n",
        "                hour_highs.append(partial_high)\n",
        "            if partial_low is not None:\n",
        "                hour_lows.append(partial_low)\n",
        "            for start_t, end_t in periods:\n",
        "                h_high = df_sym.filter(\n",
        "                    (pl.col(\"TradeDate\") == prev_day_entry) &\n",
        "                    (pl.col(\"TradeTime\") >= start_t) &\n",
        "                    (pl.col(\"TradeTime\") <= end_t)\n",
        "                ).select(pl.max(\"High\")).item()\n",
        "                if h_high is not None:\n",
        "                    hour_highs.append(h_high)\n",
        "                h_low = df_sym.filter(\n",
        "                    (pl.col(\"TradeDate\") == prev_day_entry) &\n",
        "                    (pl.col(\"TradeTime\") >= start_t) &\n",
        "                    (pl.col(\"TradeTime\") <= end_t)\n",
        "                ).select(pl.min(\"Low\")).item()\n",
        "                if h_low is not None:\n",
        "                    hour_lows.append(h_low)\n",
        "            if len(hour_highs) != 5 or len(hour_lows) != 5:\n",
        "                continue\n",
        "            prev5_hour_high = max(hour_highs)\n",
        "            prev5_hour_low = min(hour_lows)\n",
        "\n",
        "            # Evaluate bullish/bearish (revised: daily only, no hourly/VWAP, actual High/Low for prev5)\n",
        "            bullish = False\n",
        "            try:\n",
        "                cond1 = (prev_5day_high_val is not None) and (signal_close > prev_5day_high_val)\n",
        "                cond2 = (today_daily_20sma is not None) and (signal_close > today_daily_20sma)\n",
        "                cond_hour_sma = cross_above\n",
        "                cond_hour_high = (entry_open > prev5_hour_high)\n",
        "                bullish = all([cond1, cond2, cond_hour_sma, cond_hour_high])\n",
        "            except Exception:\n",
        "                bullish = False\n",
        "\n",
        "            bearish = False\n",
        "            try:\n",
        "                bcond1 = (prev_5day_low_val is not None) and (signal_close < prev_5day_low_val)\n",
        "                bcond2 = (today_daily_20sma is not None) and (signal_close < today_daily_20sma)\n",
        "                bcond3 = (prev_daily_close is not None) and (prev_daily_20sma is not None) and (prev_daily_close >= prev_daily_20sma)\n",
        "                bcond_hour_sma = cross_below\n",
        "                bcond_hour_low = (entry_open < prev5_hour_low)\n",
        "                bearish = all([bcond1, bcond2, bcond3, bcond_hour_sma, bcond_hour_low])\n",
        "            except Exception:\n",
        "                bearish = False\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                signal_date,\n",
        "                entry_date,\n",
        "                sym,\n",
        "                prev_daily_close,  # previous close before signal\n",
        "                signal_close,      # signal close (used as \"prev_close\" for entry)\n",
        "                entry_open,        # entry open\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date,\n",
        "                today_daily_20sma,\n",
        "                prev_5day_high_val,\n",
        "                prev_5day_low_val,\n",
        "                bullish,\n",
        "                bearish\n",
        "            ])\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with revised columns (no hourly/VWAP)\n",
        "breakdown_df = pd.DataFrame(all_breakdowns, columns=[\n",
        "    \"SIGNAL_DATE\", \"ENTRY_DATE\", \"SYMBOL\", \"PREV_CLOSE_BEFORE_SIGNAL\", \"SIGNAL_CLOSE\", \"ENTRY_OPEN\", \"ROI_%\", \"NIFTY500_ROI_%\",\n",
        "    \"DAILY_20SMA\", \"PREV_5DAY_HIGH\", \"PREV_5DAY_LOW\", \"BULLISH_SETUP\", \"BEARISH_SETUP\"\n",
        "])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df[\"DAILY_20SMA\"] = pd.to_numeric(breakdown_df[\"DAILY_20SMA\"], errors='coerce').round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, revised daily setups)\")\n",
        "\n",
        "# ----- Ranking logic: For each ENTRY_DATE pick up to 2 LONG and 2 SHORT (no ranking, just head(2)) -----\n",
        "ranked_signals = []\n",
        "\n",
        "for entry_date, daily_df in breakdown_df.groupby(\"ENTRY_DATE\"):\n",
        "    # Compute NIFTY ROI for the entry day if present\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    if len(nifty_vals) == 0:\n",
        "        nifty_roi_for_date = None\n",
        "    else:\n",
        "        nifty_roi_for_date = float(nifty_vals[0])\n",
        "\n",
        "    bullish_candidates = daily_df[daily_df[\"BULLISH_SETUP\"] == True].copy()\n",
        "    bearish_candidates = daily_df[daily_df[\"BEARISH_SETUP\"] == True].copy()\n",
        "\n",
        "    all_long = bullish_candidates.copy()\n",
        "    if not all_long.empty:\n",
        "        all_long[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "    all_short = bearish_candidates.copy()\n",
        "    if not all_short.empty:\n",
        "        all_short[\"SIDE\"] = \"SHORT\"\n",
        "\n",
        "    day_selected = pd.concat([all_long, all_short], ignore_index=True) if (not all_long.empty or not all_short.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After selection ‚Üí {len(ranked_df)} signals selected for trading (all per entry date)\")\n",
        "\n",
        "# ----- Backtest/execution loop (revised: entry on ENTRY_DATE, prev_close is SIGNAL_CLOSE) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for entry_date, day_group in ranked_df.groupby(\"ENTRY_DATE\"):\n",
        "    # For each entry day, collect the symbols and sides\n",
        "    signals = day_group.set_index(\"SYMBOL\")[\"SIDE\"].to_dict()\n",
        "    num_signals = len(signals)\n",
        "    if num_signals == 0:\n",
        "        continue\n",
        "    weight = 1.0 / num_signals  # equal weight\n",
        "\n",
        "    # Get entry prices (ENTRY_OPEN), indiv SL prices\n",
        "    entries = {}\n",
        "    indiv_sls = {}\n",
        "    for sym, side in list(signals.items()):\n",
        "        entry_price = symbol_close_start_end.get(sym, {}).get(\"open_start\", {}).get(entry_date, None)\n",
        "        if entry_price is None or entry_price == 0:\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entries[sym] = entry_price\n",
        "        if side == \"LONG\":\n",
        "            indiv_sls[sym] = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "        else:\n",
        "            indiv_sls[sym] = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    if not signals:\n",
        "        continue\n",
        "\n",
        "    num_signals = len(signals)\n",
        "    weight = 1.0 / num_signals\n",
        "\n",
        "    # Collect day prices for symbols on entry_date\n",
        "    all_times = set()\n",
        "    day_prices = {}\n",
        "    for sym in signals:\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == entry_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_df = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)].copy()\n",
        "        if day_df.empty:\n",
        "            continue\n",
        "        day_df = day_df.set_index(\"TradeTime\")\n",
        "        day_df = day_df[~day_df.index.duplicated(keep='last')]\n",
        "        day_prices[sym] = day_df[\"Close\"]\n",
        "        all_times.update(day_df.index)\n",
        "\n",
        "    all_times = sorted(all_times)\n",
        "\n",
        "    sim_df = pd.DataFrame(index=all_times)\n",
        "    for sym in signals:\n",
        "        sym_prices = day_prices.get(sym)\n",
        "        if sym_prices is None:\n",
        "            sim_df[sym] = entries[sym]\n",
        "            continue\n",
        "        sym_prices_reindexed = sym_prices.reindex(all_times).ffill().bfill()\n",
        "        sim_df[sym] = sym_prices_reindexed\n",
        "\n",
        "    exit_times = {sym: None for sym in signals}\n",
        "    exit_prices = {sym: None for sym in signals}\n",
        "    exit_reasons = {sym: END_TIME for sym in signals}\n",
        "\n",
        "    for t in sim_df.index:\n",
        "        if t < SL_ACTIVATION_TIME:\n",
        "            continue\n",
        "\n",
        "        current_rois = {}\n",
        "        portfolio_pnl_decimal = 0.0\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if side == \"LONG\":\n",
        "                current_roi = (cur_price - entries[sym]) / entries[sym] * 100\n",
        "            else:\n",
        "                current_roi = (entries[sym] - cur_price) / entries[sym] * 100\n",
        "            current_rois[sym] = current_roi\n",
        "            portfolio_pnl_decimal += weight * (current_roi / 100)\n",
        "\n",
        "        # Individual SL check\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if (side == \"LONG\" and cur_price <= indiv_sls[sym]) or (side == \"SHORT\" and cur_price >= indiv_sls[sym]):\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = cur_price\n",
        "                exit_reasons[sym] = f\"INDIV_SL_{t}\"\n",
        "\n",
        "    final_time = END_TIME if END_TIME in sim_df.index else (all_times[-1] if all_times else END_TIME)\n",
        "    for sym in signals:\n",
        "        if exit_times[sym] is None:\n",
        "            exit_times[sym] = final_time\n",
        "            try:\n",
        "                exit_prices[sym] = sim_df.at[final_time, sym]\n",
        "            except Exception:\n",
        "                exit_prices[sym] = entries[sym]\n",
        "            exit_reasons[sym] = f\"END_TIME_{final_time}\"\n",
        "\n",
        "    # Compute day results\n",
        "    day_portfolio_return = 0.0\n",
        "    trade_results = []\n",
        "    for sym, side in signals.items():\n",
        "        exit_price = exit_prices[sym]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entries[sym], 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entries[sym] - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        trade_results.append((sym, side, trade_pnl, roi_trade, exit_reasons[sym], exit_price))\n",
        "        day_portfolio_return += weight * roi_trade\n",
        "\n",
        "    cumulative_portfolio_return += day_portfolio_return\n",
        "    day_return = round(day_portfolio_return, 2)\n",
        "    cum_return = round(cumulative_portfolio_return, 2)\n",
        "\n",
        "    for res in trade_results:\n",
        "        sym, side, trade_pnl, roi_trade, exit_reason, exit_price = res\n",
        "        signal_date = day_group[day_group[\"SYMBOL\"] == sym][\"SIGNAL_DATE\"].iloc[0]\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,  # SIGNAL_DATE\n",
        "            entry_date,   # TRADE_DATE\n",
        "            side,\n",
        "            entries[sym],\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reason,\n",
        "            day_return,\n",
        "            cum_return\n",
        "        ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "UDVUKp1zT9I-",
        "outputId": "b6a72d1a-9a86-4361-99eb-b8290d164262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 520 cash files...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2494971098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_full_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0msymbol_full_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2494971098.py\u001b[0m in \u001b[0;36mload_full_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0msymbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     df = pl.read_csv(\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtry_parse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/io/csv/functions.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         ) as data:\n\u001b[0;32m--> 539\u001b[0;31m             df = _read_csv_impl(\n\u001b[0m\u001b[1;32m    540\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mhas_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/io/csv/functions.py\u001b[0m in \u001b[0;36m_read_csv_impl\u001b[0;34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0mprojection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_columns_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m     pydf = PyDataFrame.read_csv(\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0minfer_schema_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================\n",
        "# üîπ CONFIG (modified per your instructions)\n",
        "# ==========================\n",
        "INDIVIDUAL_SL_PCT = 0.005      # 0.5% individual SL\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"10:15\"           # Trade entry time\n",
        "SL_ACTIVATION_TIME = \"10:15\"   # SL activation time\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "\n",
        "# Hourly time-points\n",
        "HOURLY_TIMES = [\"09:15\", \"10:15\", \"11:15\", \"12:15\", \"13:15\", \"14:15\", \"15:15\"]\n",
        "\n",
        "# Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper to parse datetime strings used across the script\n",
        "# ----------------------------\n",
        "def parse_ts_to_dt(ts_str):\n",
        "    try:\n",
        "        return datetime.strptime(ts_str[:19], \"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Function to read each CSV and return symbol + polars DataFrame (unchanged)\n",
        "# ----------------------------\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN LOAD + PRECOMPUTE (revised: only daily, add daily High/Low series)\n",
        "# ----------------------------\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}   # holds 15:29 and START_TIME closes keyed by TradeDate\n",
        "symbol_hourly_series = {}\n",
        "# <<< PERFORMANCE: caches for precomputed indicators per symbol >>>\n",
        "symbol_daily_20sma = {}\n",
        "symbol_prev5day_high = {}\n",
        "symbol_prev5day_low = {}\n",
        "symbol_daily_high = {}  # new: daily High series\n",
        "symbol_daily_low = {}   # new: daily Low series\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for daily calculations)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # ensure TradeDate is datetime.date\n",
        "        pdf[\"TradeDate\"] = pd.to_datetime(pdf[\"TradeDate\"]).dt.date\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    # <<< NEW: Compute daily High and Low series >>>\n",
        "    daily_ohlc = df.group_by(\"TradeDate\").agg([\n",
        "        pl.col(\"High\").max().alias(\"DailyHigh\"),\n",
        "        pl.col(\"Low\").min().alias(\"DailyLow\")\n",
        "    ]).to_pandas()\n",
        "    if not daily_ohlc.empty:\n",
        "        daily_ohlc[\"TradeDate\"] = pd.to_datetime(daily_ohlc[\"TradeDate\"]).dt.date\n",
        "        symbol_daily_high[symbol] = daily_ohlc.set_index(\"TradeDate\")[\"DailyHigh\"].sort_index()\n",
        "        symbol_daily_low[symbol] = daily_ohlc.set_index(\"TradeDate\")[\"DailyLow\"].sort_index()\n",
        "\n",
        "    # <<< PERFORMANCE: precompute daily indicators (vectorized, using actual High/Low) >>>\n",
        "    # compute daily series from 15:29 closes (if available)\n",
        "    if symbol in symbol_close_start_end:\n",
        "        close_series = symbol_close_start_end[symbol]['close_1529']\n",
        "        if isinstance(close_series, pd.Series) and not close_series.empty:\n",
        "            # ensure index is datetime.date objects already (we set above)\n",
        "            # daily 20 SMA\n",
        "            daily_20 = close_series.rolling(window=20, min_periods=1).mean()\n",
        "            symbol_daily_20sma[symbol] = daily_20\n",
        "\n",
        "            # NEW: prev-5-day high/low using actual daily High/Low: shift(1) then rolling on previous 5\n",
        "            daily_high = symbol_daily_high.get(symbol)\n",
        "            daily_low = symbol_daily_low.get(symbol)\n",
        "            if daily_high is not None and not daily_high.empty:\n",
        "                # align indices if needed\n",
        "                common_idx = close_series.index.intersection(daily_high.index)\n",
        "                if len(common_idx) > 0:\n",
        "                    daily_high_aligned = daily_high.loc[common_idx]\n",
        "                    prev5_high = daily_high_aligned.shift(1).rolling(window=5, min_periods=1).max()\n",
        "                    symbol_prev5day_high[symbol] = prev5_high.reindex(close_series.index).ffill().bfill()\n",
        "            if daily_low is not None and not daily_low.empty:\n",
        "                common_idx = close_series.index.intersection(daily_low.index)\n",
        "                if len(common_idx) > 0:\n",
        "                    daily_low_aligned = daily_low.loc[common_idx]\n",
        "                    prev5_low = daily_low_aligned.shift(1).rolling(window=5, min_periods=1).min()\n",
        "                    symbol_prev5day_low[symbol] = prev5_low.reindex(close_series.index).ffill().bfill()\n",
        "\n",
        "    # Precompute hourly closes for SMA\n",
        "    hr_df = df.filter(pl.col(\"TradeTime\").is_in(HOURLY_TIMES)).select([\"dt\", \"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "    if not hr_df.empty:\n",
        "        if 'dt' in hr_df and hr_df['dt'].dtype == 'object':\n",
        "            hr_df['dt_ts'] = hr_df['dt'].apply(parse_ts_to_dt)\n",
        "        else:\n",
        "            hr_df['dt_ts'] = hr_df['dt']\n",
        "        hr_df['TradeDate'] = pd.to_datetime(hr_df['TradeDate']).dt.date\n",
        "        hr_df['dt_index'] = hr_df.apply(lambda r: pd.Timestamp(str(r['TradeDate']) + ' ' + r['TradeTime']), axis=1)\n",
        "        hr_df = hr_df.sort_values('dt_index')\n",
        "        symbol_hourly_series[symbol] = hr_df.set_index('dt_index')['Close']\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times and precomputed indicators\")\n",
        "\n",
        "# --- Load NIFTY500 series (prev close 15:29 and start time START_TIME) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_open_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty_pdf[\"TradeDate\"] = pd.to_datetime(nifty_pdf[\"TradeDate\"]).dt.date\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_open_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the next trading day\n",
        "def get_next_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    next_dates = [d for d in all_dates if pd.Timestamp(d).date() > trade_date]\n",
        "    if not next_dates:\n",
        "        return None\n",
        "    return min(next_dates)\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d).date() < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date with revised daily-only indicators -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    entry_date = get_next_trading_day(signal_date, unique_trade_dates)\n",
        "    if entry_date is None:\n",
        "        continue\n",
        "\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)  # for crossover in bearish\n",
        "\n",
        "    # Compute NIFTY500 ROI for entry (overnight from signal close to entry open)\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_open_start is not None:\n",
        "        try:\n",
        "            nifty_signal_close = float(nifty500_close_1529.loc[signal_date])\n",
        "            nifty_entry_open = float(nifty500_open_start.loc[entry_date])\n",
        "            if nifty_entry_open != 0:\n",
        "                nifty_roi_for_date = ((nifty_entry_open - nifty_signal_close) / nifty_entry_open) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            signal_close = None\n",
        "            entry_open = None\n",
        "            try:\n",
        "                signal_close = float(d[\"close_1529\"].loc[signal_date])\n",
        "            except Exception:\n",
        "                signal_close = None\n",
        "            try:\n",
        "                entry_open = float(d[\"open_start\"].loc[entry_date])\n",
        "            except Exception:\n",
        "                entry_open = None\n",
        "\n",
        "            if signal_close is None or entry_open is None or entry_open == 0:\n",
        "                continue\n",
        "\n",
        "            roi_pct = ((entry_open - signal_close) / entry_open) * 100.0\n",
        "\n",
        "            # fetch precomputed daily indicators for signal_date\n",
        "            daily_20s = symbol_daily_20sma.get(sym)\n",
        "            today_daily_20sma = None\n",
        "            prev_daily_20sma = None\n",
        "            prev_daily_close = None\n",
        "            if daily_20s is not None and signal_date in daily_20s.index:\n",
        "                today_daily_20sma = float(daily_20s.loc[signal_date])\n",
        "            if prev_trade_date is not None and daily_20s is not None and prev_trade_date in daily_20s.index:\n",
        "                prev_daily_20sma = float(daily_20s.loc[prev_trade_date])\n",
        "            # prev daily close (for crossover):\n",
        "            try:\n",
        "                series_15_29 = d[\"close_1529\"]\n",
        "                if prev_trade_date is not None and prev_trade_date in series_15_29.index:\n",
        "                    prev_daily_close = float(series_15_29.loc[prev_trade_date])\n",
        "            except Exception:\n",
        "                prev_daily_close = None\n",
        "\n",
        "            prev_5day_high_val = None\n",
        "            prev_5day_low_val = None\n",
        "            prev5_high_series = symbol_prev5day_high.get(sym)\n",
        "            prev5_low_series = symbol_prev5day_low.get(sym)\n",
        "            if prev5_high_series is not None and signal_date in prev5_high_series.index:\n",
        "                prev_5day_high_val = float(prev5_high_series.loc[signal_date])\n",
        "            if prev5_low_series is not None and signal_date in prev5_low_series.index:\n",
        "                prev_5day_low_val = float(prev5_low_series.loc[signal_date])\n",
        "\n",
        "            # Compute hourly conditions\n",
        "            entry_dt = pd.Timestamp(str(entry_date) + \" \" + START_TIME)\n",
        "            hr_series = symbol_hourly_series.get(sym)\n",
        "            if hr_series is None or entry_dt not in hr_series.index:\n",
        "                continue\n",
        "            sma_series = hr_series.rolling(window=20, min_periods=1).mean()\n",
        "            sma_entry = float(sma_series.loc[entry_dt])\n",
        "            prev_candidates = hr_series[hr_series.index < entry_dt]\n",
        "            if prev_candidates.empty:\n",
        "                continue\n",
        "            prev_dt = prev_candidates.index[-1]\n",
        "            prev_close = float(hr_series.loc[prev_dt])\n",
        "            sma_prev = float(sma_series.loc[prev_dt])\n",
        "            # Revised: no crossover, just position relative to SMA\n",
        "            above_sma = entry_open > sma_entry\n",
        "            below_sma = entry_open < sma_entry\n",
        "\n",
        "            # Compute prev 5 hour high/low (revised: 5 full hourly periods from prev day, no partial)\n",
        "            prev_day_entry = get_prev_trading_day(entry_date, unique_trade_dates)\n",
        "            if prev_day_entry is None:\n",
        "                continue\n",
        "            df_sym = symbol_full_data[sym]\n",
        "            # No partial high/low\n",
        "            # hour_highs = []\n",
        "            # hour_lows = []\n",
        "            # if partial_high is not None:\n",
        "            #     hour_highs.append(partial_high)\n",
        "            # if partial_low is not None:\n",
        "            #     hour_lows.append(partial_low)\n",
        "            periods = [\n",
        "                (\"10:15\", \"11:15\"),\n",
        "                (\"11:15\", \"12:15\"),\n",
        "                (\"12:15\", \"13:15\"),\n",
        "                (\"13:15\", \"14:15\"),\n",
        "                (\"14:15\", \"15:30\")\n",
        "            ]\n",
        "            hour_highs = []\n",
        "            hour_lows = []\n",
        "            for start_t, end_t in periods:\n",
        "                h_high = df_sym.filter(\n",
        "                    (pl.col(\"TradeDate\") == prev_day_entry) &\n",
        "                    (pl.col(\"TradeTime\") >= start_t) &\n",
        "                    (pl.col(\"TradeTime\") <= end_t)\n",
        "                ).select(pl.max(\"High\")).item()\n",
        "                if h_high is not None:\n",
        "                    hour_highs.append(h_high)\n",
        "                h_low = df_sym.filter(\n",
        "                    (pl.col(\"TradeDate\") == prev_day_entry) &\n",
        "                    (pl.col(\"TradeTime\") >= start_t) &\n",
        "                    (pl.col(\"TradeTime\") <= end_t)\n",
        "                ).select(pl.min(\"Low\")).item()\n",
        "                if h_low is not None:\n",
        "                    hour_lows.append(h_low)\n",
        "            if len(hour_highs) != 5 or len(hour_lows) != 5:\n",
        "                continue\n",
        "            prev5_hour_high = max(hour_highs)\n",
        "            prev5_hour_low = min(hour_lows)\n",
        "\n",
        "            # Evaluate bullish/bearish (revised: daily only, no hourly/VWAP, actual High/Low for prev5; no crossovers)\n",
        "            bullish = False\n",
        "            try:\n",
        "                cond1 = (prev_5day_high_val is not None) and (signal_close > prev_5day_high_val)\n",
        "                cond2 = (today_daily_20sma is not None) and (signal_close > today_daily_20sma)\n",
        "                cond_hour_sma = above_sma\n",
        "                cond_hour_high = (entry_open > prev5_hour_high)\n",
        "                bullish = all([cond1, cond2, cond_hour_sma, cond_hour_high])\n",
        "            except Exception:\n",
        "                bullish = False\n",
        "\n",
        "            bearish = False\n",
        "            try:\n",
        "                bcond1 = (prev_5day_low_val is not None) and (signal_close < prev_5day_low_val)\n",
        "                bcond2 = (today_daily_20sma is not None) and (signal_close < today_daily_20sma)\n",
        "                # Removed bcond3 (prev close >= prev SMA)\n",
        "                bcond_hour_sma = below_sma\n",
        "                bcond_hour_low = (entry_open < prev5_hour_low)\n",
        "                bearish = all([bcond1, bcond2, bcond_hour_sma, bcond_hour_low])\n",
        "            except Exception:\n",
        "                bearish = False\n",
        "\n",
        "            all_breakdowns.append([\n",
        "                signal_date,\n",
        "                entry_date,\n",
        "                sym,\n",
        "                prev_daily_close,  # previous close before signal\n",
        "                signal_close,      # signal close (used as \"prev_close\" for entry)\n",
        "                entry_open,        # entry open\n",
        "                roi_pct,\n",
        "                nifty_roi_for_date,\n",
        "                today_daily_20sma,\n",
        "                prev_5day_high_val,\n",
        "                prev_5day_low_val,\n",
        "                bullish,\n",
        "                bearish\n",
        "            ])\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with revised columns (no hourly/VWAP)\n",
        "breakdown_df = pd.DataFrame(all_breakdowns, columns=[\n",
        "    \"SIGNAL_DATE\", \"ENTRY_DATE\", \"SYMBOL\", \"PREV_CLOSE_BEFORE_SIGNAL\", \"SIGNAL_CLOSE\", \"ENTRY_OPEN\", \"ROI_%\", \"NIFTY500_ROI_%\",\n",
        "    \"DAILY_20SMA\", \"PREV_5DAY_HIGH\", \"PREV_5DAY_LOW\", \"BULLISH_SETUP\", \"BEARISH_SETUP\"\n",
        "])\n",
        "breakdown_df[\"ROI_%\"] = breakdown_df[\"ROI_%\"].astype(float).round(6)\n",
        "breakdown_df[\"DAILY_20SMA\"] = pd.to_numeric(breakdown_df[\"DAILY_20SMA\"], errors='coerce').round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, revised daily setups)\")\n",
        "\n",
        "# ----- Ranking logic: For each ENTRY_DATE pick up to 2 LONG and 2 SHORT (no ranking, just head(2)) -----\n",
        "ranked_signals = []\n",
        "\n",
        "for entry_date, daily_df in breakdown_df.groupby(\"ENTRY_DATE\"):\n",
        "    # Compute NIFTY ROI for the entry day if present\n",
        "    nifty_vals = daily_df[\"NIFTY500_ROI_%\"].dropna().unique()\n",
        "    if len(nifty_vals) == 0:\n",
        "        nifty_roi_for_date = None\n",
        "    else:\n",
        "        nifty_roi_for_date = float(nifty_vals[0])\n",
        "\n",
        "    bullish_candidates = daily_df[daily_df[\"BULLISH_SETUP\"] == True].copy()\n",
        "    bearish_candidates = daily_df[daily_df[\"BEARISH_SETUP\"] == True].copy()\n",
        "\n",
        "    all_long = bullish_candidates.copy()\n",
        "    if not all_long.empty:\n",
        "        all_long[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "    all_short = bearish_candidates.copy()\n",
        "    if not all_short.empty:\n",
        "        all_short[\"SIDE\"] = \"SHORT\"\n",
        "\n",
        "    day_selected = pd.concat([all_long, all_short], ignore_index=True) if (not all_long.empty or not all_short.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After selection ‚Üí {len(ranked_df)} signals selected for trading (all per entry date)\")\n",
        "\n",
        "# ----- Backtest/execution loop (revised: entry on ENTRY_DATE, prev_close is SIGNAL_CLOSE) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for entry_date, day_group in ranked_df.groupby(\"ENTRY_DATE\"):\n",
        "    # For each entry day, collect the symbols and sides\n",
        "    signals = day_group.set_index(\"SYMBOL\")[\"SIDE\"].to_dict()\n",
        "    num_signals = len(signals)\n",
        "    if num_signals == 0:\n",
        "        continue\n",
        "    weight = 1.0 / num_signals  # equal weight\n",
        "\n",
        "    # Get entry prices (ENTRY_OPEN), indiv SL prices\n",
        "    entries = {}\n",
        "    indiv_sls = {}\n",
        "    for sym, side in list(signals.items()):\n",
        "        entry_price = symbol_close_start_end.get(sym, {}).get(\"open_start\", {}).get(entry_date, None)\n",
        "        if entry_price is None or entry_price == 0:\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entries[sym] = entry_price\n",
        "        if side == \"LONG\":\n",
        "            indiv_sls[sym] = entry_price * (1 - INDIVIDUAL_SL_PCT)\n",
        "        else:\n",
        "            indiv_sls[sym] = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "    if not signals:\n",
        "        continue\n",
        "\n",
        "    num_signals = len(signals)\n",
        "    weight = 1.0 / num_signals\n",
        "\n",
        "    # Collect day prices for symbols on entry_date\n",
        "    all_times = set()\n",
        "    day_prices = {}\n",
        "    for sym in signals:\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == entry_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_df = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)].copy()\n",
        "        if day_df.empty:\n",
        "            continue\n",
        "        day_df = day_df.set_index(\"TradeTime\")\n",
        "        day_df = day_df[~day_df.index.duplicated(keep='last')]\n",
        "        day_prices[sym] = day_df[\"Close\"]\n",
        "        all_times.update(day_df.index)\n",
        "\n",
        "    all_times = sorted(all_times)\n",
        "\n",
        "    sim_df = pd.DataFrame(index=all_times)\n",
        "    for sym in signals:\n",
        "        sym_prices = day_prices.get(sym)\n",
        "        if sym_prices is None:\n",
        "            sim_df[sym] = entries[sym]\n",
        "            continue\n",
        "        sym_prices_reindexed = sym_prices.reindex(all_times).ffill().bfill()\n",
        "        sim_df[sym] = sym_prices_reindexed\n",
        "\n",
        "    exit_times = {sym: None for sym in signals}\n",
        "    exit_prices = {sym: None for sym in signals}\n",
        "    exit_reasons = {sym: END_TIME for sym in signals}\n",
        "\n",
        "    for t in sim_df.index:\n",
        "        if t < SL_ACTIVATION_TIME:\n",
        "            continue\n",
        "\n",
        "        current_rois = {}\n",
        "        portfolio_pnl_decimal = 0.0\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if side == \"LONG\":\n",
        "                current_roi = (cur_price - entries[sym]) / entries[sym] * 100\n",
        "            else:\n",
        "                current_roi = (entries[sym] - cur_price) / entries[sym] * 100\n",
        "            current_rois[sym] = current_roi\n",
        "            portfolio_pnl_decimal += weight * (current_roi / 100)\n",
        "\n",
        "        # Individual SL check\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if (side == \"LONG\" and cur_price <= indiv_sls[sym]) or (side == \"SHORT\" and cur_price >= indiv_sls[sym]):\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = cur_price\n",
        "                exit_reasons[sym] = f\"INDIV_SL_{t}\"\n",
        "\n",
        "    final_time = END_TIME if END_TIME in sim_df.index else (all_times[-1] if all_times else END_TIME)\n",
        "    for sym in signals:\n",
        "        if exit_times[sym] is None:\n",
        "            exit_times[sym] = final_time\n",
        "            try:\n",
        "                exit_prices[sym] = sim_df.at[final_time, sym]\n",
        "            except Exception:\n",
        "                exit_prices[sym] = entries[sym]\n",
        "            exit_reasons[sym] = f\"END_TIME_{final_time}\"\n",
        "\n",
        "    # Compute day results\n",
        "    day_portfolio_return = 0.0\n",
        "    trade_results = []\n",
        "    for sym, side in signals.items():\n",
        "        exit_price = exit_prices[sym]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entries[sym], 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entries[sym] - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        trade_results.append((sym, side, trade_pnl, roi_trade, exit_reasons[sym], exit_price))\n",
        "        day_portfolio_return += weight * roi_trade\n",
        "\n",
        "    cumulative_portfolio_return += day_portfolio_return\n",
        "    day_return = round(day_portfolio_return, 2)\n",
        "    cum_return = round(cumulative_portfolio_return, 2)\n",
        "\n",
        "    for res in trade_results:\n",
        "        sym, side, trade_pnl, roi_trade, exit_reason, exit_price = res\n",
        "        signal_date = day_group[day_group[\"SYMBOL\"] == sym][\"SIGNAL_DATE\"].iloc[0]\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,  # SIGNAL_DATE\n",
        "            entry_date,   # TRADE_DATE\n",
        "            side,\n",
        "            entries[sym],\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reason,\n",
        "            day_return,\n",
        "            cum_return\n",
        "        ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvfCN185Jcv-",
        "outputId": "365cfef3-fa34-425e-f0bd-3d3cd5f4871c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 520 cash files...\n",
            "‚úÖ Processed 50/520 symbols\n",
            "‚úÖ Processed 100/520 symbols\n",
            "‚úÖ Processed 150/520 symbols\n",
            "‚úÖ Processed 200/520 symbols\n",
            "‚úÖ Processed 250/520 symbols\n",
            "‚úÖ Processed 300/520 symbols\n",
            "‚úÖ Processed 350/520 symbols\n",
            "‚úÖ Processed 400/520 symbols\n",
            "‚úÖ Processed 450/520 symbols\n",
            "‚úÖ Processed 500/520 symbols\n",
            "‚úÖ Loaded 520 symbols with required times and precomputed first hour indicators\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 312 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 157253 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, candle setups)\n",
            "‚úÖ After selection ‚Üí 463 signals selected for trading (top 2 per side per entry date)\n",
            "‚úÖ Backtest completed. 463 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================\n",
        "# üîπ CONFIG (revised for new strategy)\n",
        "# ==========================\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # 1% portfolio target\n",
        "PORTFOLIO_SL_PCT = -0.01       # -1% portfolio SL\n",
        "START_TIME = \"10:20\"           # Entry time (trigger)\n",
        "SL_ACTIVATION_TIME = \"10:20\"   # SL activation time\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "\n",
        "# ==========================\n",
        "# Path with many cash CSV files\n",
        "# ==========================\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper to parse datetime strings used across the script\n",
        "# ----------------------------\n",
        "def parse_ts_to_dt(ts_str):\n",
        "    try:\n",
        "        return datetime.strptime(ts_str[:19], \"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Function to read each CSV and return symbol + polars DataFrame (unchanged)\n",
        "# ----------------------------\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN LOAD + PRECOMPUTE (revised for first hour OHLC)\n",
        "# ----------------------------\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}   # holds 15:29, 09:15, 10:15, 10:20 closes keyed by TradeDate\n",
        "symbol_first_high = {}        # first hour (09:15-10:15) High series\n",
        "symbol_first_low = {}         # first hour Low series\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for required times\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:15\", \"10:15\", \"10:20\", \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # ensure TradeDate is datetime.date\n",
        "        pdf[\"TradeDate\"] = pd.to_datetime(pdf[\"TradeDate\"]).dt.date\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_915 = pdf[pdf[\"TradeTime\"] == \"09:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_1015 = pdf[pdf[\"TradeTime\"] == \"10:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_1020 = pdf[pdf[\"TradeTime\"] == \"10:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\n",
        "            \"close_1529\": close_1529,\n",
        "            \"close_915\": close_915,\n",
        "            \"close_1015\": close_1015,\n",
        "            \"close_1020\": close_1020\n",
        "        }\n",
        "\n",
        "    # Precompute first hour High and Low series\n",
        "    first_high_df = df.filter(\n",
        "        (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= \"10:15\")\n",
        "    ).group_by(\"TradeDate\").agg(\n",
        "        pl.max(\"High\").alias(\"FirstHigh\")\n",
        "    ).to_pandas()\n",
        "    if not first_high_df.empty:\n",
        "        first_high_df[\"TradeDate\"] = pd.to_datetime(first_high_df[\"TradeDate\"]).dt.date\n",
        "        symbol_first_high[symbol] = first_high_df.set_index(\"TradeDate\")[\"FirstHigh\"].sort_index()\n",
        "\n",
        "    first_low_df = df.filter(\n",
        "        (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= \"10:15\")\n",
        "    ).group_by(\"TradeDate\").agg(\n",
        "        pl.min(\"Low\").alias(\"FirstLow\")\n",
        "    ).to_pandas()\n",
        "    if not first_low_df.empty:\n",
        "        first_low_df[\"TradeDate\"] = pd.to_datetime(first_low_df[\"TradeDate\"]).dt.date\n",
        "        symbol_first_low[symbol] = first_low_df.set_index(\"TradeDate\")[\"FirstLow\"].sort_index()\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times and precomputed first hour indicators\")\n",
        "\n",
        "# --- Load NIFTY500 series (revised: add 09:15, 10:15, 10:20) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty_close_915 = None\n",
        "nifty_close_1015 = None\n",
        "nifty_close_1020 = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([\"09:15\", \"10:15\", \"10:20\", \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty_pdf[\"TradeDate\"] = pd.to_datetime(nifty_pdf[\"TradeDate\"]).dt.date\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty_close_915 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"09:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty_close_1015 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"10:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty_close_1020 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"10:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the next trading day\n",
        "def get_next_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    next_dates = [d for d in all_dates if pd.Timestamp(d).date() > trade_date]\n",
        "    if not next_dates:\n",
        "        return None\n",
        "    return min(next_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date with candle patterns -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    entry_date = get_next_trading_day(signal_date, unique_trade_dates)\n",
        "    if entry_date is None:\n",
        "        continue\n",
        "\n",
        "    # Compute NIFTY500 GAP for entry (from signal close to 09:15 open)\n",
        "    nifty_gap_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty_close_915 is not None:\n",
        "        try:\n",
        "            nifty_signal_close = float(nifty500_close_1529.loc[signal_date])\n",
        "            nifty_entry_915 = float(nifty_close_915.loc[entry_date])\n",
        "            if nifty_entry_915 != 0:\n",
        "                nifty_gap_for_date = ((nifty_entry_915 - nifty_signal_close) / nifty_entry_915) * 100.0\n",
        "        except Exception:\n",
        "            nifty_gap_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            prev_close = float(d[\"close_1529\"].loc[signal_date])\n",
        "            first_open = float(d[\"close_915\"].loc[entry_date])\n",
        "            first_close = float(d[\"close_1015\"].loc[entry_date])\n",
        "            trigger_close = float(d[\"close_1020\"].loc[entry_date])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close is None or first_open is None or first_close is None or trigger_close is None or first_open == 0:\n",
        "            continue\n",
        "\n",
        "        gap_pct = ((first_open - prev_close) / first_open) * 100.0\n",
        "\n",
        "        # Fetch precomputed first hour High/Low\n",
        "        first_high = None\n",
        "        first_low = None\n",
        "        if sym in symbol_first_high and entry_date in symbol_first_high[sym].index:\n",
        "            first_high = float(symbol_first_high[sym].loc[entry_date])\n",
        "        if sym in symbol_first_low and entry_date in symbol_first_low[sym].index:\n",
        "            first_low = float(symbol_first_low[sym].loc[entry_date])\n",
        "\n",
        "        if first_high is None or first_low is None:\n",
        "            continue\n",
        "\n",
        "        # Compute candle properties\n",
        "        body = abs(first_close - first_open)\n",
        "        if body == 0:\n",
        "            continue\n",
        "        upper_shadow = first_high - max(first_open, first_close)\n",
        "        lower_shadow = min(first_open, first_close) - first_low\n",
        "\n",
        "        # Pattern detection\n",
        "        is_inverted_pin = (upper_shadow >= 2 * body) and (lower_shadow <= 0.5 * body)\n",
        "        is_hammer = (lower_shadow >= 2 * body) and (upper_shadow <= 0.5 * body)\n",
        "\n",
        "        # Trigger conditions\n",
        "        bearish_trigger = trigger_close < first_close\n",
        "        bullish_trigger = trigger_close > first_close\n",
        "\n",
        "        # Gap conditions\n",
        "        gap_up = gap_pct > 0\n",
        "        gap_down = gap_pct < 0\n",
        "\n",
        "        # Setups\n",
        "        bullish = gap_down and is_hammer and bullish_trigger\n",
        "        bearish = gap_up and is_inverted_pin and bearish_trigger\n",
        "\n",
        "        range_size = first_high - first_low\n",
        "\n",
        "        all_breakdowns.append([\n",
        "            signal_date,\n",
        "            entry_date,\n",
        "            sym,\n",
        "            prev_close,\n",
        "            first_open,\n",
        "            first_high,\n",
        "            first_low,\n",
        "            first_close,\n",
        "            trigger_close,\n",
        "            gap_pct,\n",
        "            range_size,\n",
        "            nifty_gap_for_date,\n",
        "            bullish,\n",
        "            bearish\n",
        "        ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with revised columns\n",
        "breakdown_df = pd.DataFrame(all_breakdowns, columns=[\n",
        "    \"SIGNAL_DATE\", \"ENTRY_DATE\", \"SYMBOL\", \"PREV_CLOSE\", \"FIRST_HOUR_OPEN\", \"FIRST_HOUR_HIGH\", \"FIRST_HOUR_LOW\", \"FIRST_HOUR_CLOSE\",\n",
        "    \"TRIGGER_CLOSE\", \"GAP_%\", \"CANDLE_RANGE\", \"NIFTY_GAP_%\", \"BULLISH_SETUP\", \"BEARISH_SETUP\"\n",
        "])\n",
        "breakdown_df[\"GAP_%\"] = breakdown_df[\"GAP_%\"].astype(float).round(6)\n",
        "breakdown_df[\"CANDLE_RANGE\"] = breakdown_df[\"CANDLE_RANGE\"].astype(float).round(4)\n",
        "breakdown_df[\"NIFTY_GAP_%\"] = pd.to_numeric(breakdown_df[\"NIFTY_GAP_%\"], errors='coerce').round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, candle setups)\")\n",
        "\n",
        "# ----- Ranking logic: For each ENTRY_DATE pick top 2 LONG and top 2 SHORT ranked by CANDLE_RANGE desc -----\n",
        "ranked_signals = []\n",
        "\n",
        "for entry_date, daily_df in breakdown_df.groupby(\"ENTRY_DATE\"):\n",
        "    # Compute NIFTY GAP for the entry day if present\n",
        "    nifty_vals = daily_df[\"NIFTY_GAP_%\"].dropna().unique()\n",
        "    if len(nifty_vals) == 0:\n",
        "        nifty_gap_for_date = None\n",
        "    else:\n",
        "        nifty_gap_for_date = float(nifty_vals[0])\n",
        "\n",
        "    bullish_candidates = daily_df[daily_df[\"BULLISH_SETUP\"] == True].copy().sort_values(\"CANDLE_RANGE\", ascending=False).head(2)\n",
        "    bearish_candidates = daily_df[daily_df[\"BEARISH_SETUP\"] == True].copy().sort_values(\"CANDLE_RANGE\", ascending=False).head(2)\n",
        "\n",
        "    all_long = bullish_candidates.copy()\n",
        "    if not all_long.empty:\n",
        "        all_long[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "    all_short = bearish_candidates.copy()\n",
        "    if not all_short.empty:\n",
        "        all_short[\"SIDE\"] = \"SHORT\"\n",
        "\n",
        "    day_selected = pd.concat([all_long, all_short], ignore_index=True) if (not all_long.empty or not all_short.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After selection ‚Üí {len(ranked_df)} signals selected for trading (top 2 per side per entry date)\")\n",
        "\n",
        "# ----- Backtest/execution loop (revised: entry at 10:20, fixed SL from first hour, portfolio exits) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for entry_date, day_group in ranked_df.groupby(\"ENTRY_DATE\"):\n",
        "    # For each entry day, collect the symbols and sides\n",
        "    signals = day_group.set_index(\"SYMBOL\")[\"SIDE\"].to_dict()\n",
        "    num_signals = len(signals)\n",
        "    if num_signals == 0:\n",
        "        continue\n",
        "    weight = 1.0 / num_signals  # equal weight\n",
        "\n",
        "    # Get entry prices and SL prices\n",
        "    entries = {}\n",
        "    sl_prices = {}\n",
        "    for sym, side in list(signals.items()):\n",
        "        d = symbol_close_start_end.get(sym, {})\n",
        "        if entry_date not in d.get(\"close_1020\", pd.Series()):\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entry_price = float(d[\"close_1020\"].loc[entry_date])\n",
        "        if entry_price == 0:\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entries[sym] = entry_price\n",
        "\n",
        "        # Fixed SL from first hour\n",
        "        if sym in symbol_first_high and entry_date in symbol_first_high[sym].index:\n",
        "            first_h = float(symbol_first_high[sym].loc[entry_date])\n",
        "        else:\n",
        "            first_h = entry_price  # fallback\n",
        "        if sym in symbol_first_low and entry_date in symbol_first_low[sym].index:\n",
        "            first_l = float(symbol_first_low[sym].loc[entry_date])\n",
        "        else:\n",
        "            first_l = entry_price  # fallback\n",
        "        if side == \"LONG\":\n",
        "            sl_prices[sym] = first_l\n",
        "        else:\n",
        "            sl_prices[sym] = first_h\n",
        "\n",
        "    if not signals:\n",
        "        continue\n",
        "\n",
        "    num_signals = len(signals)\n",
        "    weight = 1.0 / num_signals\n",
        "\n",
        "    # Collect day prices for symbols on entry_date (from 10:20 onwards)\n",
        "    all_times = set()\n",
        "    day_prices = {}\n",
        "    for sym in signals:\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == entry_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_df = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)].copy()\n",
        "        if day_df.empty:\n",
        "            continue\n",
        "        day_df = day_df.set_index(\"TradeTime\")\n",
        "        day_df = day_df[~day_df.index.duplicated(keep='last')]\n",
        "        day_prices[sym] = day_df[\"Close\"]\n",
        "        all_times.update(day_df.index)\n",
        "\n",
        "    all_times = sorted(all_times)\n",
        "    if not all_times:\n",
        "        continue\n",
        "\n",
        "    sim_df = pd.DataFrame(index=all_times)\n",
        "    for sym in signals:\n",
        "        sym_prices = day_prices.get(sym)\n",
        "        if sym_prices is None:\n",
        "            sim_df[sym] = entries[sym]\n",
        "            continue\n",
        "        sym_prices_reindexed = sym_prices.reindex(all_times).ffill().bfill()\n",
        "        sim_df[sym] = sym_prices_reindexed\n",
        "\n",
        "    exit_times = {sym: None for sym in signals}\n",
        "    exit_prices = {sym: None for sym in signals}\n",
        "    exit_reasons = {sym: END_TIME for sym in signals}\n",
        "\n",
        "    for t in sim_df.index:\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        if not open_trades:\n",
        "            break\n",
        "\n",
        "        current_rois = {}\n",
        "        portfolio_pnl_decimal = 0.0\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            if side == \"LONG\":\n",
        "                current_roi = (cur_price - entries[sym]) / entries[sym] * 100\n",
        "            else:\n",
        "                current_roi = (entries[sym] - cur_price) / entries[sym] * 100\n",
        "            current_rois[sym] = current_roi\n",
        "            portfolio_pnl_decimal += weight * (current_roi / 100)\n",
        "\n",
        "        # Portfolio-level exit check\n",
        "        if portfolio_pnl_decimal >= PORTFOLIO_TARGET_PCT or portfolio_pnl_decimal <= PORTFOLIO_SL_PCT:\n",
        "            exit_reason = \"PORTFOLIO_TARGET\" if portfolio_pnl_decimal >= PORTFOLIO_TARGET_PCT else \"PORTFOLIO_SL\"\n",
        "            for sym in open_trades:\n",
        "                if exit_times[sym] is None:\n",
        "                    cur_price = sim_df.at[t, sym]\n",
        "                    exit_times[sym] = t\n",
        "                    exit_prices[sym] = cur_price\n",
        "                    exit_reasons[sym] = exit_reason\n",
        "\n",
        "        # Individual SL check (updated open_trades if portfolio exited some)\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            sl = sl_prices[sym]\n",
        "            if (side == \"LONG\" and cur_price <= sl) or (side == \"SHORT\" and cur_price >= sl):\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = cur_price\n",
        "                exit_reasons[sym] = f\"INDIV_SL_{t}\"\n",
        "\n",
        "    # Final exit at END_TIME for remaining\n",
        "    final_time = END_TIME if END_TIME in sim_df.index else all_times[-1]\n",
        "    for sym in signals:\n",
        "        if exit_times[sym] is None:\n",
        "            exit_times[sym] = final_time\n",
        "            try:\n",
        "                exit_prices[sym] = sim_df.at[final_time, sym]\n",
        "            except Exception:\n",
        "                exit_prices[sym] = entries[sym]\n",
        "            exit_reasons[sym] = f\"END_TIME_{final_time}\"\n",
        "\n",
        "    # Compute day results\n",
        "    day_portfolio_return = 0.0\n",
        "    trade_results = []\n",
        "    for sym, side in signals.items():\n",
        "        exit_price = exit_prices[sym]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entries[sym], 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entries[sym] - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        trade_results.append((sym, side, trade_pnl, roi_trade, exit_reasons[sym], exit_price))\n",
        "        day_portfolio_return += weight * roi_trade\n",
        "\n",
        "    cumulative_portfolio_return += day_portfolio_return\n",
        "    day_return = round(day_portfolio_return, 2)\n",
        "    cum_return = round(cumulative_portfolio_return, 2)\n",
        "\n",
        "    for res in trade_results:\n",
        "        sym, side, trade_pnl, roi_trade, exit_reason, exit_price = res\n",
        "        signal_date = day_group[day_group[\"SYMBOL\"] == sym][\"SIGNAL_DATE\"].iloc[0]\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,  # SIGNAL_DATE\n",
        "            entry_date,   # TRADE_DATE\n",
        "            side,\n",
        "            entries[sym],\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reason,\n",
        "            day_return,\n",
        "            cum_return\n",
        "        ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3anRn1ISfHvE",
        "outputId": "99255659-8e8b-448f-996f-095be73af5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 520 cash files...\n",
            "‚úÖ Processed 50/520 symbols\n",
            "‚úÖ Processed 100/520 symbols\n",
            "‚úÖ Processed 150/520 symbols\n",
            "‚úÖ Processed 200/520 symbols\n",
            "‚úÖ Processed 250/520 symbols\n",
            "‚úÖ Processed 300/520 symbols\n",
            "‚úÖ Processed 350/520 symbols\n",
            "‚úÖ Processed 400/520 symbols\n",
            "‚úÖ Processed 450/520 symbols\n",
            "‚úÖ Processed 500/520 symbols\n",
            "‚úÖ Loaded 520 symbols with required times and precomputed first hour indicators\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 312 potential trade dates from symbol data\n",
            "‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí 157253 rows\n",
            "üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, candle setups)\n",
            "‚úÖ After selection ‚Üí 1233 signals selected for trading (top 2 per side per entry date)\n",
            "‚úÖ Backtest completed. 1233 trades executed.\n",
            "üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "üìÑ Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================\n",
        "# üîπ CONFIG (revised for new strategy)\n",
        "# ==========================\n",
        "PORTFOLIO_TARGET_PCT = 0.01    # (unused now for portfolio exits; left for compatibility)\n",
        "PORTFOLIO_SL_PCT = -0.01       # (unused now)\n",
        "START_TIME = \"10:20\"           # Entry time (trigger)\n",
        "SL_ACTIVATION_TIME = \"10:20\"   # SL activation time\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "\n",
        "# ==========================\n",
        "# Path with many cash CSV files\n",
        "# ==========================\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper to parse datetime strings used across the script\n",
        "# ----------------------------\n",
        "def parse_ts_to_dt(ts_str):\n",
        "    try:\n",
        "        return datetime.strptime(ts_str[:19], \"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Function to read each CSV and return symbol + polars DataFrame (unchanged)\n",
        "# ----------------------------\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN LOAD + PRECOMPUTE (revised for first hour OHLC)\n",
        "# ----------------------------\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}   # holds 15:29, 09:15, 10:15, 10:20 closes keyed by TradeDate\n",
        "symbol_first_high = {}        # first hour (09:15-10:15) High series\n",
        "symbol_first_low = {}         # first hour Low series\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for required times\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([\"09:15\", \"10:15\", \"10:20\", \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # ensure TradeDate is datetime.date\n",
        "        pdf[\"TradeDate\"] = pd.to_datetime(pdf[\"TradeDate\"]).dt.date\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_915 = pdf[pdf[\"TradeTime\"] == \"09:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_1015 = pdf[pdf[\"TradeTime\"] == \"10:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        close_1020 = pdf[pdf[\"TradeTime\"] == \"10:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\n",
        "            \"close_1529\": close_1529,\n",
        "            \"close_915\": close_915,\n",
        "            \"close_1015\": close_1015,\n",
        "            \"close_1020\": close_1020\n",
        "        }\n",
        "\n",
        "    # Precompute first hour High and Low series\n",
        "    first_high_df = df.filter(\n",
        "        (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= \"10:15\")\n",
        "    ).group_by(\"TradeDate\").agg(\n",
        "        pl.max(\"High\").alias(\"FirstHigh\")\n",
        "    ).to_pandas()\n",
        "    if not first_high_df.empty:\n",
        "        first_high_df[\"TradeDate\"] = pd.to_datetime(first_high_df[\"TradeDate\"]).dt.date\n",
        "        symbol_first_high[symbol] = first_high_df.set_index(\"TradeDate\")[\"FirstHigh\"].sort_index()\n",
        "\n",
        "    first_low_df = df.filter(\n",
        "        (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= \"10:15\")\n",
        "    ).group_by(\"TradeDate\").agg(\n",
        "        pl.min(\"Low\").alias(\"FirstLow\")\n",
        "    ).to_pandas()\n",
        "    if not first_low_df.empty:\n",
        "        first_low_df[\"TradeDate\"] = pd.to_datetime(first_low_df[\"TradeDate\"]).dt.date\n",
        "        symbol_first_low[symbol] = first_low_df.set_index(\"TradeDate\")[\"FirstLow\"].sort_index()\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times and precomputed first hour indicators\")\n",
        "\n",
        "# --- Load NIFTY500 series (revised: add 09:15, 10:15, 10:20) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty_close_915 = None\n",
        "nifty_close_1015 = None\n",
        "nifty_close_1020 = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([\"09:15\", \"10:15\", \"10:20\", \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty_pdf[\"TradeDate\"] = pd.to_datetime(nifty_pdf[\"TradeDate\"]).dt.date\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty_close_915 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"09:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty_close_1015 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"10:15\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty_close_1020 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"10:20\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the next trading day\n",
        "def get_next_trading_day(trade_date, all_dates):\n",
        "    trade_date = pd.Timestamp(trade_date).date() if not isinstance(trade_date, (pd.Timestamp, datetime)) else pd.Timestamp(trade_date).date()\n",
        "    next_dates = [d for d in all_dates if pd.Timestamp(d).date() > trade_date]\n",
        "    if not next_dates:\n",
        "        return None\n",
        "    return min(next_dates)\n",
        "\n",
        "# ----- Build ALL_BREAKDOWNS list for all symbols on each date with candle patterns -----\n",
        "all_breakdowns = []\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    entry_date = get_next_trading_day(signal_date, unique_trade_dates)\n",
        "    if entry_date is None:\n",
        "        continue\n",
        "\n",
        "    # Compute NIFTY500 GAP for entry (from signal close to 09:15 open)\n",
        "    nifty_gap_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty_close_915 is not None:\n",
        "        try:\n",
        "            nifty_signal_close = float(nifty500_close_1529.loc[signal_date])\n",
        "            nifty_entry_915 = float(nifty_close_915.loc[entry_date])\n",
        "            if nifty_entry_915 != 0:\n",
        "                nifty_gap_for_date = ((nifty_entry_915 - nifty_signal_close) / nifty_entry_915) * 100.0\n",
        "        except Exception:\n",
        "            nifty_gap_for_date = None\n",
        "\n",
        "    for sym, d in symbol_close_start_end.items():\n",
        "        try:\n",
        "            prev_close = float(d[\"close_1529\"].loc[signal_date])\n",
        "            first_open = float(d[\"close_915\"].loc[entry_date])\n",
        "            first_close = float(d[\"close_1015\"].loc[entry_date])\n",
        "            trigger_close = float(d[\"close_1020\"].loc[entry_date])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close is None or first_open is None or first_close is None or trigger_close is None or first_open == 0:\n",
        "            continue\n",
        "\n",
        "        # Fetch precomputed first hour High/Low\n",
        "        first_high = None\n",
        "        first_low = None\n",
        "        if sym in symbol_first_high and entry_date in symbol_first_high[sym].index:\n",
        "            first_high = float(symbol_first_high[sym].loc[entry_date])\n",
        "        if sym in symbol_first_low and entry_date in symbol_first_low[sym].index:\n",
        "            first_low = float(symbol_first_low[sym].loc[entry_date])\n",
        "\n",
        "        if first_high is None or first_low is None:\n",
        "            continue\n",
        "\n",
        "        # Compute candle properties\n",
        "        body = abs(first_close - first_open)\n",
        "        if body == 0:\n",
        "            continue\n",
        "        upper_shadow = first_high - max(first_open, first_close)\n",
        "        lower_shadow = min(first_open, first_close) - first_low\n",
        "\n",
        "        # Pattern detection (kept - not used as primary trigger but kept for potential additional filtering)\n",
        "        is_inverted_pin = (upper_shadow >= 2 * body) and (lower_shadow <= 0.5 * body)\n",
        "        is_hammer = (lower_shadow >= 2 * body) and (upper_shadow <= 0.5 * body)\n",
        "\n",
        "        # --- NEW: compute last 5 hourly closes/highs/lows from previous trading day (signal_date)\n",
        "        # We'll take the last 5 rows of the previous day's intraday series (by TradeTime ordering)\n",
        "        last5_max_close = None\n",
        "        last5_min_close = None\n",
        "        last5_max_high = None\n",
        "        last5_min_low = None\n",
        "        try:\n",
        "            df_prev_day = symbol_full_data.get(sym).filter(pl.col(\"TradeDate\") == signal_date).select([\"TradeTime\",\"Close\",\"High\",\"Low\"]).to_pandas()\n",
        "            if not df_prev_day.empty:\n",
        "                # Ensure sorted by time\n",
        "                df_prev_day = df_prev_day.sort_values(\"TradeTime\")\n",
        "                last5 = df_prev_day.tail(5)\n",
        "                if len(last5) >= 1:\n",
        "                    last5_max_close = float(last5[\"Close\"].max())\n",
        "                    last5_min_close = float(last5[\"Close\"].min())\n",
        "                    last5_max_high = float(last5[\"High\"].max())\n",
        "                    last5_min_low = float(last5[\"Low\"].min())\n",
        "        except Exception:\n",
        "            last5_max_close = last5_min_close = last5_max_high = last5_min_low = None\n",
        "\n",
        "        if last5_max_close is None or last5_min_close is None or last5_max_high is None or last5_min_low is None:\n",
        "            continue\n",
        "\n",
        "        # Trigger conditions based on your new rules:\n",
        "        # SHORT setup:\n",
        "        #   first_hour_open > last5_max_close  AND first_hour_close < last5_min_close  -> Entry SHORT\n",
        "        # LONG setup:\n",
        "        #   first_hour_open < last5_min_close  AND first_hour_close > last5_max_close  -> Entry LONG\n",
        "        bearish = (first_open > last5_max_close) and (first_close < last5_min_close)\n",
        "        bullish = (first_open < last5_min_close) and (first_close > last5_max_close)\n",
        "\n",
        "        range_size = first_high - first_low\n",
        "\n",
        "        all_breakdowns.append([\n",
        "            signal_date,\n",
        "            entry_date,\n",
        "            sym,\n",
        "            prev_close,\n",
        "            first_open,\n",
        "            first_high,\n",
        "            first_low,\n",
        "            first_close,\n",
        "            trigger_close,\n",
        "            # keep gap_pct for bookkeeping (not used in trigger now)\n",
        "            ((first_open - prev_close) / first_open) * 100.0,\n",
        "            range_size,\n",
        "            nifty_gap_for_date,\n",
        "            bullish,\n",
        "            bearish,\n",
        "            # include last5 metrics in breakdown for reference\n",
        "            last5_max_close,\n",
        "            last5_min_close,\n",
        "            last5_max_high,\n",
        "            last5_min_low\n",
        "        ])\n",
        "\n",
        "print(f\"‚úÖ Built ALL_BREAKDOWNS for all symbols ‚Üí {len(all_breakdowns)} rows\")\n",
        "\n",
        "# Save ALL_BREAKDOWNS.csv with revised columns (including last5 metrics)\n",
        "breakdown_df = pd.DataFrame(all_breakdowns, columns=[\n",
        "    \"SIGNAL_DATE\", \"ENTRY_DATE\", \"SYMBOL\", \"PREV_CLOSE\", \"FIRST_HOUR_OPEN\", \"FIRST_HOUR_HIGH\", \"FIRST_HOUR_LOW\", \"FIRST_HOUR_CLOSE\",\n",
        "    \"TRIGGER_CLOSE\", \"GAP_%\", \"CANDLE_RANGE\", \"NIFTY_GAP_%\", \"BULLISH_SETUP\", \"BEARISH_SETUP\",\n",
        "    \"LAST5_MAX_CLOSE\", \"LAST5_MIN_CLOSE\", \"LAST5_MAX_HIGH\", \"LAST5_MIN_LOW\"\n",
        "])\n",
        "breakdown_df[\"GAP_%\"] = breakdown_df[\"GAP_%\"].astype(float).round(6)\n",
        "breakdown_df[\"CANDLE_RANGE\"] = breakdown_df[\"CANDLE_RANGE\"].astype(float).round(4)\n",
        "breakdown_df[\"NIFTY_GAP_%\"] = pd.to_numeric(breakdown_df[\"NIFTY_GAP_%\"], errors='coerce').round(4)\n",
        "\n",
        "breakdown_df.to_csv(\"ALL_BREAKDOWNS.csv\", index=False)\n",
        "print(\"üìÑ Saved ALL_BREAKDOWNS.csv (all symbols, candle setups)\")\n",
        "\n",
        "# ----- Ranking logic: For each ENTRY_DATE pick top 2 LONG and top 2 SHORT ranked by CANDLE_RANGE desc -----\n",
        "ranked_signals = []\n",
        "\n",
        "for entry_date, daily_df in breakdown_df.groupby(\"ENTRY_DATE\"):\n",
        "    # Compute NIFTY GAP for the entry day if present\n",
        "    nifty_vals = daily_df[\"NIFTY_GAP_%\"].dropna().unique()\n",
        "    if len(nifty_vals) == 0:\n",
        "        nifty_gap_for_date = None\n",
        "    else:\n",
        "        nifty_gap_for_date = float(nifty_vals[0])\n",
        "\n",
        "    bullish_candidates = daily_df[daily_df[\"BULLISH_SETUP\"] == True].copy().sort_values(\"CANDLE_RANGE\", ascending=False).head(2)\n",
        "    bearish_candidates = daily_df[daily_df[\"BEARISH_SETUP\"] == True].copy().sort_values(\"CANDLE_RANGE\", ascending=False).head(2)\n",
        "\n",
        "    all_long = bullish_candidates.copy()\n",
        "    if not all_long.empty:\n",
        "        all_long[\"SIDE\"] = \"LONG\"\n",
        "\n",
        "    all_short = bearish_candidates.copy()\n",
        "    if not all_short.empty:\n",
        "        all_short[\"SIDE\"] = \"SHORT\"\n",
        "\n",
        "    day_selected = pd.concat([all_long, all_short], ignore_index=True) if (not all_long.empty or not all_short.empty) else pd.DataFrame()\n",
        "    if not day_selected.empty:\n",
        "        ranked_signals.append(day_selected)\n",
        "\n",
        "if ranked_signals:\n",
        "    ranked_df = pd.concat(ranked_signals, ignore_index=True)\n",
        "else:\n",
        "    ranked_df = pd.DataFrame(columns=breakdown_df.columns.tolist() + [\"SIDE\"])\n",
        "\n",
        "print(f\"‚úÖ After selection ‚Üí {len(ranked_df)} signals selected for trading (top 2 per side per entry date)\")\n",
        "\n",
        "# ----- Backtest/execution loop (revised: entry at 10:20, INDIVIDUAL SL based on last5 highs/lows, NO portfolio SL/TARGET) -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for entry_date, day_group in ranked_df.groupby(\"ENTRY_DATE\"):\n",
        "    # For each entry day, collect the symbols and sides\n",
        "    signals = day_group.set_index(\"SYMBOL\")[\"SIDE\"].to_dict()\n",
        "    num_signals = len(signals)\n",
        "    if num_signals == 0:\n",
        "        continue\n",
        "    weight = 1.0 / num_signals  # equal weight\n",
        "\n",
        "    # Get entry prices and SL prices (individual SL based on previous day's last5 highs/lows)\n",
        "    entries = {}\n",
        "    sl_prices = {}\n",
        "    for sym, side in list(signals.items()):\n",
        "        d = symbol_close_start_end.get(sym, {})\n",
        "        if entry_date not in d.get(\"close_1020\", pd.Series()):\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entry_price = float(d[\"close_1020\"].loc[entry_date])\n",
        "        if entry_price == 0:\n",
        "            del signals[sym]\n",
        "            continue\n",
        "        entries[sym] = entry_price\n",
        "\n",
        "        # Fetch last5 metrics from breakdown_df if available\n",
        "        try:\n",
        "            row = day_group[day_group[\"SYMBOL\"] == sym].iloc[0]\n",
        "            last5_max_high = float(row[\"LAST5_MAX_HIGH\"])\n",
        "            last5_min_low = float(row[\"LAST5_MIN_LOW\"])\n",
        "        except Exception:\n",
        "            # Fallback: use first hour high/low as conservative SLs\n",
        "            last5_max_high = float(symbol_first_high.get(sym, pd.Series()).get(entry_date, entry_price))\n",
        "            last5_min_low = float(symbol_first_low.get(sym, pd.Series()).get(entry_date, entry_price))\n",
        "\n",
        "        if side == \"LONG\":\n",
        "            # SL for LONG is last 5 hour minimum low\n",
        "            sl_prices[sym] = last5_min_low\n",
        "        else:\n",
        "            # SL for SHORT is last 5 hour maximum high\n",
        "            sl_prices[sym] = last5_max_high\n",
        "\n",
        "    if not signals:\n",
        "        continue\n",
        "\n",
        "    num_signals = len(signals)\n",
        "    weight = 1.0 / num_signals\n",
        "\n",
        "    # Collect day prices for symbols on entry_date (from 10:20 onwards)\n",
        "    all_times = set()\n",
        "    day_prices = {}\n",
        "    for sym in signals:\n",
        "        df_full = symbol_full_data.get(sym)\n",
        "        if df_full is None:\n",
        "            continue\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == entry_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        day_df = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)].copy()\n",
        "        if day_df.empty:\n",
        "            continue\n",
        "        day_df = day_df.set_index(\"TradeTime\")\n",
        "        day_df = day_df[~day_df.index.duplicated(keep='last')]\n",
        "        day_prices[sym] = day_df[\"Close\"]\n",
        "        all_times.update(day_df.index)\n",
        "\n",
        "    all_times = sorted(all_times)\n",
        "    if not all_times:\n",
        "        continue\n",
        "\n",
        "    sim_df = pd.DataFrame(index=all_times)\n",
        "    for sym in signals:\n",
        "        sym_prices = day_prices.get(sym)\n",
        "        if sym_prices is None:\n",
        "            sim_df[sym] = entries[sym]\n",
        "            continue\n",
        "        sym_prices_reindexed = sym_prices.reindex(all_times).ffill().bfill()\n",
        "        sim_df[sym] = sym_prices_reindexed\n",
        "\n",
        "    exit_times = {sym: None for sym in signals}\n",
        "    exit_prices = {sym: None for sym in signals}\n",
        "    exit_reasons = {sym: END_TIME for sym in signals}\n",
        "\n",
        "    for t in sim_df.index:\n",
        "        open_trades = [sym for sym in signals if exit_times[sym] is None]\n",
        "        if not open_trades:\n",
        "            break\n",
        "\n",
        "        # Individual SL check only (no portfolio-level exit)\n",
        "        for sym in open_trades:\n",
        "            cur_price = sim_df.at[t, sym]\n",
        "            side = signals[sym]\n",
        "            sl = sl_prices[sym]\n",
        "            if (side == \"LONG\" and cur_price <= sl) or (side == \"SHORT\" and cur_price >= sl):\n",
        "                exit_times[sym] = t\n",
        "                exit_prices[sym] = cur_price\n",
        "                exit_reasons[sym] = f\"INDIV_SL_{t}\"\n",
        "\n",
        "    # Final exit at END_TIME for remaining\n",
        "    final_time = END_TIME if END_TIME in sim_df.index else all_times[-1]\n",
        "    for sym in signals:\n",
        "        if exit_times[sym] is None:\n",
        "            exit_times[sym] = final_time\n",
        "            try:\n",
        "                exit_prices[sym] = sim_df.at[final_time, sym]\n",
        "            except Exception:\n",
        "                exit_prices[sym] = entries[sym]\n",
        "            exit_reasons[sym] = f\"END_TIME_{final_time}\"\n",
        "\n",
        "    # Compute day results\n",
        "    day_portfolio_return = 0.0\n",
        "    trade_results = []\n",
        "    for sym, side in signals.items():\n",
        "        exit_price = exit_prices[sym]\n",
        "        if side == \"LONG\":\n",
        "            trade_pnl = round(exit_price - entries[sym], 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        else:\n",
        "            trade_pnl = round(entries[sym] - exit_price, 2)\n",
        "            roi_trade = round((trade_pnl / entries[sym]) * 100, 2)\n",
        "        trade_results.append((sym, side, trade_pnl, roi_trade, exit_reasons[sym], exit_price))\n",
        "        day_portfolio_return += weight * roi_trade\n",
        "\n",
        "    cumulative_portfolio_return += day_portfolio_return\n",
        "    day_return = round(day_portfolio_return, 2)\n",
        "    cum_return = round(cumulative_portfolio_return, 2)\n",
        "\n",
        "    for res in trade_results:\n",
        "        sym, side, trade_pnl, roi_trade, exit_reason, exit_price = res\n",
        "        signal_date = day_group[day_group[\"SYMBOL\"] == sym][\"SIGNAL_DATE\"].iloc[0]\n",
        "        output_trades.append([\n",
        "            sym,\n",
        "            signal_date,  # SIGNAL_DATE\n",
        "            entry_date,   # TRADE_DATE\n",
        "            side,\n",
        "            entries[sym],\n",
        "            exit_price,\n",
        "            trade_pnl,\n",
        "            roi_trade,\n",
        "            exit_reason,\n",
        "            day_return,\n",
        "            cum_return\n",
        "        ])\n",
        "\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"BUY_START/ENTRY\", \"EXIT_PRICE\", \"PNL\", \"TRADE_ROI%\", \"EXIT_REASON\",\n",
        "                                  \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ‚úÖ Generate Daily PnL from executed trades\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(f\"üìÑ Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL sheet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQzk2v8NXIz"
      },
      "source": [
        "#inverted pin & hammer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKE07HDqqUXH",
        "outputId": "ddd0ca6f-8f1a-4cf1-d1fa-b9de01a2a9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 312 potential trade dates from symbol data\n",
            "Backtest completed. 1244 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "START_TIME = \"09:15\"           # Snapshot time for first candle\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding (not used currently)\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for prev_close)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)\n",
        "    if prev_trade_date is None:\n",
        "        continue\n",
        "\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for sym in symbol_full_data:\n",
        "        if sym not in symbol_close_start_end:\n",
        "            continue\n",
        "        d = symbol_close_start_end[sym]\n",
        "        try:\n",
        "            prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        # Pull full-day minute prices for signal_date\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        candle_0915 = day_df.filter(pl.col(\"TradeTime\") == \"09:15\")\n",
        "        if candle_0915.is_empty():\n",
        "            continue\n",
        "\n",
        "        o = float(candle_0915[\"Open\"][0])\n",
        "        h = float(candle_0915[\"High\"][0])\n",
        "        l = float(candle_0915[\"Low\"][0])\n",
        "        c = float(candle_0915[\"Close\"][0])\n",
        "\n",
        "        # Compute gap ROI\n",
        "        gap_roi = ((c - prev_close) / prev_close) * 100.0\n",
        "\n",
        "        range_size = h - l\n",
        "        if range_size <= 0:\n",
        "            continue\n",
        "\n",
        "        side = None\n",
        "        trigger_level = None\n",
        "        sl_level = None\n",
        "\n",
        "        if gap_roi > 0:  # Gap up ‚Üí check inverted pin\n",
        "            lower_body = c - l\n",
        "            if (lower_body / range_size) <= 0.3:\n",
        "                side = \"SHORT\"\n",
        "                trigger_level = l\n",
        "                sl_level = h\n",
        "        elif gap_roi < 0:  # Gap down ‚Üí check hammer\n",
        "            upper_shadow = h - c\n",
        "            if (upper_shadow / range_size) <= 0.3:\n",
        "                side = \"LONG\"\n",
        "                trigger_level = h\n",
        "                sl_level = l\n",
        "\n",
        "        if side is None:\n",
        "            continue\n",
        "\n",
        "        # Get later minutes after 09:15\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:15\").sort(\"dt\")\n",
        "        if later_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        for _, row in later_pd.iterrows():\n",
        "            cur_close = row[\"Close\"]\n",
        "            cur_time_str = row[\"TradeTime\"]\n",
        "            cur_dt = row[\"dt\"]\n",
        "\n",
        "            if cur_time_str > ENTRY_CUTOFF_TIME:\n",
        "                break\n",
        "\n",
        "            if (side == \"SHORT\" and cur_close < trigger_level) or \\\n",
        "               (side == \"LONG\" and cur_close > trigger_level):\n",
        "                entry_price = cur_close\n",
        "                trigger_dt = cur_dt\n",
        "                trigger_time_str = cur_time_str\n",
        "                triggered = True\n",
        "                break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"sl_level\": sl_level\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        sl_level = entry[\"sl_level\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # === FIXED SECTION START ===\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == signal_date).select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        # Correct boolean masking\n",
        "        mask = (day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)\n",
        "        day_prices = day_prices[mask].sort_values(\"dt\").reset_index(drop=True)\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        # Find trigger candle index\n",
        "        trigger_mask = day_prices[\"TradeTime\"] == trigger_time_str\n",
        "        if not trigger_mask.any():\n",
        "            continue\n",
        "        trigger_idx = day_prices[trigger_mask].index[0]\n",
        "\n",
        "        post_entry_prices = day_prices.iloc[trigger_idx + 1:].reset_index(drop=True)\n",
        "        # === FIXED SECTION END ===\n",
        "\n",
        "        if post_entry_prices.empty:\n",
        "            exit_price = entry_price\n",
        "            exit_reason = \"NO_POST_ENTRY_CANDLES\"\n",
        "            exit_dt = day_prices.iloc[trigger_idx][\"dt\"]\n",
        "        else:\n",
        "            exit_price = None\n",
        "            exit_reason = END_TIME\n",
        "            exit_dt = None\n",
        "            prev_is_sl_condition = False\n",
        "            sl_hit = False\n",
        "\n",
        "            for _, minute_row in post_entry_prices.iterrows():\n",
        "                cur_close = minute_row[\"Close\"]\n",
        "                cur_time = minute_row[\"TradeTime\"]\n",
        "                cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "                is_sl_cond = (cur_close < sl_level) if side == \"LONG\" else (cur_close > sl_level)\n",
        "                hit_condition = is_sl_cond and prev_is_sl_condition\n",
        "                prev_is_sl_condition = is_sl_cond\n",
        "\n",
        "                if hit_condition:\n",
        "                    exit_price = cur_close\n",
        "                    exit_reason = f\"SL_{cur_time}\"\n",
        "                    exit_dt = cur_dt\n",
        "                    sl_hit = True\n",
        "                    break\n",
        "\n",
        "            if not sl_hit:\n",
        "                end_time_mask = post_entry_prices[\"TradeTime\"] == END_TIME\n",
        "                if end_time_mask.any():\n",
        "                    exit_price = post_entry_prices[end_time_mask][\"Close\"].values[0]\n",
        "                    exit_dt = post_entry_prices[end_time_mask][\"dt\"].values[0]\n",
        "                else:\n",
        "                    exit_price = post_entry_prices[\"Close\"].iloc[-1]\n",
        "                    exit_dt = post_entry_prices[\"dt\"].iloc[-1]\n",
        "                    exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL\n",
        "        position_pnl = qty * (exit_price - entry_price) if side == \"LONG\" else qty * (entry_price - exit_price)\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, signal_date, side,\n",
        "            entry_price, qty, position_value, exit_price, position_pnl,\n",
        "            trade_roi_pct, portfolio_return_pct, cumulative_return_pct,\n",
        "            exit_reason, trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "# === Save Results ===\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# === Daily PnL ===\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXACbJUSNb0-",
        "outputId": "ede9214c-5dae-43be-d3f9-ca833cbe0b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 313 potential trade dates from symbol data\n",
            "Backtest completed. 1242 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "START_TIME = \"09:15\"           # Snapshot time for first candle\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding (not used currently)\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "symbol_daily_hlc = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Daily HLC\n",
        "    df_daily = df.group_by(pl.col(\"TradeDate\")).agg([\n",
        "        pl.col(\"High\").max().alias(\"DailyHigh\"),\n",
        "        pl.col(\"Low\").min().alias(\"DailyLow\"),\n",
        "        pl.col(\"Close\").last().alias(\"DailyClose\")\n",
        "    ]).sort(\"TradeDate\")\n",
        "    symbol_daily_hlc[symbol] = df_daily.select([\"TradeDate\", \"DailyHigh\", \"DailyLow\", \"DailyClose\"]).to_pandas().set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for prev_close, kept for compatibility)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily_hlc)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily_hlc.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)\n",
        "    if prev_trade_date is None:\n",
        "        continue\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible (optional, not used)\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[signal_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    # Get last 15 trading days back from signal_date\n",
        "    dates_back = []\n",
        "    current = signal_date\n",
        "    for _ in range(15):\n",
        "        current = get_prev_trading_day(current, unique_trade_dates)\n",
        "        if current is None:\n",
        "            break\n",
        "        dates_back.append(current)\n",
        "\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for sym in symbol_full_data:\n",
        "        if sym not in symbol_daily_hlc:\n",
        "            continue\n",
        "        d_hlc = symbol_daily_hlc[sym]\n",
        "        try:\n",
        "            prev_close = float(d_hlc.loc[prev_trade_date, \"DailyClose\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        # Get HLC back for this sym\n",
        "        try:\n",
        "            hlc_back = d_hlc.loc[dates_back]\n",
        "            if len(hlc_back) == 0:\n",
        "                continue\n",
        "            avg_h = hlc_back[\"DailyHigh\"].mean()\n",
        "            avg_l = hlc_back[\"DailyLow\"].mean()\n",
        "            avg_c = hlc_back[\"DailyClose\"].mean()\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        # Compute Fibonacci Pivot Levels\n",
        "        pp = (avg_h + avg_l + avg_c) / 3\n",
        "        pivot_range = avg_h - avg_l\n",
        "        levels = {\n",
        "            'PP': pp,\n",
        "            'R1': pp + 0.382 * pivot_range,\n",
        "            'R2': pp + 0.618 * pivot_range,\n",
        "            'R3': pp + 1.0 * pivot_range,\n",
        "            'S1': pp - 0.382 * pivot_range,\n",
        "            'S2': pp - 0.618 * pivot_range,\n",
        "            'S3': pp - 1.0 * pivot_range,\n",
        "        }\n",
        "\n",
        "        # Pull full-day minute prices for signal_date\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        candle_0915 = day_df.filter(pl.col(\"TradeTime\") == \"09:15\")\n",
        "        if candle_0915.is_empty():\n",
        "            continue\n",
        "\n",
        "        o = float(candle_0915[\"Open\"][0])\n",
        "        h = float(candle_0915[\"High\"][0])\n",
        "        l = float(candle_0915[\"Low\"][0])\n",
        "        c = float(candle_0915[\"Close\"][0])\n",
        "\n",
        "        # Compute gap ROI\n",
        "        gap_roi = ((c - prev_close) / prev_close) * 100.0\n",
        "\n",
        "        range_size = h - l\n",
        "        if range_size <= 0:\n",
        "            continue\n",
        "\n",
        "        side = None\n",
        "        trigger_level = None\n",
        "        sl_level = None\n",
        "\n",
        "        if gap_roi > 0:  # Gap up ‚Üí check inverted pin\n",
        "            lower_body = c - l\n",
        "            if (lower_body / range_size) <= 0.3:\n",
        "                side = \"SHORT\"\n",
        "                trigger_level = l\n",
        "                sl_level = h\n",
        "        elif gap_roi < 0:  # Gap down ‚Üí check hammer\n",
        "            upper_shadow = h - c\n",
        "            if (upper_shadow / range_size) <= 0.3:\n",
        "                side = \"LONG\"\n",
        "                trigger_level = h\n",
        "                sl_level = l\n",
        "\n",
        "        if side is None:\n",
        "            continue\n",
        "\n",
        "        # Get later minutes after 09:15\n",
        "        later_df = day_df.filter(pl.col(\"TradeTime\") > \"09:15\").sort(\"dt\")\n",
        "        if later_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        later_pd = later_df.select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        triggered = False\n",
        "        trigger_dt = None\n",
        "        trigger_time_str = None\n",
        "        entry_price = None\n",
        "        for _, row in later_pd.iterrows():\n",
        "            cur_close = row[\"Close\"]\n",
        "            cur_time_str = row[\"TradeTime\"]\n",
        "            cur_dt = row[\"dt\"]\n",
        "\n",
        "            if cur_time_str > ENTRY_CUTOFF_TIME:\n",
        "                break\n",
        "\n",
        "            if (side == \"SHORT\" and cur_close < trigger_level) or \\\n",
        "               (side == \"LONG\" and cur_close > trigger_level):\n",
        "                entry_price = cur_close\n",
        "                trigger_dt = cur_dt\n",
        "                trigger_time_str = cur_time_str\n",
        "                triggered = True\n",
        "                break\n",
        "\n",
        "        if not triggered:\n",
        "            continue\n",
        "\n",
        "        # Compute target_level\n",
        "        target_level = None\n",
        "        if side == \"LONG\":\n",
        "            candidates = [levels['PP'], levels['R1'], levels['R2'], levels['R3']]\n",
        "            upper = [lv for lv in candidates if lv > entry_price]\n",
        "            if upper:\n",
        "                target_level = min(upper)\n",
        "        else:  # SHORT\n",
        "            candidates = [levels['PP'], levels['S1'], levels['S2'], levels['S3']]\n",
        "            lower = [lv for lv in candidates if lv < entry_price]\n",
        "            if lower:\n",
        "                target_level = max(lower)\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"sl_level\": sl_level,\n",
        "            \"target_level\": target_level\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        sl_level = entry[\"sl_level\"]\n",
        "        target_level = entry[\"target_level\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # === FIXED SECTION START ===\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == signal_date).select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        # Correct boolean masking\n",
        "        mask = (day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)\n",
        "        day_prices = day_prices[mask].sort_values(\"dt\").reset_index(drop=True)\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        # Find trigger candle index\n",
        "        trigger_mask = day_prices[\"TradeTime\"] == trigger_time_str\n",
        "        if not trigger_mask.any():\n",
        "            continue\n",
        "        trigger_idx = day_prices[trigger_mask].index[0]\n",
        "\n",
        "        post_entry_prices = day_prices.iloc[trigger_idx + 1:].reset_index(drop=True)\n",
        "        # === FIXED SECTION END ===\n",
        "\n",
        "        if post_entry_prices.empty:\n",
        "            exit_price = entry_price\n",
        "            exit_reason = \"NO_POST_ENTRY_CANDLES\"\n",
        "            exit_dt = day_prices.iloc[trigger_idx][\"dt\"]\n",
        "        else:\n",
        "            exit_price = None\n",
        "            exit_reason = END_TIME\n",
        "            exit_dt = None\n",
        "            prev_is_sl_condition = False\n",
        "            exited = False\n",
        "\n",
        "            for _, minute_row in post_entry_prices.iterrows():\n",
        "                cur_close = minute_row[\"Close\"]\n",
        "                cur_time = minute_row[\"TradeTime\"]\n",
        "                cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "                # Check target hit\n",
        "                target_hit = False\n",
        "                if target_level is not None:\n",
        "                    if (side == \"LONG\" and cur_close > target_level) or \\\n",
        "                       (side == \"SHORT\" and cur_close < target_level):\n",
        "                        target_hit = True\n",
        "\n",
        "                # Check SL condition\n",
        "                is_sl_cond = (cur_close < sl_level) if side == \"LONG\" else (cur_close > sl_level)\n",
        "                hit_condition = is_sl_cond and prev_is_sl_condition\n",
        "                prev_is_sl_condition = is_sl_cond\n",
        "\n",
        "                if target_hit or hit_condition:\n",
        "                    exit_price = cur_close\n",
        "                    exit_dt = cur_dt\n",
        "                    if target_hit:\n",
        "                        exit_reason = f\"TARGET_{cur_time}\"\n",
        "                    else:\n",
        "                        exit_reason = f\"SL_{cur_time}\"\n",
        "                    exited = True\n",
        "                    break\n",
        "\n",
        "            if not exited:\n",
        "                end_time_mask = post_entry_prices[\"TradeTime\"] == END_TIME\n",
        "                if end_time_mask.any():\n",
        "                    exit_price = post_entry_prices[end_time_mask][\"Close\"].values[0]\n",
        "                    exit_dt = post_entry_prices[end_time_mask][\"dt\"].values[0]\n",
        "                else:\n",
        "                    exit_price = post_entry_prices[\"Close\"].iloc[-1]\n",
        "                    exit_dt = post_entry_prices[\"dt\"].iloc[-1]\n",
        "                    exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL\n",
        "        position_pnl = qty * (exit_price - entry_price) if side == \"LONG\" else qty * (entry_price - exit_price)\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, signal_date, side,\n",
        "            entry_price, qty, position_value, exit_price, position_pnl,\n",
        "            trade_roi_pct, portfolio_return_pct, cumulative_return_pct,\n",
        "            exit_reason, trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "# === Save Results ===\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# === Daily PnL ===\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEGqw2tqdqck",
        "outputId": "d724b006-002d-4a3b-b418-2b87d59598a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 521 cash files...\n",
            "Processed 50/521 symbols\n",
            "Processed 100/521 symbols\n",
            "Processed 150/521 symbols\n",
            "Processed 200/521 symbols\n",
            "Processed 250/521 symbols\n",
            "Processed 300/521 symbols\n",
            "Processed 350/521 symbols\n",
            "Processed 400/521 symbols\n",
            "Processed 450/521 symbols\n",
            "Processed 500/521 symbols\n",
            "Loaded 521 symbols with required times\n",
            "NIFTY500 file not found at nifty500_path. Please check path.\n",
            "Found 312 potential trade dates from symbol data\n",
            "Backtest completed. 1244 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "START_TIME = \"09:15\"           # Snapshot time for first candle\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding (not used currently)\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for prev_close)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)\n",
        "    if prev_trade_date is None:\n",
        "        continue\n",
        "\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for sym in symbol_full_data:\n",
        "        if sym not in symbol_close_start_end:\n",
        "            continue\n",
        "        d = symbol_close_start_end[sym]\n",
        "        try:\n",
        "            prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        # Pull full-day minute prices for signal_date\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        candle_0915 = day_df.filter(pl.col(\"TradeTime\") == \"09:15\")\n",
        "        if candle_0915.is_empty():\n",
        "            continue\n",
        "\n",
        "        o = float(candle_0915[\"Open\"][0])\n",
        "        h = float(candle_0915[\"High\"][0])\n",
        "        l = float(candle_0915[\"Low\"][0])\n",
        "        c = float(candle_0915[\"Close\"][0])\n",
        "        dt_0915 = candle_0915[\"dt\"][0]  # datetime of 09:15 candle\n",
        "\n",
        "        # Compute gap ROI\n",
        "        gap_roi = ((c - prev_close) / prev_close) * 100.0\n",
        "\n",
        "        range_size = h - l\n",
        "        if range_size <= 0:\n",
        "            continue\n",
        "\n",
        "        side = None\n",
        "        sl_level = None\n",
        "\n",
        "        if gap_roi > 0:  # Gap up ‚Üí check inverted pin (short)\n",
        "            lower_body = c - l\n",
        "            if (lower_body / range_size) <= 0.3:\n",
        "                side = \"SHORT\"\n",
        "                sl_level = h\n",
        "        elif gap_roi < 0:  # Gap down ‚Üí check hammer (long)\n",
        "            upper_shadow = h - c\n",
        "            if (upper_shadow / range_size) <= 0.3:\n",
        "                side = \"LONG\"\n",
        "                sl_level = l\n",
        "\n",
        "        if side is None:\n",
        "            continue\n",
        "\n",
        "        # === ENTRY AT 9:15 CLOSE ITSELF ===\n",
        "        entry_price = c  # Close of 09:15 candle\n",
        "        trigger_time_str = \"09:15\"\n",
        "        trigger_dt = dt_0915\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"sl_level\": sl_level\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time (all at 09:15, but keeps order deterministic)\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        sl_level = entry[\"sl_level\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # === FIXED SECTION START: Get post-entry prices from 09:16 onwards ===\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == signal_date).select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        # Start from 09:15 inclusive for SL check (since SL can trigger immediately after entry)\n",
        "        mask = (day_prices[\"TradeTime\"] >= \"09:15\") & (day_prices[\"TradeTime\"] <= END_TIME)\n",
        "        day_prices = day_prices[mask].sort_values(\"dt\").reset_index(drop=True)\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        # Find 09:15 candle index (entry candle)\n",
        "        trigger_mask = day_prices[\"TradeTime\"] == \"09:15\"\n",
        "        if not trigger_mask.any():\n",
        "            continue\n",
        "        trigger_idx = day_prices[trigger_mask].index[0]\n",
        "\n",
        "        # Post-entry: from next minute (09:16) onward\n",
        "        post_entry_prices = day_prices.iloc[trigger_idx + 1:].reset_index(drop=True)\n",
        "        # === FIXED SECTION END ===\n",
        "\n",
        "        if post_entry_prices.empty:\n",
        "            exit_price = entry_price\n",
        "            exit_reason = \"NO_POST_ENTRY_CANDLES\"\n",
        "            exit_dt = day_prices.iloc[trigger_idx][\"dt\"]\n",
        "        else:\n",
        "            exit_price = None\n",
        "            exit_reason = END_TIME\n",
        "            exit_dt = None\n",
        "            prev_is_sl_condition = False\n",
        "            sl_hit = False\n",
        "\n",
        "            for _, minute_row in post_entry_prices.iterrows():\n",
        "                cur_close = minute_row[\"Close\"]\n",
        "                cur_time = minute_row[\"TradeTime\"]\n",
        "                cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "                is_sl_cond = (cur_close < sl_level) if side == \"LONG\" else (cur_close > sl_level)\n",
        "                hit_condition = is_sl_cond and prev_is_sl_condition\n",
        "                prev_is_sl_condition = is_sl_cond\n",
        "\n",
        "                if hit_condition:\n",
        "                    exit_price = cur_close\n",
        "                    exit_reason = f\"SL_{cur_time}\"\n",
        "                    exit_dt = cur_dt\n",
        "                    sl_hit = True\n",
        "                    break\n",
        "\n",
        "            if not sl_hit:\n",
        "                end_time_mask = post_entry_prices[\"TradeTime\"] == END_TIME\n",
        "                if end_time_mask.any():\n",
        "                    exit_price = post_entry_prices[end_time_mask][\"Close\"].values[0]\n",
        "                    exit_dt = post_entry_prices[end_time_mask][\"dt\"].values[0]\n",
        "                else:\n",
        "                    exit_price = post_entry_prices[\"Close\"].iloc[-1]\n",
        "                    exit_dt = post_entry_prices[\"dt\"].iloc[-1]\n",
        "                    exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL\n",
        "        position_pnl = qty * (exit_price - entry_price) if side == \"LONG\" else qty * (entry_price - exit_price)\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, signal_date, side,\n",
        "            entry_price, qty, position_value, exit_price, position_pnl,\n",
        "            trade_roi_pct, portfolio_return_pct, cumulative_return_pct,\n",
        "            exit_reason, trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "# === Save Results ===\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# === Daily PnL ===\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AZqxUKLlkcm",
        "outputId": "17f61f85-5f64-4e16-b2f3-58dd4f42027f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 313 potential trade dates from symbol data\n",
            "Backtest completed. 1243 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "START_TIME = \"09:15\"           # Snapshot time for first candle\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding (not used currently)\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "symbol_daily_hlc = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Daily HLC\n",
        "    df_daily = df.group_by(pl.col(\"TradeDate\")).agg([\n",
        "        pl.col(\"High\").max().alias(\"DailyHigh\"),\n",
        "        pl.col(\"Low\").min().alias(\"DailyLow\"),\n",
        "        pl.col(\"Close\").last().alias(\"DailyClose\")\n",
        "    ]).sort(\"TradeDate\")\n",
        "    symbol_daily_hlc[symbol] = df_daily.select([\"TradeDate\", \"DailyHigh\", \"DailyLow\", \"DailyClose\"]).to_pandas().set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for prev_close, kept for compatibility)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily_hlc)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily_hlc.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)\n",
        "    if prev_trade_date is None:\n",
        "        continue\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible (optional, not used)\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[signal_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    # Get last 15 trading days back from signal_date\n",
        "    dates_back = []\n",
        "    current = signal_date\n",
        "    for _ in range(15):\n",
        "        current = get_prev_trading_day(current, unique_trade_dates)\n",
        "        if current is None:\n",
        "            break\n",
        "        dates_back.append(current)\n",
        "\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for sym in symbol_full_data:\n",
        "        if sym not in symbol_daily_hlc:\n",
        "            continue\n",
        "        d_hlc = symbol_daily_hlc[sym]\n",
        "        try:\n",
        "            prev_close = float(d_hlc.loc[prev_trade_date, \"DailyClose\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        # Get HLC back for this sym\n",
        "        try:\n",
        "            hlc_back = d_hlc.loc[dates_back]\n",
        "            if len(hlc_back) == 0:\n",
        "                continue\n",
        "            avg_h = hlc_back[\"DailyHigh\"].mean()\n",
        "            avg_l = hlc_back[\"DailyLow\"].mean()\n",
        "            avg_c = hlc_back[\"DailyClose\"].mean()\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        # Compute Fibonacci Pivot Levels\n",
        "        pp = (avg_h + avg_l + avg_c) / 3\n",
        "        pivot_range = avg_h - avg_l\n",
        "        levels = {\n",
        "            'PP': pp,\n",
        "            'R1': pp + 0.382 * pivot_range,\n",
        "            'R2': pp + 0.618 * pivot_range,\n",
        "            'R3': pp + 1.0 * pivot_range,\n",
        "            'S1': pp - 0.382 * pivot_range,\n",
        "            'S2': pp - 0.618 * pivot_range,\n",
        "            'S3': pp - 1.0 * pivot_range,\n",
        "        }\n",
        "\n",
        "        # Pull full-day minute prices for signal_date\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        candle_0915 = day_df.filter(pl.col(\"TradeTime\") == \"09:15\")\n",
        "        if candle_0915.is_empty():\n",
        "            continue\n",
        "\n",
        "        o = float(candle_0915[\"Open\"][0])\n",
        "        h = float(candle_0915[\"High\"][0])\n",
        "        l = float(candle_0915[\"Low\"][0])\n",
        "        c = float(candle_0915[\"Close\"][0])\n",
        "        trigger_dt = candle_0915[\"dt\"][0]\n",
        "\n",
        "        # Compute gap ROI\n",
        "        gap_roi = ((c - prev_close) / prev_close) * 100.0\n",
        "\n",
        "        range_size = h - l\n",
        "        if range_size <= 0:\n",
        "            continue\n",
        "\n",
        "        side = None\n",
        "        trigger_level = None\n",
        "        sl_level = None\n",
        "\n",
        "        if gap_roi > 0:  # Gap up ‚Üí check inverted pin\n",
        "            lower_body = c - l\n",
        "            if (lower_body / range_size) <= 0.3:\n",
        "                side = \"SHORT\"\n",
        "                trigger_level = l\n",
        "                sl_level = h\n",
        "        elif gap_roi < 0:  # Gap down ‚Üí check hammer\n",
        "            upper_shadow = h - c\n",
        "            if (upper_shadow / range_size) <= 0.3:\n",
        "                side = \"LONG\"\n",
        "                trigger_level = h\n",
        "                sl_level = l\n",
        "\n",
        "        if side is None:\n",
        "            continue\n",
        "\n",
        "        entry_price = c\n",
        "        trigger_time_str = \"09:15\"\n",
        "\n",
        "        # Compute target_level\n",
        "        target_level = None\n",
        "        if side == \"LONG\":\n",
        "            candidates = [levels['PP'], levels['R1'], levels['R2'], levels['R3']]\n",
        "            upper = [lv for lv in candidates if lv > entry_price]\n",
        "            if upper:\n",
        "                target_level = min(upper)\n",
        "        else:  # SHORT\n",
        "            candidates = [levels['PP'], levels['S1'], levels['S2'], levels['S3']]\n",
        "            lower = [lv for lv in candidates if lv < entry_price]\n",
        "            if lower:\n",
        "                target_level = max(lower)\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"sl_level\": sl_level,\n",
        "            \"target_level\": target_level\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        sl_level = entry[\"sl_level\"]\n",
        "        target_level = entry[\"target_level\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # === FIXED SECTION START ===\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == signal_date).select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        # Correct boolean masking\n",
        "        mask = (day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)\n",
        "        day_prices = day_prices[mask].sort_values(\"dt\").reset_index(drop=True)\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        # Find trigger candle index\n",
        "        trigger_mask = day_prices[\"TradeTime\"] == trigger_time_str\n",
        "        if not trigger_mask.any():\n",
        "            continue\n",
        "        trigger_idx = day_prices[trigger_mask].index[0]\n",
        "\n",
        "        post_entry_prices = day_prices.iloc[trigger_idx + 1:].reset_index(drop=True)\n",
        "        # === FIXED SECTION END ===\n",
        "\n",
        "        if post_entry_prices.empty:\n",
        "            exit_price = entry_price\n",
        "            exit_reason = \"NO_POST_ENTRY_CANDLES\"\n",
        "            exit_dt = day_prices.iloc[trigger_idx][\"dt\"]\n",
        "        else:\n",
        "            exit_price = None\n",
        "            exit_reason = END_TIME\n",
        "            exit_dt = None\n",
        "            prev_is_sl_condition = False\n",
        "            exited = False\n",
        "\n",
        "            for _, minute_row in post_entry_prices.iterrows():\n",
        "                cur_close = minute_row[\"Close\"]\n",
        "                cur_time = minute_row[\"TradeTime\"]\n",
        "                cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "                # Check target hit\n",
        "                target_hit = False\n",
        "                if target_level is not None:\n",
        "                    if (side == \"LONG\" and cur_close > target_level) or \\\n",
        "                       (side == \"SHORT\" and cur_close < target_level):\n",
        "                        target_hit = True\n",
        "\n",
        "                # Check SL condition\n",
        "                is_sl_cond = (cur_close < sl_level) if side == \"LONG\" else (cur_close > sl_level)\n",
        "                hit_condition = is_sl_cond and prev_is_sl_condition\n",
        "                prev_is_sl_condition = is_sl_cond\n",
        "\n",
        "                if target_hit or hit_condition:\n",
        "                    exit_price = cur_close\n",
        "                    exit_dt = cur_dt\n",
        "                    if target_hit:\n",
        "                        exit_reason = f\"TARGET_{cur_time}\"\n",
        "                    else:\n",
        "                        exit_reason = f\"SL_{cur_time}\"\n",
        "                    exited = True\n",
        "                    break\n",
        "\n",
        "            if not exited:\n",
        "                end_time_mask = post_entry_prices[\"TradeTime\"] == END_TIME\n",
        "                if end_time_mask.any():\n",
        "                    exit_price = post_entry_prices[end_time_mask][\"Close\"].values[0]\n",
        "                    exit_dt = post_entry_prices[end_time_mask][\"dt\"].values[0]\n",
        "                else:\n",
        "                    exit_price = post_entry_prices[\"Close\"].iloc[-1]\n",
        "                    exit_dt = post_entry_prices[\"dt\"].iloc[-1]\n",
        "                    exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL\n",
        "        position_pnl = qty * (exit_price - entry_price) if side == \"LONG\" else qty * (entry_price - exit_price)\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, signal_date, side,\n",
        "            entry_price, qty, position_value, exit_price, position_pnl,\n",
        "            trade_roi_pct, portfolio_return_pct, cumulative_return_pct,\n",
        "            exit_reason, trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "# === Save Results ===\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# === Daily PnL ===\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nllh984127v",
        "outputId": "68f58d25-e524-4f24-97ab-d01faac69308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 313 potential trade dates from symbol data\n",
            "Backtest completed. 1243 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "START_TIME = \"09:15\"           # Snapshot time for first candle\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding (not used currently)\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "symbol_daily_hlc = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Daily HLC\n",
        "    df_daily = df.group_by(pl.col(\"TradeDate\")).agg([\n",
        "        pl.col(\"High\").max().alias(\"DailyHigh\"),\n",
        "        pl.col(\"Low\").min().alias(\"DailyLow\"),\n",
        "        pl.col(\"Close\").last().alias(\"DailyClose\")\n",
        "    ]).sort(\"TradeDate\")\n",
        "    symbol_daily_hlc[symbol] = df_daily.select([\"TradeDate\", \"DailyHigh\", \"DailyLow\", \"DailyClose\"]).to_pandas().set_index(\"TradeDate\").sort_index()\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for prev_close, kept for compatibility)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_daily_hlc)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_daily_hlc.items():\n",
        "    all_dates.update(d.index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Backtest/execution loop with candle trigger simulation -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)\n",
        "    if prev_trade_date is None:\n",
        "        continue\n",
        "\n",
        "    # Compute NIFTY500 ROI for this date if possible (optional, not used)\n",
        "    nifty_roi_for_date = None\n",
        "    if nifty500_close_1529 is not None and nifty500_close_start is not None:\n",
        "        try:\n",
        "            nifty_prev = float(nifty500_close_1529.loc[prev_trade_date])\n",
        "            nifty_start = float(nifty500_close_start.loc[signal_date])\n",
        "            if nifty_prev != 0:\n",
        "                nifty_roi_for_date = ((nifty_start - nifty_prev) / nifty_prev) * 100.0\n",
        "        except Exception:\n",
        "            nifty_roi_for_date = None\n",
        "\n",
        "    # Get last 15 trading days back from signal_date\n",
        "    dates_back = []\n",
        "    current = signal_date\n",
        "    for _ in range(15):\n",
        "        current = get_prev_trading_day(current, unique_trade_dates)\n",
        "        if current is None:\n",
        "            break\n",
        "        dates_back.append(current)\n",
        "\n",
        "    # For each day, collect potential entries with their trigger times\n",
        "    potential_entries = []\n",
        "\n",
        "    for sym in symbol_full_data:\n",
        "        if sym not in symbol_daily_hlc:\n",
        "            continue\n",
        "        d_hlc = symbol_daily_hlc[sym]\n",
        "        try:\n",
        "            prev_close = float(d_hlc.loc[prev_trade_date, \"DailyClose\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        # Get HLC back for this sym\n",
        "        try:\n",
        "            hlc_back = d_hlc.loc[dates_back]\n",
        "            if len(hlc_back) == 0:\n",
        "                continue\n",
        "            avg_h = hlc_back[\"DailyHigh\"].mean()\n",
        "            avg_l = hlc_back[\"DailyLow\"].mean()\n",
        "            avg_c = hlc_back[\"DailyClose\"].mean()\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        # Compute Fibonacci Pivot Levels\n",
        "        pp = (avg_h + avg_l + avg_c) / 3\n",
        "        pivot_range = avg_h - avg_l\n",
        "        levels = {\n",
        "            'PP': pp,\n",
        "            'R1': pp + 0.382 * pivot_range,\n",
        "            'R2': pp + 0.618 * pivot_range,\n",
        "            'R3': pp + 1.0 * pivot_range,\n",
        "            'S1': pp - 0.382 * pivot_range,\n",
        "            'S2': pp - 0.618 * pivot_range,\n",
        "            'S3': pp - 1.0 * pivot_range,\n",
        "        }\n",
        "\n",
        "        # Pull full-day minute prices for signal_date\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        candle_0915 = day_df.filter(pl.col(\"TradeTime\") == \"09:15\")\n",
        "        if candle_0915.is_empty():\n",
        "            continue\n",
        "\n",
        "        o = float(candle_0915[\"Open\"][0])\n",
        "        h = float(candle_0915[\"High\"][0])\n",
        "        l = float(candle_0915[\"Low\"][0])\n",
        "        c = float(candle_0915[\"Close\"][0])\n",
        "        dt_0915 = candle_0915[\"dt\"][0]\n",
        "\n",
        "        # Compute gap ROI\n",
        "        gap_roi = ((c - prev_close) / prev_close) * 100.0\n",
        "\n",
        "        range_size = h - l\n",
        "        if range_size <= 0:\n",
        "            continue\n",
        "\n",
        "        side = None\n",
        "        sl_level = None\n",
        "\n",
        "        if gap_roi > 0:  # Gap up ‚Üí check inverted pin\n",
        "            lower_body = c - l\n",
        "            if (lower_body / range_size) <= 0.3:\n",
        "                side = \"SHORT\"\n",
        "                sl_level = c + (range_size / 2)\n",
        "        elif gap_roi < 0:  # Gap down ‚Üí check hammer\n",
        "            upper_shadow = h - c\n",
        "            if (upper_shadow / range_size) <= 0.3:\n",
        "                side = \"LONG\"\n",
        "                sl_level = c - (range_size / 2)\n",
        "\n",
        "        if side is None:\n",
        "            continue\n",
        "\n",
        "        entry_price = c\n",
        "        trigger_dt = dt_0915\n",
        "        trigger_time_str = \"09:15\"\n",
        "\n",
        "        # Compute target_level\n",
        "        target_level = None\n",
        "        if side == \"LONG\":\n",
        "            candidates = [levels['PP'], levels['R1'], levels['R2'], levels['R3']]\n",
        "            upper = [lv for lv in candidates if lv > entry_price]\n",
        "            if upper:\n",
        "                target_level = min(upper)\n",
        "        else:  # SHORT\n",
        "            candidates = [levels['PP'], levels['S1'], levels['S2'], levels['S3']]\n",
        "            lower = [lv for lv in candidates if lv < entry_price]\n",
        "            if lower:\n",
        "                target_level = max(lower)\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str,\n",
        "            \"sl_level\": sl_level,\n",
        "            \"target_level\": target_level\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "        sl_level = entry[\"sl_level\"]\n",
        "        target_level = entry[\"target_level\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # === FIXED SECTION START ===\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == signal_date).select([\"dt\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "\n",
        "        # Correct boolean masking\n",
        "        mask = (day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)\n",
        "        day_prices = day_prices[mask].sort_values(\"dt\").reset_index(drop=True)\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        # Find trigger candle index\n",
        "        trigger_mask = day_prices[\"TradeTime\"] == trigger_time_str\n",
        "        if not trigger_mask.any():\n",
        "            continue\n",
        "        trigger_idx = day_prices[trigger_mask].index[0]\n",
        "\n",
        "        post_entry_prices = day_prices.iloc[trigger_idx + 1:].reset_index(drop=True)\n",
        "        # === FIXED SECTION END ===\n",
        "\n",
        "        if post_entry_prices.empty:\n",
        "            exit_price = entry_price\n",
        "            exit_reason = \"NO_POST_ENTRY_CANDLES\"\n",
        "            exit_dt = day_prices.iloc[trigger_idx][\"dt\"]\n",
        "        else:\n",
        "            exit_price = None\n",
        "            exit_reason = END_TIME\n",
        "            exit_dt = None\n",
        "            prev_is_sl_condition = False\n",
        "            exited = False\n",
        "\n",
        "            for _, minute_row in post_entry_prices.iterrows():\n",
        "                cur_close = minute_row[\"Close\"]\n",
        "                cur_time = minute_row[\"TradeTime\"]\n",
        "                cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "                # Check target hit\n",
        "                target_hit = False\n",
        "                if target_level is not None:\n",
        "                    if (side == \"LONG\" and cur_close > target_level) or \\\n",
        "                       (side == \"SHORT\" and cur_close < target_level):\n",
        "                        target_hit = True\n",
        "\n",
        "                # Check SL condition\n",
        "                is_sl_cond = (cur_close < sl_level) if side == \"LONG\" else (cur_close > sl_level)\n",
        "                hit_condition = is_sl_cond and prev_is_sl_condition\n",
        "                prev_is_sl_condition = is_sl_cond\n",
        "\n",
        "                if target_hit or hit_condition:\n",
        "                    exit_price = cur_close\n",
        "                    exit_dt = cur_dt\n",
        "                    if target_hit:\n",
        "                        exit_reason = f\"TARGET_{cur_time}\"\n",
        "                    else:\n",
        "                        exit_reason = f\"SL_{cur_time}\"\n",
        "                    exited = True\n",
        "                    break\n",
        "\n",
        "            if not exited:\n",
        "                end_time_mask = post_entry_prices[\"TradeTime\"] == END_TIME\n",
        "                if end_time_mask.any():\n",
        "                    exit_price = post_entry_prices[end_time_mask][\"Close\"].values[0]\n",
        "                    exit_dt = post_entry_prices[end_time_mask][\"dt\"].values[0]\n",
        "                else:\n",
        "                    exit_price = post_entry_prices[\"Close\"].iloc[-1]\n",
        "                    exit_dt = post_entry_prices[\"dt\"].iloc[-1]\n",
        "                    exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL\n",
        "        position_pnl = qty * (exit_price - entry_price) if side == \"LONG\" else qty * (entry_price - exit_price)\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, signal_date, side,\n",
        "            entry_price, qty, position_value, exit_price, position_pnl,\n",
        "            trade_roi_pct, portfolio_return_pct, cumulative_return_pct,\n",
        "            exit_reason, trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "# === Save Results ===\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# === Daily PnL ===\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLvhQiy8OduW"
      },
      "source": [
        "With trail sl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wW43pIg6yWm",
        "outputId": "0db52210-2639-4556-8ebc-aa0e6fcce077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Found 521 cash files...\n",
            "‚úÖ Processed 50/521 symbols\n",
            "‚úÖ Processed 100/521 symbols\n",
            "‚úÖ Processed 150/521 symbols\n",
            "‚úÖ Processed 200/521 symbols\n",
            "‚úÖ Processed 250/521 symbols\n",
            "‚úÖ Processed 300/521 symbols\n",
            "‚úÖ Processed 350/521 symbols\n",
            "‚úÖ Processed 400/521 symbols\n",
            "‚úÖ Processed 450/521 symbols\n",
            "‚úÖ Processed 500/521 symbols\n",
            "‚úÖ Loaded 521 symbols with required times\n",
            "‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\n",
            "‚úÖ Found 312 potential trade dates from symbol data\n",
            "Backtest completed. 1244 trades executed.\n",
            "Executed trades saved in: OUTPUT_BACKTEST.csv\n",
            "Daily PnL summary saved in: DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "# ‚úÖ User-configurable SL/Target params\n",
        "START_TIME = \"09:15\"           # Snapshot time for first candle\n",
        "SL_ACTIVATION_TIME = \"09:15\"   # SL activation immediate, but set to start\n",
        "END_TIME = \"15:15\"             # Trade exit cutoff\n",
        "ENTRY_CUTOFF_TIME = \"15:15\"    # No entries after this\n",
        "CAPITAL = 50000.0              # Account capital\n",
        "LEVERAGE = 2.5                 # Leverage factor\n",
        "MAX_POSITIONS = 4              # Max open positions\n",
        "TICK_SIZE = 0.05               # Assume default tick size for rounding (not used currently)\n",
        "\n",
        "# ‚úÖ Path with many cash CSV files\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üöÄ Found {len(all_files)} cash files...\")\n",
        "\n",
        "# Path to NIFTY 500 cash file (optional, not used in selection)\n",
        "nifty500_path = \"/content/drive/MyDrive/Cash_data/cash_NIFTY 500.csv\"\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    \"\"\"Read CSV with polars and return (symbol, dataframe with dt, TradeDate, TradeTime).\"\"\"\n",
        "    symbol = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df = pl.read_csv(\n",
        "        file_path,\n",
        "        try_parse_dates=False,\n",
        "        low_memory=True,\n",
        "    ).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "\n",
        "    # Keep first 19 chars to ensure no fractional seconds etc\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\")\n",
        "    )\n",
        "\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "\n",
        "    return symbol, df\n",
        "\n",
        "# Load all symbols into memory\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    # Select rows for START_TIME and 15:29 (for prev_close)\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        # 15:29 closes: indexed by TradeDate\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        # start time close (09:15): indexed by TradeDate\n",
        "        close_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"close_start\": close_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(symbol_close_start_end)} symbols with required times\")\n",
        "\n",
        "# --- Load NIFTY500 series (optional) ---\n",
        "nifty500_close_1529 = None\n",
        "nifty500_close_start = None\n",
        "if os.path.exists(nifty500_path):\n",
        "    nifty_sym, nifty_df = load_full_data(nifty500_path)\n",
        "    nifty_sel = nifty_df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not nifty_sel.is_empty():\n",
        "        nifty_pdf = nifty_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        nifty500_close_1529 = nifty_pdf[nifty_pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        nifty500_close_start = nifty_pdf[nifty_pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"].sort_index()\n",
        "        print(\"‚úÖ Loaded NIFTY500 reference series\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NIFTY500 file found but didn't contain required times\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NIFTY500 file not found at nifty500_path. Please check path.\")\n",
        "\n",
        "# Build list of unique trade dates from all symbols' prev-close indices\n",
        "all_dates = set()\n",
        "for sym, d in symbol_close_start_end.items():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"‚úÖ Found {len(unique_trade_dates)} potential trade dates from symbol data\")\n",
        "\n",
        "# Helper function to get the previous trading day\n",
        "def get_prev_trading_day(trade_date, all_dates):\n",
        "    \"\"\"Return the previous trading day from all_dates, assuming trade_date is a pandas Timestamp.\"\"\"\n",
        "    trade_date = pd.Timestamp(trade_date)\n",
        "    prev_dates = [d for d in all_dates if pd.Timestamp(d) < trade_date]\n",
        "    if not prev_dates:\n",
        "        return None\n",
        "    return max(prev_dates)\n",
        "\n",
        "# ----- Backtest/execution loop with immediate entry at 09:15 close -----\n",
        "output_trades = []\n",
        "cumulative_portfolio_pnl = 0.0\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "for signal_date in unique_trade_dates:\n",
        "    # Get previous trading day\n",
        "    prev_trade_date = get_prev_trading_day(signal_date, unique_trade_dates)\n",
        "    if prev_trade_date is None:\n",
        "        continue\n",
        "\n",
        "    # For each day, collect potential entries at 09:15 close\n",
        "    potential_entries = []\n",
        "\n",
        "    for sym in symbol_full_data:\n",
        "        if sym not in symbol_close_start_end:\n",
        "            continue\n",
        "        d = symbol_close_start_end[sym]\n",
        "        try:\n",
        "            prev_close = float(d[\"close_1529\"].loc[prev_trade_date])\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if prev_close == 0:\n",
        "            continue\n",
        "\n",
        "        # Pull full-day minute prices for signal_date\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter((pl.col(\"TradeDate\") == signal_date) & (pl.col(\"TradeTime\") >= \"09:15\") & (pl.col(\"TradeTime\") <= END_TIME)).sort(\"dt\")\n",
        "        if day_df.is_empty():\n",
        "            continue\n",
        "\n",
        "        # Get 09:15 candle\n",
        "        candle_0915 = day_df.filter(pl.col(\"TradeTime\") == \"09:15\")\n",
        "        if candle_0915.is_empty():\n",
        "            continue\n",
        "\n",
        "        o = float(candle_0915[\"Open\"][0])\n",
        "        h = float(candle_0915[\"High\"][0])\n",
        "        l = float(candle_0915[\"Low\"][0])\n",
        "        c = float(candle_0915[\"Close\"][0])\n",
        "\n",
        "        # Compute gap ROI\n",
        "        gap_roi = ((c - prev_close) / prev_close) * 100.0\n",
        "\n",
        "        range_size = h - l\n",
        "        if range_size <= 0:\n",
        "            continue\n",
        "\n",
        "        side = None\n",
        "\n",
        "        if gap_roi > 0:  # Gap up ‚Üí check inverted pin\n",
        "            lower_body = c - l\n",
        "            if (lower_body / range_size) <= 0.3:\n",
        "                side = \"SHORT\"\n",
        "        elif gap_roi < 0:  # Gap down ‚Üí check hammer\n",
        "            upper_shadow = h - c\n",
        "            if (upper_shadow / range_size) <= 0.3:\n",
        "                side = \"LONG\"\n",
        "\n",
        "        if side is None:\n",
        "            continue\n",
        "\n",
        "        # Entry immediately at 09:15 close\n",
        "        entry_price = c\n",
        "\n",
        "        # Get entry time and datetime\n",
        "        entry_row = candle_0915.select([\"dt\", \"TradeTime\"]).to_pandas().iloc[0]\n",
        "        trigger_dt = entry_row[\"dt\"]\n",
        "        trigger_time_str = entry_row[\"TradeTime\"]\n",
        "\n",
        "        # Skip if entry time after cutoff\n",
        "        if trigger_time_str > ENTRY_CUTOFF_TIME:\n",
        "            continue\n",
        "\n",
        "        potential_entries.append({\n",
        "            \"trigger_dt\": trigger_dt,\n",
        "            \"symbol\": sym,\n",
        "            \"side\": side,\n",
        "            \"entry_price\": entry_price,\n",
        "            \"trigger_time_str\": trigger_time_str\n",
        "        })\n",
        "\n",
        "    # Sort potential entries by trigger time (all at 09:15)\n",
        "    potential_entries.sort(key=lambda x: x[\"trigger_dt\"])\n",
        "\n",
        "    # Simulate entries up to MAX_POSITIONS\n",
        "    entered_count = 0\n",
        "\n",
        "    for entry in potential_entries:\n",
        "        if entered_count >= MAX_POSITIONS:\n",
        "            break\n",
        "\n",
        "        sym = entry[\"symbol\"]\n",
        "        side = entry[\"side\"]\n",
        "        entry_price = entry[\"entry_price\"]\n",
        "        trigger_time_str = entry[\"trigger_time_str\"]\n",
        "\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "\n",
        "        position_value = qty * entry_price\n",
        "\n",
        "        # Get day prices including High and Low for trailing SL\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_prices = df_full.filter(pl.col(\"TradeDate\") == signal_date).select([\"dt\", \"TradeTime\", \"Close\", \"High\", \"Low\"]).to_pandas()\n",
        "\n",
        "        # Mask from entry time onwards\n",
        "        mask = (day_prices[\"TradeTime\"] >= trigger_time_str) & (day_prices[\"TradeTime\"] <= END_TIME)\n",
        "        day_prices = day_prices[mask].sort_values(\"dt\").reset_index(drop=True)\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        # Find entry candle index (09:15)\n",
        "        trigger_mask = day_prices[\"TradeTime\"] == trigger_time_str\n",
        "        if not trigger_mask.any():\n",
        "            continue\n",
        "        trigger_idx = day_prices[trigger_mask].index[0]\n",
        "\n",
        "        # Post-entry prices: starting from next minute (09:16+)\n",
        "        post_entry_prices = day_prices.iloc[trigger_idx + 1:].reset_index(drop=True)\n",
        "\n",
        "        if post_entry_prices.empty:\n",
        "            exit_price = entry_price\n",
        "            exit_reason = \"NO_POST_ENTRY_CANDLES\"\n",
        "            exit_dt = day_prices.iloc[trigger_idx][\"dt\"]\n",
        "        else:\n",
        "            exit_price = None\n",
        "            exit_reason = END_TIME\n",
        "            exit_dt = None\n",
        "            sl_hit = False\n",
        "            current_sl = entry_price  # Initial SL at entry price\n",
        "\n",
        "            for _, minute_row in post_entry_prices.iterrows():\n",
        "                cur_close = minute_row[\"Close\"]\n",
        "                cur_high = minute_row[\"High\"]\n",
        "                cur_low = minute_row[\"Low\"]\n",
        "                cur_time = minute_row[\"TradeTime\"]\n",
        "                cur_dt = minute_row[\"dt\"]\n",
        "\n",
        "                # Check initial/trailing SL hit on close\n",
        "                sl_hit_condition = (side == \"SHORT\" and cur_close > current_sl) or (side == \"LONG\" and cur_close < current_sl)\n",
        "                if sl_hit_condition:\n",
        "                    exit_price = cur_close\n",
        "                    exit_reason = f\"SL_{cur_time}\"\n",
        "                    exit_dt = cur_dt\n",
        "                    sl_hit = True\n",
        "                    break\n",
        "                else:\n",
        "                    # Favorable close: update trailing SL to this candle's extreme\n",
        "                    if side == \"SHORT\":\n",
        "                        current_sl = cur_high\n",
        "                    else:  # LONG\n",
        "                        current_sl = cur_low\n",
        "\n",
        "            if not sl_hit:\n",
        "                # Exit at END_TIME if available\n",
        "                end_time_mask = post_entry_prices[\"TradeTime\"] == END_TIME\n",
        "                if end_time_mask.any():\n",
        "                    exit_price = post_entry_prices[end_time_mask][\"Close\"].values[0]\n",
        "                    exit_dt = post_entry_prices[end_time_mask][\"dt\"].values[0]\n",
        "                else:\n",
        "                    exit_price = post_entry_prices[\"Close\"].iloc[-1]\n",
        "                    exit_dt = post_entry_prices[\"dt\"].iloc[-1]\n",
        "                    exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        # Compute PnL\n",
        "        position_pnl = qty * (exit_price - entry_price) if side == \"LONG\" else qty * (entry_price - exit_price)\n",
        "        trade_roi_pct = (position_pnl / position_value) * 100 if position_value > 0 else 0\n",
        "        portfolio_return_pct = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_portfolio_pnl += position_pnl\n",
        "        cumulative_return_pct = (cumulative_portfolio_pnl / CAPITAL) * 100\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, signal_date, side,\n",
        "            entry_price, qty, position_value, exit_price, position_pnl,\n",
        "            trade_roi_pct, portfolio_return_pct, cumulative_return_pct,\n",
        "            exit_reason, trigger_time_str\n",
        "        ])\n",
        "\n",
        "        entered_count += 1\n",
        "\n",
        "# === Save Results ===\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "                         columns=[\"SYMBOL\", \"SIGNAL_DATE\", \"TRADE_DATE\", \"SIDE\",\n",
        "                                  \"ENTRY_PRICE\", \"QTY\", \"POSITION_VALUE\", \"EXIT_PRICE\", \"POSITION_PNL\",\n",
        "                                  \"TRADE_ROI%\", \"PORTFOLIO_RETURN%\", \"CUMULATIVE_PORTFOLIO_RETURN%\",\n",
        "                                  \"EXIT_REASON\", \"ENTRY_TIME\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"Backtest completed. {len(output_df)} trades executed.\")\n",
        "print(\"Executed trades saved in: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# === Daily PnL ===\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_RETURN%\"] = daily_pnl_df[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Daily PnL summary saved in: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades found, skipping Daily PnL sheet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiRMadq_WvSB"
      },
      "source": [
        "# new strat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NHs46JRBvJJ",
        "outputId": "1c7f8a6f-686b-4aa5-df54-6234a96da5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 521 cash files...\n",
            "Loading cash data and extracting symbols...\n",
            "Processed 50/521 files ‚Üí symbol: CHOLAFIN\n",
            "Processed 100/521 files ‚Üí symbol: ASHOKLEY\n",
            "Processed 150/521 files ‚Üí symbol: IGL\n",
            "Processed 200/521 files ‚Üí symbol: CASTROLIND\n",
            "Processed 250/521 files ‚Üí symbol: PATANJALI\n",
            "Processed 300/521 files ‚Üí symbol: GESHIP\n",
            "Processed 350/521 files ‚Üí symbol: BERGEPAINT\n",
            "Processed 400/521 files ‚Üí symbol: PFC\n",
            "Processed 450/521 files ‚Üí symbol: BDL\n",
            "Processed 500/521 files ‚Üí symbol: INDIANB\n",
            "Loaded 521 symbols with 09:15 & 15:29 data\n",
            "Found 312 unique trading days\n",
            "\n",
            "Loading signal file...\n",
            "Loaded 0 signal entries\n",
            "Sample signals:\n",
            "Empty DataFrame\n",
            "Columns: [symbol, signal_date]\n",
            "Index: []\n",
            "Unique symbols in signal file: 0\n",
            "\n",
            "Will check 0 entry dates for gap-up SHORTs\n",
            "\n",
            "Starting backtest...\n",
            "============================================================\n",
            "\n",
            "BACKTEST COMPLETE!\n",
            "Total SHORT trades: 0\n",
            "Saved: OUTPUT_BACKTEST.csv\n",
            "No trades executed.\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================= CONFIG =============================\n",
        "INDIVIDUAL_SL_PCT = 0.004      # 0.4% SL\n",
        "START_TIME = \"09:15\"\n",
        "SL_ACTIVATION_TIME = \"09:30\"\n",
        "END_TIME = \"15:20\"\n",
        "CAPITAL = 50000.0\n",
        "LEVERAGE = 2.5\n",
        "MAX_POSITIONS = 4\n",
        "PER_STOCK_ALLOC = CAPITAL * LEVERAGE / MAX_POSITIONS\n",
        "\n",
        "# ============================= PATHS =============================\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"Found {len(all_files)} cash files...\")\n",
        "\n",
        "signal_path = \"/content/filtered_fno_symbols_all_dates.csv\"\n",
        "\n",
        "# ============================= LOAD DATA FUNCTION =============================\n",
        "def load_full_data(file_path):\n",
        "    # Extract symbol: remove \"cash_\" prefix and .csv\n",
        "    filename = os.path.basename(file_path)\n",
        "    symbol = filename.replace(\"cash_\", \"\").replace(\".csv\", \"\").strip()\n",
        "\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True)\n",
        "    df = df.rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "    df = df.with_columns(pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"))\n",
        "    df = df.with_columns(pl.col(\"ts_clean\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(\"dt\"))\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "    return symbol, df\n",
        "\n",
        "# ============================= LOAD ALL SYMBOLS =============================\n",
        "symbol_full_data = {}\n",
        "symbol_close_start_end = {}\n",
        "\n",
        "print(\"Loading cash data and extracting symbols...\")\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    symbol, df = load_full_data(f)\n",
        "    symbol_full_data[symbol] = df\n",
        "\n",
        "    df_sel = df.filter(pl.col(\"TradeTime\").is_in([START_TIME, \"15:29\"]))\n",
        "    if not df_sel.is_empty():\n",
        "        pdf = df_sel.select([\"TradeDate\", \"TradeTime\", \"Close\"]).to_pandas()\n",
        "        close_1529 = pdf[pdf[\"TradeTime\"] == \"15:29\"].set_index(\"TradeDate\")[\"Close\"]\n",
        "        open_start = pdf[pdf[\"TradeTime\"] == START_TIME].set_index(\"TradeDate\")[\"Close\"]\n",
        "        symbol_close_start_end[symbol] = {\"close_1529\": close_1529, \"open_start\": open_start}\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Processed {i}/{len(all_files)} files ‚Üí symbol: {symbol}\")\n",
        "\n",
        "print(f\"Loaded {len(symbol_close_start_end)} symbols with 09:15 & 15:29 data\")\n",
        "\n",
        "# ============================= TRADE DATES =============================\n",
        "all_dates = set()\n",
        "for d in symbol_close_start_end.values():\n",
        "    all_dates.update(d[\"close_1529\"].index)\n",
        "    all_dates.update(d[\"open_start\"].index)\n",
        "unique_trade_dates = sorted(all_dates)\n",
        "print(f\"Found {len(unique_trade_dates)} unique trading days\")\n",
        "\n",
        "def get_prev_trading_day(date, dates_list):\n",
        "    date = pd.Timestamp(date)\n",
        "    prev = [d for d in dates_list if pd.Timestamp(d) < date]\n",
        "    return max(prev) if prev else None\n",
        "\n",
        "def get_next_trading_day(date, dates_list):\n",
        "    date = pd.Timestamp(date)\n",
        "    nxt = [d for d in dates_list if pd.Timestamp(d) > date]\n",
        "    return min(nxt) if nxt else None\n",
        "\n",
        "# ============================= LOAD SIGNAL FILE (FIXED) =============================\n",
        "print(\"\\nLoading signal file...\")\n",
        "signal_df = pd.read_csv(\n",
        "    signal_path,\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    names=['date_full', 'symbol', 'marketcapname', 'sector', 'date_only'],\n",
        "    dtype={'symbol': str}\n",
        ")\n",
        "\n",
        "# Clean symbol column\n",
        "signal_df['symbol'] = signal_df['symbol'].str.strip()\n",
        "\n",
        "# Parse date_only (dd-mm-yy)\n",
        "signal_df['signal_date'] = pd.to_datetime(signal_df['date_only'], format='%d-%m-%y', errors='coerce')\n",
        "signal_df = signal_df.dropna(subset=['signal_date'])\n",
        "signal_df = signal_df.drop_duplicates(subset=['symbol', 'signal_date'])\n",
        "\n",
        "print(f\"Loaded {len(signal_df)} signal entries\")\n",
        "print(f\"Sample signals:\\n{signal_df[['symbol', 'signal_date']].head(10)}\")\n",
        "print(f\"Unique symbols in signal file: {signal_df['symbol'].nunique()}\")\n",
        "\n",
        "# ============================= MAP SIGNAL ‚Üí NEXT TRADING DAY =============================\n",
        "entry_to_symbols = {}\n",
        "for sig_date in signal_df['signal_date'].unique():\n",
        "    entry_date = get_next_trading_day(sig_date, unique_trade_dates)\n",
        "    if entry_date is None:\n",
        "        continue\n",
        "    symbols = signal_df[signal_df['signal_date'] == sig_date]['symbol'].tolist()\n",
        "    entry_to_symbols.setdefault(entry_date, []).extend(symbols)\n",
        "    entry_to_symbols[entry_date] = list(set(entry_to_symbols[entry_date]))\n",
        "\n",
        "unique_entry_dates = sorted(entry_to_symbols.keys())\n",
        "print(f\"\\nWill check {len(unique_entry_dates)} entry dates for gap-up SHORTs\")\n",
        "\n",
        "# ============================= BACKTEST LOOP =============================\n",
        "output_trades = []\n",
        "cumulative_pnl = 0.0\n",
        "\n",
        "print(\"\\nStarting backtest...\\n\" + \"=\"*60)\n",
        "\n",
        "for entry_date in unique_entry_dates:\n",
        "    symbols = entry_to_symbols[entry_date]\n",
        "    prev_date = get_prev_trading_day(entry_date, unique_trade_dates)\n",
        "    if prev_date is None:\n",
        "        continue\n",
        "\n",
        "    gap_candidates = []\n",
        "    for sym in symbols:\n",
        "        if sym not in symbol_close_start_end:\n",
        "            continue\n",
        "        d = symbol_close_start_end[sym]\n",
        "        try:\n",
        "            prev_close = float(d[\"close_1529\"].loc[prev_date])\n",
        "            entry_price = float(d[\"open_start\"].loc[entry_date])\n",
        "            if prev_close <= 0 or entry_price <= 0:\n",
        "                continue\n",
        "            gap_pct = (entry_price - prev_close) / prev_close * 100\n",
        "            if gap_pct > 0:\n",
        "                gap_candidates.append({\n",
        "                    \"symbol\": sym,\n",
        "                    \"entry_price\": entry_price,\n",
        "                    \"gap_pct\": gap_pct\n",
        "                })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    if not gap_candidates:\n",
        "        continue\n",
        "\n",
        "    # Rank: highest gap-up first\n",
        "    gap_candidates.sort(key=lambda x: x[\"gap_pct\"], reverse=True)\n",
        "    selected = gap_candidates[:MAX_POSITIONS]\n",
        "\n",
        "    print(f\"{entry_date.date()} ‚Üí {len(selected)} gap-up SHORTs selected\")\n",
        "\n",
        "    for cand in selected:\n",
        "        sym = cand[\"symbol\"]\n",
        "        entry_price = cand[\"entry_price\"]\n",
        "        qty = math.floor(PER_STOCK_ALLOC / entry_price)\n",
        "        if qty <= 0:\n",
        "            continue\n",
        "        position_value = qty * entry_price\n",
        "        sl_price = entry_price * (1 + INDIVIDUAL_SL_PCT)\n",
        "\n",
        "        # Full day prices\n",
        "        df_full = symbol_full_data[sym]\n",
        "        day_df = df_full.filter(pl.col(\"TradeDate\") == entry_date).to_pandas()\n",
        "        day_prices = day_df[(day_df[\"TradeTime\"] >= START_TIME) & (day_df[\"TradeTime\"] <= END_TIME)]\n",
        "\n",
        "        if day_prices.empty:\n",
        "            continue\n",
        "\n",
        "        exit_price = None\n",
        "        exit_reason = \"EOD_CLOSE\"\n",
        "        exited = False\n",
        "\n",
        "        for _, row in day_prices.iterrows():\n",
        "            cur_price = row[\"Close\"]\n",
        "            cur_time = row[\"TradeTime\"]\n",
        "            if cur_time >= SL_ACTIVATION_TIME and cur_price >= sl_price:\n",
        "                exit_price = cur_price\n",
        "                exit_reason = f\"SL_{cur_time}\"\n",
        "                exited = True\n",
        "                break\n",
        "\n",
        "        if not exited:\n",
        "            eod_row = day_prices[day_prices[\"TradeTime\"] == END_TIME]\n",
        "            exit_price = eod_row[\"Close\"].values[0] if not eod_row.empty else day_prices[\"Close\"].iloc[-1]\n",
        "\n",
        "        position_pnl = qty * (entry_price - exit_price)\n",
        "        trade_roi = (position_pnl / position_value) * 100\n",
        "        portfolio_ret = (position_pnl / CAPITAL) * 100\n",
        "\n",
        "        cumulative_pnl += position_pnl\n",
        "        cum_ret_pct = (cumulative_pnl / CAPITAL) * 100\n",
        "\n",
        "        signal_date = get_prev_trading_day(entry_date, unique_trade_dates)\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, signal_date, entry_date, \"SHORT\",\n",
        "            round(entry_price, 2), qty, round(position_value, 2),\n",
        "            round(exit_price, 2), round(position_pnl, 2),\n",
        "            round(trade_roi, 2), round(portfolio_ret, 2), round(cum_ret_pct, 2),\n",
        "            exit_reason, START_TIME, round(cand[\"gap_pct\"], 2)\n",
        "        ])\n",
        "\n",
        "# ============================= SAVE RESULTS =============================\n",
        "output_df = pd.DataFrame(output_trades,\n",
        "    columns=[\"SYMBOL\",\"SIGNAL_DATE\",\"TRADE_DATE\",\"SIDE\",\n",
        "             \"ENTRY_PRICE\",\"QTY\",\"POSITION_VALUE\",\"EXIT_PRICE\",\"POSITION_PNL\",\n",
        "             \"TRADE_ROI%\",\"PORTFOLIO_RETURN%\",\"CUMULATIVE_RETURN%\",\n",
        "             \"EXIT_REASON\",\"ENTRY_TIME\",\"GAP_UP_%\"])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"\\nBACKTEST COMPLETE!\")\n",
        "print(f\"Total SHORT trades: {len(output_df)}\")\n",
        "print(\"Saved: OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# Daily PnL\n",
        "if not output_df.empty:\n",
        "    daily = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"POSITION_PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\",\n",
        "        \"PORTFOLIO_RETURN%\": \"sum\"\n",
        "    }).reset_index()\n",
        "    daily.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"POSITION_PNL\": \"DAILY_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_ROI%\",\n",
        "        \"PORTFOLIO_RETURN%\": \"DAILY_RETURN%\"\n",
        "    }, inplace=True)\n",
        "    daily[\"CUMULATIVE_RETURN%\"] = daily[\"DAILY_RETURN%\"].cumsum()\n",
        "    daily.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"Saved: DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"No trades executed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUO3LFziW3CV",
        "outputId": "cfc776ea-05b4-4227-c9db-53151326bd8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting short-all strategy backtest...\n",
            "‚úÖ Loaded 24 trading dates from filtered F&O file\n",
            "üìÇ Found 521 cash files\n",
            "‚úÖ Loaded 50/521 symbols\n",
            "‚úÖ Loaded 100/521 symbols\n",
            "‚úÖ Loaded 150/521 symbols\n",
            "‚úÖ Loaded 200/521 symbols\n",
            "‚úÖ Loaded 250/521 symbols\n",
            "‚úÖ Loaded 300/521 symbols\n",
            "‚úÖ Loaded 350/521 symbols\n",
            "‚úÖ Loaded 400/521 symbols\n",
            "‚úÖ Loaded 450/521 symbols\n",
            "‚úÖ Loaded 500/521 symbols\n",
            "‚úÖ Loaded all symbol minute data: 521 symbols\n",
            "\n",
            "üìÖ Processing 2025-09-22 (9 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-09-23 (18 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-09-24 (10 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-09-25 (4 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-09-26 (1 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-09-29 (6 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-09-30 (7 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-01 (16 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-03 (27 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-06 (40 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-07 (40 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-08 (14 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-09 (30 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-10 (38 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-13 (18 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-14 (8 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-15 (37 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-16 (47 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-17 (30 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-20 (36 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-21 (10 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-23 (31 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-24 (14 symbols)...\n",
            "\n",
            "üìÖ Processing 2025-10-27 (37 symbols)...\n",
            "\n",
            "‚úÖ Backtest completed ‚Üí 468 trades executed.\n",
            "üìÑ Saved OUTPUT_BACKTEST.csv\n",
            "üìÑ Saved DAILY_PNL.csv\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "START_TIME = \"09:15\"\n",
        "END_TIME = \"09:19\"\n",
        "SL_ACTIVATION_TIME = \"09:15\"  # not used, but kept for consistency\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Cash_data\"\n",
        "filtered_fno_path = \"/content/filtered_fno_symbols_all_dates.csv\"\n",
        "\n",
        "print(\"üöÄ Starting short-all strategy backtest...\")\n",
        "\n",
        "# ===================== LOAD F&O SYMBOL LIST =====================\n",
        "allowed_symbols_by_date = {}\n",
        "if os.path.exists(filtered_fno_path):\n",
        "    try:\n",
        "        fdf = pd.read_csv(filtered_fno_path, sep=None, engine=\"python\")\n",
        "    except Exception:\n",
        "        fdf = pd.read_csv(filtered_fno_path)\n",
        "\n",
        "    fdf.columns = [c.strip() for c in fdf.columns]\n",
        "    if \"date_only\" not in fdf.columns and \"date\" in fdf.columns:\n",
        "        fdf.rename(columns={\"date\": \"date_only\"}, inplace=True)\n",
        "\n",
        "    def _parse_date(val):\n",
        "        for fmt in (\"%y-%m-%d\", \"%Y-%m-%d\", \"%d-%m-%y\", \"%d-%m-%Y\"):\n",
        "            try:\n",
        "                return datetime.strptime(str(val).strip(), fmt).date()\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None\n",
        "\n",
        "    fdf[\"date_parsed\"] = fdf[\"date_only\"].apply(_parse_date)\n",
        "    for _, r in fdf.iterrows():\n",
        "        d = r.get(\"date_parsed\")\n",
        "        sym = str(r[\"symbol\"]).strip()\n",
        "        if d and sym and sym.lower() != \"nan\":\n",
        "            allowed_symbols_by_date.setdefault(d, set()).add(sym)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(allowed_symbols_by_date)} trading dates from filtered F&O file\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"‚ùå filtered_fno_symbols_all_dates.csv not found at {filtered_fno_path}\")\n",
        "\n",
        "# ===================== LOAD PRICE DATA =====================\n",
        "# ===================== LOAD PRICE DATA =====================\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "print(f\"üìÇ Found {len(all_files)} cash files\")\n",
        "\n",
        "def load_full_data(file_path):\n",
        "    # Remove 'cash_' prefix and extension to match F&O symbol names\n",
        "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    if base_name.lower().startswith(\"cash_\"):\n",
        "        symbol = base_name[5:].strip().upper()\n",
        "    else:\n",
        "        symbol = base_name.strip().upper()\n",
        "\n",
        "    df = pl.read_csv(file_path, try_parse_dates=False, low_memory=True).rename({\n",
        "        \"date\": \"Timestamp\",\n",
        "        \"open\": \"Open\", \"high\": \"High\",\n",
        "        \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"\n",
        "    })\n",
        "    df = df.with_columns(pl.col(\"Timestamp\").str.slice(0, 19).alias(\"ts_clean\"))\n",
        "    df = df.with_columns(pl.col(\"ts_clean\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"dt\"))\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"dt\").dt.date().alias(\"TradeDate\"),\n",
        "        pl.col(\"dt\").dt.strftime(\"%H:%M\").alias(\"TradeTime\")\n",
        "    ])\n",
        "    return symbol, df\n",
        "\n",
        "symbol_full_data = {}\n",
        "for i, f in enumerate(all_files, 1):\n",
        "    sym, df = load_full_data(f)\n",
        "    symbol_full_data[sym] = df\n",
        "    if i % 50 == 0:\n",
        "        print(f\"‚úÖ Loaded {i}/{len(all_files)} symbols\")\n",
        "\n",
        "print(f\"‚úÖ Loaded all symbol minute data: {len(symbol_full_data)} symbols\")\n",
        "\n",
        "\n",
        "# ===================== BACKTEST =====================\n",
        "output_trades = []\n",
        "cumulative_portfolio_return = 0.0\n",
        "\n",
        "for trade_date, allowed_syms in allowed_symbols_by_date.items():\n",
        "    print(f\"\\nüìÖ Processing {trade_date} ({len(allowed_syms)} symbols)...\")\n",
        "\n",
        "    for sym in allowed_syms:\n",
        "        if sym not in symbol_full_data:\n",
        "            continue\n",
        "\n",
        "        df = symbol_full_data[sym]\n",
        "        day_df = df.filter(pl.col(\"TradeDate\") == trade_date).select([\"TradeTime\", \"Close\"]).to_pandas()\n",
        "        if day_df.empty:\n",
        "            continue\n",
        "\n",
        "        # Entry at 09:15\n",
        "        entry_row = day_df[day_df[\"TradeTime\"] == START_TIME]\n",
        "        if entry_row.empty:\n",
        "            continue\n",
        "\n",
        "        entry_price = float(entry_row[\"Close\"].values[0])\n",
        "        side = \"SHORT\"\n",
        "\n",
        "        # Iterate through the day's minute closes\n",
        "        closes_above_entry = 0\n",
        "        exit_price = None\n",
        "        exit_reason = \"END_TIME\"\n",
        "\n",
        "        for _, r in day_df.iterrows():\n",
        "            cur_time = r[\"TradeTime\"]\n",
        "            cur_price = r[\"Close\"]\n",
        "\n",
        "            if cur_time < START_TIME:\n",
        "                continue\n",
        "\n",
        "            if cur_time > END_TIME:\n",
        "                break\n",
        "\n",
        "            if cur_price > entry_price:\n",
        "                closes_above_entry += 1\n",
        "            else:\n",
        "                closes_above_entry = 0\n",
        "\n",
        "            # Exit if price closed above entry twice consecutively\n",
        "            if closes_above_entry >= 2:\n",
        "                exit_price = cur_price\n",
        "                exit_reason = f\"EXIT_2_CLOSES_ABOVE_{cur_time}\"\n",
        "                break\n",
        "\n",
        "        if exit_price is None:\n",
        "            # Exit at 15:15 or last price\n",
        "            end_price = day_df[day_df[\"TradeTime\"] == END_TIME]\n",
        "            if not end_price.empty:\n",
        "                exit_price = float(end_price[\"Close\"].values[0])\n",
        "            else:\n",
        "                exit_price = float(day_df[\"Close\"].iloc[-1])\n",
        "                exit_reason = \"FALLBACK_LAST_PRICE\"\n",
        "\n",
        "        trade_pnl = round(entry_price - exit_price, 2)  # short trade\n",
        "        roi_trade = round((trade_pnl / entry_price) * 100, 2)\n",
        "        cumulative_portfolio_return += roi_trade\n",
        "\n",
        "        output_trades.append([\n",
        "            sym, trade_date, side,\n",
        "            entry_price, exit_price, trade_pnl, roi_trade,\n",
        "            exit_reason, round(cumulative_portfolio_return, 2)\n",
        "        ])\n",
        "\n",
        "# ===================== SAVE RESULTS =====================\n",
        "output_df = pd.DataFrame(output_trades, columns=[\n",
        "    \"SYMBOL\", \"TRADE_DATE\", \"SIDE\",\n",
        "    \"ENTRY_PRICE\", \"EXIT_PRICE\", \"PNL\",\n",
        "    \"TRADE_ROI%\", \"EXIT_REASON\", \"CUMULATIVE_PORTFOLIO_RETURN%\"\n",
        "])\n",
        "\n",
        "output_df.to_csv(\"OUTPUT_BACKTEST.csv\", index=False)\n",
        "print(f\"\\n‚úÖ Backtest completed ‚Üí {len(output_df)} trades executed.\")\n",
        "print(\"üìÑ Saved OUTPUT_BACKTEST.csv\")\n",
        "\n",
        "# ===================== DAILY PNL =====================\n",
        "if not output_df.empty:\n",
        "    daily_pnl_df = output_df.groupby(\"TRADE_DATE\").agg({\n",
        "        \"PNL\": \"sum\",\n",
        "        \"TRADE_ROI%\": \"mean\",\n",
        "        \"SYMBOL\": \"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_pnl_df.rename(columns={\n",
        "        \"SYMBOL\": \"NUM_TRADES\",\n",
        "        \"PNL\": \"DAILY_TOTAL_PNL\",\n",
        "        \"TRADE_ROI%\": \"AVG_TRADE_ROI%\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    daily_pnl_df[\"CUMULATIVE_PNL\"] = daily_pnl_df[\"AVG_TRADE_ROI%\"].cumsum()\n",
        "\n",
        "    daily_pnl_df.to_csv(\"DAILY_PNL.csv\", index=False)\n",
        "    print(\"üìÑ Saved DAILY_PNL.csv\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trades found, skipping Daily PnL export.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrvk2AAqlx7m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}